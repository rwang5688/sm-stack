{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (1.24.30)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.30 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3) (1.27.30)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.30->boto3) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.30->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.30->boto3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sagemaker in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (2.100.0)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (21.2.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (4.8.1)\n",
      "Requirement already satisfied: pandas in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (1.3.2)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pathos in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (0.2.9)\n",
      "Requirement already satisfied: google-pasta in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (1.20.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (21.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (3.18.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (1.24.30)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.30 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.30->boto3<2.0,>=1.20.21->sagemaker) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.30->boto3<2.0,>=1.20.21->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pandas->sagemaker) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.5)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.5.1)\n",
      "Requirement already satisfied: pox>=0.3.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sagemaker in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (2.100.0)\n",
      "Requirement already satisfied: pathos in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (0.2.9)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (1.24.30)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (3.18.0)\n",
      "Requirement already satisfied: google-pasta in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (21.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (4.8.1)\n",
      "Requirement already satisfied: pandas in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (1.3.2)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (21.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from sagemaker) (1.20.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.30 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.30)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.30->boto3<2.0,>=1.20.21->sagemaker) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.30->boto3<2.0,>=1.20.21->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pandas->sagemaker) (2021.1)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.5.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.13)\n",
      "Requirement already satisfied: pox>=0.3.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchmetrics in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from torchmetrics) (1.20.3)\n",
      "Requirement already satisfied: packaging in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from torchmetrics) (21.0)\n",
      "Requirement already satisfied: torch>=1.3.1 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from torchmetrics) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/evanphillips/opt/anaconda3/lib/python3.8/site-packages (from packaging->torchmetrics) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "%pip install boto3\n",
    "%pip install sagemaker\n",
    "%pip install -U sagemaker\n",
    "%pip install torchmetrics\n",
    "# %jupyter notebook --NotebookApp.max_buffer_size=12000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 01\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import sagemaker\n",
    "import torchmetrics\n",
    "import torch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = 'AmazonSageMaker-ExecutionRole-20220714T204241'\n",
    "# role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 02\n",
    "training_data_uri = 's3://multibert-8b87f872-a3fe-4a19-a016-e02402275450/training/multi_label_new.csv'\n",
    "#test_data_uri = 's3://multibert-8b87f872-a3fe-4a19-a016-e02402275450/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 05\n",
    "pytorch_estimator = PyTorch('multibert.py',\n",
    "                            instance_type='ml.g4dn.16xlarge',\n",
    "                            instance_count=3,\n",
    "                            role = role,\n",
    "                            framework_version='1.11.0',\n",
    "                            py_version='py38',\n",
    "                            source_dir = 'code',\n",
    "                            hyperparameters = {'epochs': 4, 'batch-size': 32, 'learning-rate': 2e-05})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-19 05:39:00 Starting - Starting the training job...\n",
      "2022-07-19 05:39:17 Starting - Preparing the instances for trainingProfilerReport-1658209139: InProgress\n",
      ".........\n",
      "2022-07-19 05:41:00 Downloading - Downloading input data\n",
      "2022-07-19 05:41:00 Training - Downloading the training image........................\n",
      "2022-07-19 05:45:04 Training - Training image download completed. Training in progress.\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2022-07-19 05:45:07,598 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2022-07-19 05:45:07,616 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2022-07-19 05:45:07,621 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2022-07-19 05:45:08,023 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.11.0+cu113)\u001b[0m\n",
      "\u001b[32mCollecting watermark\u001b[0m\n",
      "\u001b[32mDownloading watermark-2.3.1-py2.py3-none-any.whl (7.2 kB)\u001b[0m\n",
      "\u001b[32mCollecting transformers\u001b[0m\n",
      "\u001b[32mDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 21.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting pytorch-lightning\u001b[0m\n",
      "\u001b[32mDownloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 585.9/585.9 kB 32.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting colored\u001b[0m\n",
      "\u001b[32mDownloading colored-1.4.3.tar.gz (29 kB)\u001b[0m\n",
      "\u001b[32mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-07-19 05:45:07,828 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-07-19 05:45:07,848 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-07-19 05:45:07,853 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-07-19 05:45:08,329 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.11.0+cu113)\u001b[0m\n",
      "\u001b[34mCollecting watermark\u001b[0m\n",
      "\u001b[34mDownloading watermark-2.3.1-py2.py3-none-any.whl (7.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 52.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 585.9/585.9 kB 50.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting colored\u001b[0m\n",
      "\u001b[34mDownloading colored-1.4.3.tar.gz (29 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-0.9.2-py3-none-any.whl (419 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 419.7/419.7 kB 39.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython in /opt/conda/lib/python3.8/site-packages (from watermark->-r requirements.txt (line 2)) (8.1.0)\u001b[0m\n",
      "\u001b[32mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[32mCollecting torchmetrics\u001b[0m\n",
      "\u001b[32mDownloading torchmetrics-0.9.2-py3-none-any.whl (419 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 419.7/419.7 kB 29.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: ipython in /opt/conda/lib/python3.8/site-packages (from watermark->-r requirements.txt (line 2)) (8.1.0)\u001b[0m\n",
      "\u001b[32mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\u001b[0m\n",
      "\u001b[32mDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2022-07-19 05:45:07,508 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2022-07-19 05:45:07,528 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2022-07-19 05:45:07,534 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2022-07-19 05:45:07,951 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.11.0+cu113)\u001b[0m\n",
      "\u001b[35mCollecting watermark\u001b[0m\n",
      "\u001b[35mDownloading watermark-2.3.1-py2.py3-none-any.whl (7.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting transformers\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 50.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pytorch-lightning\u001b[0m\n",
      "\u001b[35mDownloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 585.9/585.9 kB 55.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting colored\u001b[0m\n",
      "\u001b[35mDownloading colored-1.4.3.tar.gz (29 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting torchmetrics\u001b[0m\n",
      "\u001b[35mDownloading torchmetrics-0.9.2-py3-none-any.whl (419 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 419.7/419.7 kB 44.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ipython in /opt/conda/lib/python3.8/site-packages (from watermark->-r requirements.txt (line 2)) (8.1.0)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[35mDownloading regex-2022.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.0/765.0 kB 67.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[35mCollecting filelock\u001b[0m\n",
      "\u001b[35mDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (2.28.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (21.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (1.22.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 105.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.0/765.0 kB 64.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 53.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (2.28.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[32mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[32mDownloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 kB 10.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (21.3)\u001b[0m\n",
      "\u001b[32mCollecting filelock\u001b[0m\n",
      "\u001b[32mDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[32mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[32mDownloading regex-2022.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.0/765.0 kB 47.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (1.22.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 4)) (2022.5.0)\u001b[0m\n",
      "\u001b[32mCollecting tensorboard>=2.2.0\u001b[0m\n",
      "\u001b[32mDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 104.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pyDeprecate>=0.3.1\u001b[0m\n",
      "\u001b[35mDownloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 4)) (2022.5.0)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard>=2.2.0\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 108.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 4)) (3.19.4)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 kB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 3)) (1.22.2)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard>=2.2.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 112.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate>=0.3.1\u001b[0m\n",
      "\u001b[34mDownloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 4)) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 4)) (3.19.4)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 81.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 3)) (3.0.9)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 28.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 77.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 4)) (3.19.4)\u001b[0m\n",
      "\u001b[32mCollecting pyDeprecate>=0.3.1\u001b[0m\n",
      "\u001b[32mDownloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[32mCollecting aiohttp\u001b[0m\n",
      "\u001b[32mDownloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 70.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 3)) (3.0.9)\u001b[0m\n",
      "\u001b[32mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[32mDownloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.8/167.8 kB 11.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting aiohttp\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 90.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 3)) (3.0.9)\u001b[0m\n",
      "\u001b[35mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[35mDownloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 29.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[35mDownloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.8/167.8 kB 16.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (2.1.2)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 117.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 113.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (63.2.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 103.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (0.37.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 70.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[32mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[32mDownloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 95.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[32mDownloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 11.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[32mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 75.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (2.1.2)\u001b[0m\n",
      "\u001b[32mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[32mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 9.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[32mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 108.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (0.37.1)\u001b[0m\n",
      "\u001b[32mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[32mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (63.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2022.6.15)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.10)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.7.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.1.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (5.3.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.3.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (5.1.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (4.8.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.18.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (2.12.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (3.0.30)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[32mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[32mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 117.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (63.2.0)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 51.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (0.37.1)\u001b[0m\n",
      "\u001b[35mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[35mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 8.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2022.6.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.18.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (5.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (5.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.7.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (2.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (4.8.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (3.0.30)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.1.3)\u001b[0m\n",
      "\u001b[35mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[35mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[35mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 15.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[35mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython->watermark->-r requirements.txt (line 2)) (0.8.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (4.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython->watermark->-r requirements.txt (line 2)) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark->-r requirements.txt (line 2)) (0.2.5)\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 34.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.8/167.8 kB 15.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (3.0.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (5.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (5.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython->watermark->-r requirements.txt (line 2)) (2.12.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 11.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython->watermark->-r requirements.txt (line 2)) (0.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (4.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython->watermark->-r requirements.txt (line 2)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark->-r requirements.txt (line 2)) (0.2.5)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 4)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 23.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[32mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[32mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 10.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[32mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[32mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython->watermark->-r requirements.txt (line 2)) (0.8.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (4.12.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython->watermark->-r requirements.txt (line 2)) (0.7.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark->-r requirements.txt (line 2)) (0.2.5)\u001b[0m\n",
      "\u001b[32mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[32mDownloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 16.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[32mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[32mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[32mDownloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[32mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[32mDownloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 29.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 4)) (21.4.0)\u001b[0m\n",
      "\u001b[32mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[32mDownloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 33.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (0.2.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (2.0.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (0.8.3)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 4)) (21.4.0)\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 33.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (0.8.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (2.0.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (0.2.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (3.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[35mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[35mDownloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.5/151.5 kB 23.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: colored\u001b[0m\n",
      "\u001b[35mBuilding wheel for colored (setup.py): started\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (3.8.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[32mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[32mDownloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.5/151.5 kB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mBuilding wheels for collected packages: colored\u001b[0m\n",
      "\u001b[32mBuilding wheel for colored (setup.py): started\u001b[0m\n",
      "\u001b[32mBuilding wheel for colored (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[32mCreated wheel for colored: filename=colored-1.4.3-py3-none-any.whl size=14323 sha256=d1884ec3cbdb3232216f0ff8f7e7fde65952cbd78037e7da5fe205f6ed24d710\u001b[0m\n",
      "\u001b[32mStored in directory: /root/.cache/pip/wheels/b3/cf/a4/23200f342c1291c99b34a54e4997a6cd9ed23f58a924ddaa49\u001b[0m\n",
      "\u001b[32mSuccessfully built colored\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 19.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (0.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (0.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->watermark->-r requirements.txt (line 2)) (2.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.5/151.5 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: colored\u001b[0m\n",
      "\u001b[34mBuilding wheel for colored (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for colored (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for colored: filename=colored-1.4.3-py3-none-any.whl size=14323 sha256=d1884ec3cbdb3232216f0ff8f7e7fde65952cbd78037e7da5fe205f6ed24d710\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b3/cf/a4/23200f342c1291c99b34a54e4997a6cd9ed23f58a924ddaa49\u001b[0m\n",
      "\u001b[34mSuccessfully built colored\u001b[0m\n",
      "\u001b[35mBuilding wheel for colored (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for colored: filename=colored-1.4.3-py3-none-any.whl size=14323 sha256=d1884ec3cbdb3232216f0ff8f7e7fde65952cbd78037e7da5fe205f6ed24d710\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/b3/cf/a4/23200f342c1291c99b34a54e4997a6cd9ed23f58a924ddaa49\u001b[0m\n",
      "\u001b[35mSuccessfully built colored\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, tensorboard-plugin-wit, colored, tensorboard-data-server, regex, pyDeprecate, pyasn1-modules, oauthlib, multidict, grpcio, frozenlist, filelock, cachetools, async-timeout, absl-py, yarl, torchmetrics, requests-oauthlib, markdown, huggingface-hub, google-auth, aiosignal, transformers, google-auth-oauthlib, aiohttp, watermark, tensorboard, pytorch-lightning\u001b[0m\n",
      "\u001b[32mInstalling collected packages: tokenizers, tensorboard-plugin-wit, colored, tensorboard-data-server, regex, pyDeprecate, pyasn1-modules, oauthlib, multidict, grpcio, frozenlist, filelock, cachetools, async-timeout, absl-py, yarl, torchmetrics, requests-oauthlib, markdown, huggingface-hub, google-auth, aiosignal, transformers, google-auth-oauthlib, aiohttp, watermark, tensorboard, pytorch-lightning\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, tensorboard-plugin-wit, colored, tensorboard-data-server, regex, pyDeprecate, pyasn1-modules, oauthlib, multidict, grpcio, frozenlist, filelock, cachetools, async-timeout, absl-py, yarl, torchmetrics, requests-oauthlib, markdown, huggingface-hub, google-auth, aiosignal, transformers, google-auth-oauthlib, aiohttp, watermark, tensorboard, pytorch-lightning\u001b[0m\n",
      "\u001b[32mSuccessfully installed absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 cachetools-5.2.0 colored-1.4.3 filelock-3.7.1 frozenlist-1.3.0 google-auth-2.9.1 google-auth-oauthlib-0.4.6 grpcio-1.47.0 huggingface-hub-0.8.1 markdown-3.4.1 multidict-6.0.2 oauthlib-3.2.0 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pytorch-lightning-1.6.5 regex-2022.7.9 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.12.1 torchmetrics-0.9.2 transformers-4.20.1 watermark-2.3.1 yarl-1.7.2\u001b[0m\n",
      "\u001b[32mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 cachetools-5.2.0 colored-1.4.3 filelock-3.7.1 frozenlist-1.3.0 google-auth-2.9.1 google-auth-oauthlib-0.4.6 grpcio-1.47.0 huggingface-hub-0.8.1 markdown-3.4.1 multidict-6.0.2 oauthlib-3.2.0 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pytorch-lightning-1.6.5 regex-2022.7.9 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.12.1 torchmetrics-0.9.2 transformers-4.20.1 watermark-2.3.1 yarl-1.7.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-07-19 05:45:20,134 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2022-07-19 05:45:20,135 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2022-07-19 05:45:20,198 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-3\",\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 32,\n",
      "        \"epochs\": 4,\n",
      "        \"learning-rate\": 2e-05\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-3\",\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-07-19-05-38-58-148\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"multibert\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-3\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"multibert.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"batch-size\":32,\"epochs\":4,\"learning-rate\":2e-05}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=multibert.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.16xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-3\",\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=multibert\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g4dn.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"hyperparameters\":{\"batch-size\":32,\"epochs\":4,\"learning-rate\":2e-05},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-07-19-05-38-58-148\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\",\"module_name\":\"multibert\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"multibert.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--batch-size\",\"32\",\"--epochs\",\"4\",\"--learning-rate\",\"2e-05\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH-SIZE=32\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING-RATE=2e-05\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 multibert.py --batch-size 32 --epochs 4 --learning-rate 2e-05\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 cachetools-5.2.0 colored-1.4.3 filelock-3.7.1 frozenlist-1.3.0 google-auth-2.9.1 google-auth-oauthlib-0.4.6 grpcio-1.47.0 huggingface-hub-0.8.1 markdown-3.4.1 multidict-6.0.2 oauthlib-3.2.0 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pytorch-lightning-1.6.5 regex-2022.7.9 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.12.1 torchmetrics-0.9.2 transformers-4.20.1 watermark-2.3.1 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-07-19 05:45:20,687 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-07-19 05:45:20,687 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-07-19 05:45:20,747 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-3\",\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 32,\n",
      "        \"epochs\": 4,\n",
      "        \"learning-rate\": 2e-05\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-3\",\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-07-19-05-38-58-148\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"multibert\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-3\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"multibert.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":32,\"epochs\":4,\"learning-rate\":2e-05}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=multibert.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-3\",\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=multibert\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g4dn.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"hyperparameters\":{\"batch-size\":32,\"epochs\":4,\"learning-rate\":2e-05},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-07-19-05-38-58-148\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\",\"module_name\":\"multibert\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"multibert.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"32\",\"--epochs\",\"4\",\"--learning-rate\",\"2e-05\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=2e-05\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 multibert.py --batch-size 32 --epochs 4 --learning-rate 2e-05\u001b[0m\n",
      "\u001b[32m2022-07-19 05:45:20,549 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2022-07-19 05:45:20,549 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2022-07-19 05:45:20,607 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-3\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-3\",\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 32,\n",
      "        \"epochs\": 4,\n",
      "        \"learning-rate\": 2e-05\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-3\",\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-07-19-05-38-58-148\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"multibert\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-3\",\n",
      "        \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-3\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"multibert.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"batch-size\":32,\"epochs\":4,\"learning-rate\":2e-05}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=multibert.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-3\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.16xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-3\",\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=multibert\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-3\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g4dn.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"hyperparameters\":{\"batch-size\":32,\"epochs\":4,\"learning-rate\":2e-05},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-07-19-05-38-58-148\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-113969896847/pytorch-training-2022-07-19-05-38-58-148/source/sourcedir.tar.gz\",\"module_name\":\"multibert\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"multibert.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--batch-size\",\"32\",\"--epochs\",\"4\",\"--learning-rate\",\"2e-05\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[32mSM_HP_BATCH-SIZE=32\u001b[0m\n",
      "\u001b[32mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[32mSM_HP_LEARNING-RATE=2e-05\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.8 multibert.py --batch-size 32 --epochs 4 --learning-rate 2e-05\u001b[0m\n",
      "\u001b[35mclean_data_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[35m1  Zimmerman we comin for yo life bitch. http://t...  ...            0.0\u001b[0m\n",
      "\u001b[35m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[35m3  Zelda bitches lol @joeylattime https://t.co/Cp...  ...            0.0\u001b[0m\n",
      "\u001b[35m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[35m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[35m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[35m7  Yup! RT @STheMisfit Then she got AIDs and died...  ...            0.0\u001b[0m\n",
      "\u001b[35m8  Yup, I officially do not like you. You are a n...  ...            0.0\u001b[0m\n",
      "\u001b[35m9              Yup RT @Durags4Eva: Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35mclean_data_input: column: tweet\u001b[0m\n",
      "\u001b[35m/opt/ml/code/preprocessing.py:38: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataframe[column] = dataframe[column].str.replace('[^A-Za-z0-9 ]+','')\u001b[0m\n",
      "\u001b[34mclean_data_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[34m1  Zimmerman we comin for yo life bitch. http://t...  ...            0.0\u001b[0m\n",
      "\u001b[34m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[34m3  Zelda bitches lol @joeylattime https://t.co/Cp...  ...            0.0\u001b[0m\n",
      "\u001b[34m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[34m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[34m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[34m7  Yup! RT @STheMisfit Then she got AIDs and died...  ...            0.0\u001b[0m\n",
      "\u001b[34m8  Yup, I officially do not like you. You are a n...  ...            0.0\u001b[0m\n",
      "\u001b[34m9              Yup RT @Durags4Eva: Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34mclean_data_input: column: tweet\u001b[0m\n",
      "\u001b[32mclean_data_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[32m1  Zimmerman we comin for yo life bitch. http://t...  ...            0.0\u001b[0m\n",
      "\u001b[32m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[32m3  Zelda bitches lol @joeylattime https://t.co/Cp...  ...            0.0\u001b[0m\n",
      "\u001b[32m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[32m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[32m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[32m7  Yup! RT @STheMisfit Then she got AIDs and died...  ...            0.0\u001b[0m\n",
      "\u001b[32m8  Yup, I officially do not like you. You are a n...  ...            0.0\u001b[0m\n",
      "\u001b[32m9              Yup RT @Durags4Eva: Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32mclean_data_input: column: tweet\u001b[0m\n",
      "\u001b[32m/opt/ml/code/preprocessing.py:38: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataframe[column] = dataframe[column].str.replace('[^A-Za-z0-9 ]+','')\u001b[0m\n",
      "\u001b[32mclean_data_output: return_val:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[32m1             Zimmerman we comin for yo life bitch    ...            0.0\u001b[0m\n",
      "\u001b[32m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[32m3                               Zelda bitches lol     ...            0.0\u001b[0m\n",
      "\u001b[32m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[32m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[32m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[32m7  Yup   Then she got AIDs and died   Jenny was a...  ...            0.0\u001b[0m\n",
      "\u001b[32m8  Yup I officially do not like you You are a nee...  ...            0.0\u001b[0m\n",
      "\u001b[32m9                            Yup   Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32mclean_data completed!\u001b[0m\n",
      "\u001b[32m====================================================================================================\u001b[0m\n",
      "\u001b[32mto_int_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[32m1             Zimmerman we comin for yo life bitch    ...            0.0\u001b[0m\n",
      "\u001b[32m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[32m3                               Zelda bitches lol     ...            0.0\u001b[0m\n",
      "\u001b[32m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[32m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[32m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[32m7  Yup   Then she got AIDs and died   Jenny was a...  ...            0.0\u001b[0m\n",
      "\u001b[32m8  Yup I officially do not like you You are a nee...  ...            0.0\u001b[0m\n",
      "\u001b[32m9                            Yup   Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32m/opt/ml/code/preprocessing.py:76: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[32mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[32mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[32mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'general criticsm']=dataframe.loc[:,'general criticsm'].astype(int)\u001b[0m\n",
      "\u001b[32m/opt/ml/code/preprocessing.py:77: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[32mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[32mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[32mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'disability shaming']=dataframe.loc[:,'disability shaming'].astype(int)\u001b[0m\n",
      "\u001b[32m/opt/ml/code/preprocessing.py:78: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[32mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[32mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[32mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'racial prejudice']=dataframe.loc[:,'racial prejudice'].astype(int)\u001b[0m\n",
      "\u001b[32m/opt/ml/code/preprocessing.py:79: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[32mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[32mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[32mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'sexism']=dataframe.loc[:,'sexism'].astype(int)\u001b[0m\n",
      "\u001b[32m/opt/ml/code/preprocessing.py:80: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[32mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[32mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[32mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'lgbtq+ phobia']=dataframe.loc[:,'lgbtq+ phobia'].astype(int)\u001b[0m\n",
      "\u001b[32mto_int_output: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m0                                  Zuko gets bitches  ...              0\u001b[0m\n",
      "\u001b[32m1             Zimmerman we comin for yo life bitch    ...              0\u001b[0m\n",
      "\u001b[32m2  Zhou Mi was just layin on his bed and SM just ...  ...              0\u001b[0m\n",
      "\u001b[32m3                               Zelda bitches lol     ...              0\u001b[0m\n",
      "\u001b[32m4         Zack still questions my love for Oreos lol  ...              0\u001b[0m\n",
      "\u001b[32m5    Zach Huggins tell yo bitch to leave me tf alone  ...              0\u001b[0m\n",
      "\u001b[32m6  Yuu Hinouchi gets fucked by two guys until she...  ...              0\u001b[0m\n",
      "\u001b[32m7  Yup   Then she got AIDs and died   Jenny was a...  ...              0\u001b[0m\n",
      "\u001b[32m8  Yup I officially do not like you You are a nee...  ...              0\u001b[0m\n",
      "\u001b[32m9                            Yup   Egg nog trash rap  ...              0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32mto_int completed!\u001b[0m\n",
      "\u001b[32m==================================================\u001b[0m\n",
      "\u001b[32msplit_data_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m0                                  Zuko gets bitches  ...              0\u001b[0m\n",
      "\u001b[32m1             Zimmerman we comin for yo life bitch    ...              0\u001b[0m\n",
      "\u001b[32m2  Zhou Mi was just layin on his bed and SM just ...  ...              0\u001b[0m\n",
      "\u001b[32m3                               Zelda bitches lol     ...              0\u001b[0m\n",
      "\u001b[32m4         Zack still questions my love for Oreos lol  ...              0\u001b[0m\n",
      "\u001b[32m5    Zach Huggins tell yo bitch to leave me tf alone  ...              0\u001b[0m\n",
      "\u001b[32m6  Yuu Hinouchi gets fucked by two guys until she...  ...              0\u001b[0m\n",
      "\u001b[32m7  Yup   Then she got AIDs and died   Jenny was a...  ...              0\u001b[0m\n",
      "\u001b[32m8  Yup I officially do not like you You are a nee...  ...              0\u001b[0m\n",
      "\u001b[32m9                            Yup   Egg nog trash rap  ...              0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32msplit_data_output: train_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m8501              before I betray them I slit my wrist    ...              0\u001b[0m\n",
      "\u001b[32m23880   Youre killin me man lt Not all conservatives ...  ...              0\u001b[0m\n",
      "\u001b[32m18909   I just wana eat some food Drink some liq Go t...  ...              0\u001b[0m\n",
      "\u001b[32m2163                                  This fuckin bird    ...              0\u001b[0m\n",
      "\u001b[32m8979           Weed aint a drug Its a fucking herb bitch  ...              0\u001b[0m\n",
      "\u001b[32m4985       all deze hoes got past demons in em YOLO n...  ...              0\u001b[0m\n",
      "\u001b[32m15926  Every nigga tryna fuck the next nigga bitch no...  ...              0\u001b[0m\n",
      "\u001b[32m4591                             Mary Jane my only bitch  ...              0\u001b[0m\n",
      "\u001b[32m5426      Women would put up with very little love as...  ...              0\u001b[0m\n",
      "\u001b[32m6406                          Basic bitch starter pack    ...              0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32msplit_data_output: val_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m16325                    David is bitch made HeIsMyBitch  ...              0\u001b[0m\n",
      "\u001b[32m23979                           how original you faggots  ...              1\u001b[0m\n",
      "\u001b[32m6004     Bad bitch HTown keep it trill yall know yall...  ...              0\u001b[0m\n",
      "\u001b[32m5656     Dont message me your number and tell me to t...  ...              0\u001b[0m\n",
      "\u001b[32m5621           So proud of mahanain  she still a hoe tho  ...              0\u001b[0m\n",
      "\u001b[32m6023     Yall we use to have to take pictures on real...  ...              0\u001b[0m\n",
      "\u001b[32m22312   ok Ill sink to your level youre a special kin...  ...              0\u001b[0m\n",
      "\u001b[32m6704     I only wear weave because I like it  A bitch...  ...              0\u001b[0m\n",
      "\u001b[32m22784    puppets like Don will intentionally and fore...  ...              0\u001b[0m\n",
      "\u001b[32m8397     Your iPad raised your kid to be a giant puss...  ...              0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32msplit_data completed!\u001b[0m\n",
      "\u001b[32m====================================================================================================\u001b[0m\n",
      "\u001b[32mlabel_cols_input: dataframe:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m8501              before I betray them I slit my wrist    ...              0\u001b[0m\n",
      "\u001b[32m23880   Youre killin me man lt Not all conservatives ...  ...              0\u001b[0m\n",
      "\u001b[32m18909   I just wana eat some food Drink some liq Go t...  ...              0\u001b[0m\n",
      "\u001b[32m2163                                  This fuckin bird    ...              0\u001b[0m\n",
      "\u001b[32m8979           Weed aint a drug Its a fucking herb bitch  ...              0\u001b[0m\n",
      "\u001b[32m4985       all deze hoes got past demons in em YOLO n...  ...              0\u001b[0m\n",
      "\u001b[32m15926  Every nigga tryna fuck the next nigga bitch no...  ...              0\u001b[0m\n",
      "\u001b[32m4591                             Mary Jane my only bitch  ...              0\u001b[0m\n",
      "\u001b[32m5426      Women would put up with very little love as...  ...              0\u001b[0m\n",
      "\u001b[32m6406                          Basic bitch starter pack    ...              0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32mlabel_cols_output: ['general criticsm', 'disability shaming', 'racial prejudice', 'sexism', 'lgbtq+ phobia']\u001b[0m\n",
      "\u001b[32mlabel_cols completed!\u001b[0m\n",
      "\u001b[32m====================================================================================================\u001b[0m\n",
      "\u001b[35mclean_data_output: return_val:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[35m1             Zimmerman we comin for yo life bitch    ...            0.0\u001b[0m\n",
      "\u001b[35m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[35m3                               Zelda bitches lol     ...            0.0\u001b[0m\n",
      "\u001b[35m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[35m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[35m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[35m7  Yup   Then she got AIDs and died   Jenny was a...  ...            0.0\u001b[0m\n",
      "\u001b[35m8  Yup I officially do not like you You are a nee...  ...            0.0\u001b[0m\n",
      "\u001b[35m9                            Yup   Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35mclean_data completed!\u001b[0m\n",
      "\u001b[35m====================================================================================================\u001b[0m\n",
      "\u001b[35mto_int_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[35m1             Zimmerman we comin for yo life bitch    ...            0.0\u001b[0m\n",
      "\u001b[35m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[35m3                               Zelda bitches lol     ...            0.0\u001b[0m\n",
      "\u001b[35m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[35m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[35m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[35m7  Yup   Then she got AIDs and died   Jenny was a...  ...            0.0\u001b[0m\n",
      "\u001b[35m8  Yup I officially do not like you You are a nee...  ...            0.0\u001b[0m\n",
      "\u001b[35m9                            Yup   Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35m/opt/ml/code/preprocessing.py:76: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[35mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[35mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[35mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'general criticsm']=dataframe.loc[:,'general criticsm'].astype(int)\u001b[0m\n",
      "\u001b[35m/opt/ml/code/preprocessing.py:77: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[35mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[35mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[35mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'disability shaming']=dataframe.loc[:,'disability shaming'].astype(int)\u001b[0m\n",
      "\u001b[35m/opt/ml/code/preprocessing.py:78: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[35mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[35mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[35mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'racial prejudice']=dataframe.loc[:,'racial prejudice'].astype(int)\u001b[0m\n",
      "\u001b[35m/opt/ml/code/preprocessing.py:79: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[35mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[35mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[35mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'sexism']=dataframe.loc[:,'sexism'].astype(int)\u001b[0m\n",
      "\u001b[35m/opt/ml/code/preprocessing.py:80: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[35mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[35mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[35mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'lgbtq+ phobia']=dataframe.loc[:,'lgbtq+ phobia'].astype(int)\u001b[0m\n",
      "\u001b[35mto_int_output: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m0                                  Zuko gets bitches  ...              0\u001b[0m\n",
      "\u001b[35m1             Zimmerman we comin for yo life bitch    ...              0\u001b[0m\n",
      "\u001b[35m2  Zhou Mi was just layin on his bed and SM just ...  ...              0\u001b[0m\n",
      "\u001b[35m3                               Zelda bitches lol     ...              0\u001b[0m\n",
      "\u001b[35m4         Zack still questions my love for Oreos lol  ...              0\u001b[0m\n",
      "\u001b[35m5    Zach Huggins tell yo bitch to leave me tf alone  ...              0\u001b[0m\n",
      "\u001b[35m6  Yuu Hinouchi gets fucked by two guys until she...  ...              0\u001b[0m\n",
      "\u001b[35m7  Yup   Then she got AIDs and died   Jenny was a...  ...              0\u001b[0m\n",
      "\u001b[35m8  Yup I officially do not like you You are a nee...  ...              0\u001b[0m\n",
      "\u001b[35m9                            Yup   Egg nog trash rap  ...              0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35mto_int completed!\u001b[0m\n",
      "\u001b[35m==================================================\u001b[0m\n",
      "\u001b[35msplit_data_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m0                                  Zuko gets bitches  ...              0\u001b[0m\n",
      "\u001b[35m1             Zimmerman we comin for yo life bitch    ...              0\u001b[0m\n",
      "\u001b[35m2  Zhou Mi was just layin on his bed and SM just ...  ...              0\u001b[0m\n",
      "\u001b[35m3                               Zelda bitches lol     ...              0\u001b[0m\n",
      "\u001b[35m4         Zack still questions my love for Oreos lol  ...              0\u001b[0m\n",
      "\u001b[35m5    Zach Huggins tell yo bitch to leave me tf alone  ...              0\u001b[0m\n",
      "\u001b[35m6  Yuu Hinouchi gets fucked by two guys until she...  ...              0\u001b[0m\n",
      "\u001b[35m7  Yup   Then she got AIDs and died   Jenny was a...  ...              0\u001b[0m\n",
      "\u001b[35m8  Yup I officially do not like you You are a nee...  ...              0\u001b[0m\n",
      "\u001b[35m9                            Yup   Egg nog trash rap  ...              0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35msplit_data_output: train_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m13812  I need a Spanish WOMAN They dont have ghetto n...  ...              0\u001b[0m\n",
      "\u001b[35m409            you a pussy ass nigga and I know it nigga  ...              0\u001b[0m\n",
      "\u001b[35m9253     I feel bad for guys that give their girlfrie...  ...              0\u001b[0m\n",
      "\u001b[35m23726   Huhlast  gamesTampa Balt Yanks and they lost ...  ...              0\u001b[0m\n",
      "\u001b[35m3535                 Since you hoes wanna be childish     ...              0\u001b[0m\n",
      "\u001b[35m6570                       Amo a esa gente que me odia    ...              0\u001b[0m\n",
      "\u001b[35m4628     I wonder how many of yall hoes got homegrown...  ...              0\u001b[0m\n",
      "\u001b[35m21052                   Im already out that bitch hahaha  ...              0\u001b[0m\n",
      "\u001b[35m5250                            J Cole aint trash Period  ...              0\u001b[0m\n",
      "\u001b[35m222    You know the head good when it feel like she h...  ...              0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35msplit_data_output: val_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m17571  Alright ya faggots Lets just stop with all the...  ...              1\u001b[0m\n",
      "\u001b[35m17221  Because of the message that sends to LGBTQ res...  ...              1\u001b[0m\n",
      "\u001b[35m23367                               choke on bread bitch  ...              0\u001b[0m\n",
      "\u001b[35m1066   Which one of you hoes then downloaded that Jai...  ...              0\u001b[0m\n",
      "\u001b[35m21038                   big fucking dealcry baby niggers  ...              0\u001b[0m\n",
      "\u001b[35m12691  Ima kill cay  ugh she stay on my last nerve bu...  ...              0\u001b[0m\n",
      "\u001b[35m3387   So Sterling bitch gotta he a hoe aye she paid ...  ...              0\u001b[0m\n",
      "\u001b[35m22681     Altyd zo geweest hoe vaak men ook zegtdit n...  ...              0\u001b[0m\n",
      "\u001b[35m3042   Talk about me behind my back I hate that pussy...  ...              0\u001b[0m\n",
      "\u001b[35m8509     Making homemade pickles and working on a fun...  ...              0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35msplit_data completed!\u001b[0m\n",
      "\u001b[35m====================================================================================================\u001b[0m\n",
      "\u001b[35mlabel_cols_input: dataframe:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m13812  I need a Spanish WOMAN They dont have ghetto n...  ...              0\u001b[0m\n",
      "\u001b[35m409            you a pussy ass nigga and I know it nigga  ...              0\u001b[0m\n",
      "\u001b[35m9253     I feel bad for guys that give their girlfrie...  ...              0\u001b[0m\n",
      "\u001b[35m23726   Huhlast  gamesTampa Balt Yanks and they lost ...  ...              0\u001b[0m\n",
      "\u001b[35m3535                 Since you hoes wanna be childish     ...              0\u001b[0m\n",
      "\u001b[35m6570                       Amo a esa gente que me odia    ...              0\u001b[0m\n",
      "\u001b[35m4628     I wonder how many of yall hoes got homegrown...  ...              0\u001b[0m\n",
      "\u001b[35m21052                   Im already out that bitch hahaha  ...              0\u001b[0m\n",
      "\u001b[35m5250                            J Cole aint trash Period  ...              0\u001b[0m\n",
      "\u001b[35m222    You know the head good when it feel like she h...  ...              0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35mlabel_cols_output: ['general criticsm', 'disability shaming', 'racial prejudice', 'sexism', 'lgbtq+ phobia']\u001b[0m\n",
      "\u001b[35mlabel_cols completed!\u001b[0m\n",
      "\u001b[35m====================================================================================================\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 44.2kB/s]\u001b[0m\n",
      "\u001b[34m/opt/ml/code/preprocessing.py:38: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataframe[column] = dataframe[column].str.replace('[^A-Za-z0-9 ]+','')\u001b[0m\n",
      "\u001b[34mclean_data_output: return_val:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[34m1             Zimmerman we comin for yo life bitch    ...            0.0\u001b[0m\n",
      "\u001b[34m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[34m3                               Zelda bitches lol     ...            0.0\u001b[0m\n",
      "\u001b[34m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[34m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[34m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[34m7  Yup   Then she got AIDs and died   Jenny was a...  ...            0.0\u001b[0m\n",
      "\u001b[34m8  Yup I officially do not like you You are a nee...  ...            0.0\u001b[0m\n",
      "\u001b[34m9                            Yup   Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34mclean_data completed!\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mto_int_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m0                                  Zuko gets bitches  ...            0.0\u001b[0m\n",
      "\u001b[34m1             Zimmerman we comin for yo life bitch    ...            0.0\u001b[0m\n",
      "\u001b[34m2  Zhou Mi was just layin on his bed and SM just ...  ...            0.0\u001b[0m\n",
      "\u001b[34m3                               Zelda bitches lol     ...            0.0\u001b[0m\n",
      "\u001b[34m4         Zack still questions my love for Oreos lol  ...            0.0\u001b[0m\n",
      "\u001b[34m5    Zach Huggins tell yo bitch to leave me tf alone  ...            0.0\u001b[0m\n",
      "\u001b[34m6  Yuu Hinouchi gets fucked by two guys until she...  ...            0.0\u001b[0m\n",
      "\u001b[34m7  Yup   Then she got AIDs and died   Jenny was a...  ...            0.0\u001b[0m\n",
      "\u001b[34m8  Yup I officially do not like you You are a nee...  ...            0.0\u001b[0m\n",
      "\u001b[34m9                            Yup   Egg nog trash rap  ...            0.0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34m/opt/ml/code/preprocessing.py:76: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'general criticsm']=dataframe.loc[:,'general criticsm'].astype(int)\u001b[0m\n",
      "\u001b[34m/opt/ml/code/preprocessing.py:77: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'disability shaming']=dataframe.loc[:,'disability shaming'].astype(int)\u001b[0m\n",
      "\u001b[34m/opt/ml/code/preprocessing.py:78: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'racial prejudice']=dataframe.loc[:,'racial prejudice'].astype(int)\u001b[0m\n",
      "\u001b[34m/opt/ml/code/preprocessing.py:79: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'sexism']=dataframe.loc[:,'sexism'].astype(int)\u001b[0m\n",
      "\u001b[34m/opt/ml/code/preprocessing.py:80: SettingWithCopyWarning: \u001b[0m\n",
      "\u001b[34mA value is trying to be set on a copy of a slice from a DataFrame.\u001b[0m\n",
      "\u001b[34mTry using .loc[row_indexer,col_indexer] = value instead\u001b[0m\n",
      "\u001b[34mSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[:,'lgbtq+ phobia']=dataframe.loc[:,'lgbtq+ phobia'].astype(int)\u001b[0m\n",
      "\u001b[34mto_int_output: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m0                                  Zuko gets bitches  ...              0\u001b[0m\n",
      "\u001b[34m1             Zimmerman we comin for yo life bitch    ...              0\u001b[0m\n",
      "\u001b[34m2  Zhou Mi was just layin on his bed and SM just ...  ...              0\u001b[0m\n",
      "\u001b[34m3                               Zelda bitches lol     ...              0\u001b[0m\n",
      "\u001b[34m4         Zack still questions my love for Oreos lol  ...              0\u001b[0m\n",
      "\u001b[34m5    Zach Huggins tell yo bitch to leave me tf alone  ...              0\u001b[0m\n",
      "\u001b[34m6  Yuu Hinouchi gets fucked by two guys until she...  ...              0\u001b[0m\n",
      "\u001b[34m7  Yup   Then she got AIDs and died   Jenny was a...  ...              0\u001b[0m\n",
      "\u001b[34m8  Yup I officially do not like you You are a nee...  ...              0\u001b[0m\n",
      "\u001b[34m9                            Yup   Egg nog trash rap  ...              0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34mto_int completed!\u001b[0m\n",
      "\u001b[34m==================================================\u001b[0m\n",
      "\u001b[34msplit_data_input: dataframe:                                                tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m0                                  Zuko gets bitches  ...              0\u001b[0m\n",
      "\u001b[34m1             Zimmerman we comin for yo life bitch    ...              0\u001b[0m\n",
      "\u001b[34m2  Zhou Mi was just layin on his bed and SM just ...  ...              0\u001b[0m\n",
      "\u001b[34m3                               Zelda bitches lol     ...              0\u001b[0m\n",
      "\u001b[34m4         Zack still questions my love for Oreos lol  ...              0\u001b[0m\n",
      "\u001b[34m5    Zach Huggins tell yo bitch to leave me tf alone  ...              0\u001b[0m\n",
      "\u001b[34m6  Yuu Hinouchi gets fucked by two guys until she...  ...              0\u001b[0m\n",
      "\u001b[34m7  Yup   Then she got AIDs and died   Jenny was a...  ...              0\u001b[0m\n",
      "\u001b[34m8  Yup I officially do not like you You are a nee...  ...              0\u001b[0m\n",
      "\u001b[34m9                            Yup   Egg nog trash rap  ...              0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34msplit_data_output: train_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m9200         Floyd Mayweather stay with a badd bitch lol  ...              0\u001b[0m\n",
      "\u001b[34m8199     Allen West amp The Slippery slope of Obamas ...  ...              0\u001b[0m\n",
      "\u001b[34m18104  ReasonsWeCantBeTogether if u was talkin to my ...  ...              0\u001b[0m\n",
      "\u001b[34m17022  Bitches love beefing over a nigga thats always...  ...              0\u001b[0m\n",
      "\u001b[34m15176                               Haha bitch ass gucci  ...              0\u001b[0m\n",
      "\u001b[34m6827                              Ross pulling hoes       ...              0\u001b[0m\n",
      "\u001b[34m12824  If you rat on your family just so you dont get...  ...              0\u001b[0m\n",
      "\u001b[34m8861     i hate when bitches be like bitch what u tho...  ...              0\u001b[0m\n",
      "\u001b[34m11764  Lmfaooo Two bad bitches giving me head at the ...  ...              0\u001b[0m\n",
      "\u001b[34m3555                  sick and watching the Yankees game  ...              0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34msplit_data_output: val_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m5654                                     Undercover hoes  ...              0\u001b[0m\n",
      "\u001b[34m20681   I popped a Molly every day last year beatthat...  ...              0\u001b[0m\n",
      "\u001b[34m15496  Fuhk you you hoe I dont want you back classic ...  ...              0\u001b[0m\n",
      "\u001b[34m5130     For niggas who claim to love pussy as much a...  ...              0\u001b[0m\n",
      "\u001b[34m8963     naw no bs thoif u under lets say  and u got ...  ...              0\u001b[0m\n",
      "\u001b[34m21059   I feel that i see nothing wrong with the gta ...  ...              0\u001b[0m\n",
      "\u001b[34m2411   These infected ass hoes think its okay to talk...  ...              0\u001b[0m\n",
      "\u001b[34m18673   You little stupid ass bitch i aint fuckin wit...  ...              0\u001b[0m\n",
      "\u001b[34m24442                      haha yolo did ya hit that hoe  ...              0\u001b[0m\n",
      "\u001b[34m15404  Girl dont hide that pussy you should be the ty...  ...              0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34msplit_data completed!\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mlabel_cols_input: dataframe:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m9200         Floyd Mayweather stay with a badd bitch lol  ...              0\u001b[0m\n",
      "\u001b[34m8199     Allen West amp The Slippery slope of Obamas ...  ...              0\u001b[0m\n",
      "\u001b[34m18104  ReasonsWeCantBeTogether if u was talkin to my ...  ...              0\u001b[0m\n",
      "\u001b[34m17022  Bitches love beefing over a nigga thats always...  ...              0\u001b[0m\n",
      "\u001b[34m15176                               Haha bitch ass gucci  ...              0\u001b[0m\n",
      "\u001b[34m6827                              Ross pulling hoes       ...              0\u001b[0m\n",
      "\u001b[34m12824  If you rat on your family just so you dont get...  ...              0\u001b[0m\n",
      "\u001b[34m8861     i hate when bitches be like bitch what u tho...  ...              0\u001b[0m\n",
      "\u001b[34m11764  Lmfaooo Two bad bitches giving me head at the ...  ...              0\u001b[0m\n",
      "\u001b[34m3555                  sick and watching the Yankees game  ...              0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34mlabel_cols_output: ['general criticsm', 'disability shaming', 'racial prejudice', 'sexism', 'lgbtq+ phobia']\u001b[0m\n",
      "\u001b[34mlabel_cols completed!\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 39.8kB/s]\u001b[0m\n",
      "\u001b[32mDownloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 32.0kB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/208k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading:  17%|█▋        | 36.0k/208k [00:00<00:00, 344kB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  87%|████████▋ | 182k/208k [00:00<00:00, 847kB/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 208k/208k [00:00<00:00, 809kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/208k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 37.0k/208k [00:00<00:00, 274kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 192k/208k [00:00<00:00, 895kB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 208k/208k [00:00<00:00, 854kB/s]\u001b[0m\n",
      "\u001b[32mDownloading:   0%|          | 0.00/208k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading:  40%|███▉      | 83.0k/208k [00:00<00:00, 642kB/s]\u001b[0m\n",
      "\u001b[32mDownloading: 100%|██████████| 208k/208k [00:00<00:00, 1.21MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:   0%|          | 0.00/426k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/426k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading:   8%|▊         | 36.0k/426k [00:00<00:01, 274kB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  42%|████▏     | 180k/426k [00:00<00:00, 847kB/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 426k/426k [00:00<00:00, 1.29MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/426k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 24.0k/426k [00:00<00:01, 229kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 196k/426k [00:00<00:00, 818kB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 426k/426k [00:00<00:00, 1.29MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  19%|█▉        | 80.0k/426k [00:00<00:00, 751kB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  94%|█████████▍| 400k/426k [00:00<00:00, 1.87MB/s]\u001b[0m\n",
      "\u001b[32mDownloading: 100%|██████████| 426k/426k [00:00<00:00, 1.82MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 570/570 [00:00<00:00, 446kB/s]\u001b[0m\n",
      "\u001b[35mcreate_tokenizer_output: tokenizer: PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\u001b[0m\n",
      "\u001b[35mcreate_tokenizer completed!\u001b[0m\n",
      "\u001b[35m====================================================================================================\u001b[0m\n",
      "\u001b[35mcreate_data_module_input: train_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m13812  I need a Spanish WOMAN They dont have ghetto n...  ...              0\u001b[0m\n",
      "\u001b[35m409            you a pussy ass nigga and I know it nigga  ...              0\u001b[0m\n",
      "\u001b[35m9253     I feel bad for guys that give their girlfrie...  ...              0\u001b[0m\n",
      "\u001b[35m23726   Huhlast  gamesTampa Balt Yanks and they lost ...  ...              0\u001b[0m\n",
      "\u001b[35m3535                 Since you hoes wanna be childish     ...              0\u001b[0m\n",
      "\u001b[35m6570                       Amo a esa gente que me odia    ...              0\u001b[0m\n",
      "\u001b[35m4628     I wonder how many of yall hoes got homegrown...  ...              0\u001b[0m\n",
      "\u001b[35m21052                   Im already out that bitch hahaha  ...              0\u001b[0m\n",
      "\u001b[35m5250                            J Cole aint trash Period  ...              0\u001b[0m\n",
      "\u001b[35m222    You know the head good when it feel like she h...  ...              0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35mcreate_data_module_input: val_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[35m17571  Alright ya faggots Lets just stop with all the...  ...              1\u001b[0m\n",
      "\u001b[35m17221  Because of the message that sends to LGBTQ res...  ...              1\u001b[0m\n",
      "\u001b[35m23367                               choke on bread bitch  ...              0\u001b[0m\n",
      "\u001b[35m1066   Which one of you hoes then downloaded that Jai...  ...              0\u001b[0m\n",
      "\u001b[35m21038                   big fucking dealcry baby niggers  ...              0\u001b[0m\n",
      "\u001b[35m12691  Ima kill cay  ugh she stay on my last nerve bu...  ...              0\u001b[0m\n",
      "\u001b[35m3387   So Sterling bitch gotta he a hoe aye she paid ...  ...              0\u001b[0m\n",
      "\u001b[35m22681     Altyd zo geweest hoe vaak men ook zegtdit n...  ...              0\u001b[0m\n",
      "\u001b[35m3042   Talk about me behind my back I hate that pussy...  ...              0\u001b[0m\n",
      "\u001b[35m8509     Making homemade pickles and working on a fun...  ...              0\u001b[0m\n",
      "\u001b[35m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[35mcreate_data_module_output: data_module: <create_model.TweetsDataModule object at 0x7f1b718eae20>\u001b[0m\n",
      "\u001b[35mcreate_data_module completed!\u001b[0m\n",
      "\u001b[35m====================================================================================================\u001b[0m\n",
      "\u001b[35mwarmup_and_totaltraining_steps completed!\u001b[0m\n",
      "\u001b[35m====================================================================================================\u001b[0m\n",
      "\u001b[32mDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading: 100%|██████████| 570/570 [00:00<00:00, 749kB/s]\u001b[0m\n",
      "\u001b[32mcreate_tokenizer_output: tokenizer: PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\u001b[0m\n",
      "\u001b[32mcreate_tokenizer completed!\u001b[0m\n",
      "\u001b[32m====================================================================================================\u001b[0m\n",
      "\u001b[32mcreate_data_module_input: train_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m8501              before I betray them I slit my wrist    ...              0\u001b[0m\n",
      "\u001b[32m23880   Youre killin me man lt Not all conservatives ...  ...              0\u001b[0m\n",
      "\u001b[32m18909   I just wana eat some food Drink some liq Go t...  ...              0\u001b[0m\n",
      "\u001b[32m2163                                  This fuckin bird    ...              0\u001b[0m\n",
      "\u001b[32m8979           Weed aint a drug Its a fucking herb bitch  ...              0\u001b[0m\n",
      "\u001b[32m4985       all deze hoes got past demons in em YOLO n...  ...              0\u001b[0m\n",
      "\u001b[32m15926  Every nigga tryna fuck the next nigga bitch no...  ...              0\u001b[0m\n",
      "\u001b[32m4591                             Mary Jane my only bitch  ...              0\u001b[0m\n",
      "\u001b[32m5426      Women would put up with very little love as...  ...              0\u001b[0m\n",
      "\u001b[32m6406                          Basic bitch starter pack    ...              0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32mcreate_data_module_input: val_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[32m16325                    David is bitch made HeIsMyBitch  ...              0\u001b[0m\n",
      "\u001b[32m23979                           how original you faggots  ...              1\u001b[0m\n",
      "\u001b[32m6004     Bad bitch HTown keep it trill yall know yall...  ...              0\u001b[0m\n",
      "\u001b[32m5656     Dont message me your number and tell me to t...  ...              0\u001b[0m\n",
      "\u001b[32m5621           So proud of mahanain  she still a hoe tho  ...              0\u001b[0m\n",
      "\u001b[32m6023     Yall we use to have to take pictures on real...  ...              0\u001b[0m\n",
      "\u001b[32m22312   ok Ill sink to your level youre a special kin...  ...              0\u001b[0m\n",
      "\u001b[32m6704     I only wear weave because I like it  A bitch...  ...              0\u001b[0m\n",
      "\u001b[32m22784    puppets like Don will intentionally and fore...  ...              0\u001b[0m\n",
      "\u001b[32m8397     Your iPad raised your kid to be a giant puss...  ...              0\u001b[0m\n",
      "\u001b[32m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[32mcreate_data_module_output: data_module: <create_model.TweetsDataModule object at 0x7fee71f9be20>\u001b[0m\n",
      "\u001b[32mcreate_data_module completed!\u001b[0m\n",
      "\u001b[32m====================================================================================================\u001b[0m\n",
      "\u001b[32mwarmup_and_totaltraining_steps completed!\u001b[0m\n",
      "\u001b[32m====================================================================================================\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/416M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading:   1%|▏         | 5.49M/416M [00:00<00:07, 57.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   3%|▎         | 11.0M/416M [00:00<00:08, 47.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   4%|▍         | 16.6M/416M [00:00<00:08, 51.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 570/570 [00:00<00:00, 756kB/s]\u001b[0m\n",
      "\u001b[34mcreate_tokenizer_output: tokenizer: PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\u001b[0m\n",
      "\u001b[34mcreate_tokenizer completed!\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mcreate_data_module_input: train_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m9200         Floyd Mayweather stay with a badd bitch lol  ...              0\u001b[0m\n",
      "\u001b[34m8199     Allen West amp The Slippery slope of Obamas ...  ...              0\u001b[0m\n",
      "\u001b[34m18104  ReasonsWeCantBeTogether if u was talkin to my ...  ...              0\u001b[0m\n",
      "\u001b[34m17022  Bitches love beefing over a nigga thats always...  ...              0\u001b[0m\n",
      "\u001b[34m15176                               Haha bitch ass gucci  ...              0\u001b[0m\n",
      "\u001b[34m6827                              Ross pulling hoes       ...              0\u001b[0m\n",
      "\u001b[34m12824  If you rat on your family just so you dont get...  ...              0\u001b[0m\n",
      "\u001b[34m8861     i hate when bitches be like bitch what u tho...  ...              0\u001b[0m\n",
      "\u001b[34m11764  Lmfaooo Two bad bitches giving me head at the ...  ...              0\u001b[0m\n",
      "\u001b[34m3555                  sick and watching the Yankees game  ...              0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34mcreate_data_module_input: val_df:                                                    tweet  ...  lgbtq+ phobia\u001b[0m\n",
      "\u001b[34m5654                                     Undercover hoes  ...              0\u001b[0m\n",
      "\u001b[34m20681   I popped a Molly every day last year beatthat...  ...              0\u001b[0m\n",
      "\u001b[34m15496  Fuhk you you hoe I dont want you back classic ...  ...              0\u001b[0m\n",
      "\u001b[34m5130     For niggas who claim to love pussy as much a...  ...              0\u001b[0m\n",
      "\u001b[34m8963     naw no bs thoif u under lets say  and u got ...  ...              0\u001b[0m\n",
      "\u001b[34m21059   I feel that i see nothing wrong with the gta ...  ...              0\u001b[0m\n",
      "\u001b[34m2411   These infected ass hoes think its okay to talk...  ...              0\u001b[0m\n",
      "\u001b[34m18673   You little stupid ass bitch i aint fuckin wit...  ...              0\u001b[0m\n",
      "\u001b[34m24442                      haha yolo did ya hit that hoe  ...              0\u001b[0m\n",
      "\u001b[34m15404  Girl dont hide that pussy you should be the ty...  ...              0\u001b[0m\n",
      "\u001b[34m[10 rows x 6 columns]\u001b[0m\n",
      "\u001b[34mcreate_data_module_output: data_module: <create_model.TweetsDataModule object at 0x7f09bf5f9e20>\u001b[0m\n",
      "\u001b[34mcreate_data_module completed!\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mwarmup_and_totaltraining_steps completed!\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/416M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading:   0%|          | 0.00/416M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading:   2%|▏         | 7.53M/416M [00:00<00:05, 78.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:   4%|▍         | 17.4M/416M [00:00<00:04, 93.1MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:   7%|▋         | 27.2M/416M [00:00<00:04, 97.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:   9%|▉         | 37.2M/416M [00:00<00:03, 101MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  11%|█▏        | 46.8M/416M [00:00<00:04, 93.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   5%|▌         | 21.6M/416M [00:00<00:09, 44.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   6%|▋         | 26.0M/416M [00:00<00:11, 35.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   8%|▊         | 32.5M/416M [00:00<00:09, 43.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   9%|▉         | 37.2M/416M [00:00<00:08, 44.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  10%|█         | 42.1M/416M [00:00<00:08, 46.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  11%|█▏        | 46.8M/416M [00:01<00:08, 46.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  13%|█▎        | 52.5M/416M [00:01<00:07, 50.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  14%|█▍        | 57.4M/416M [00:01<00:08, 46.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 7.86M/416M [00:00<00:05, 82.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 18.0M/416M [00:00<00:04, 96.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 28.1M/416M [00:00<00:04, 101MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 38.2M/416M [00:00<00:03, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 48.3M/416M [00:00<00:03, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 58.3M/416M [00:00<00:03, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▋        | 68.5M/416M [00:00<00:03, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 78.5M/416M [00:00<00:03, 94.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 87.7M/416M [00:00<00:03, 86.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  13%|█▎        | 55.8M/416M [00:00<00:05, 69.1MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  15%|█▌        | 63.2M/416M [00:00<00:05, 66.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  17%|█▋        | 70.1M/416M [00:01<00:06, 54.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  18%|█▊        | 75.9M/416M [00:01<00:06, 56.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  20%|█▉        | 81.7M/416M [00:01<00:06, 55.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  21%|██▏       | 89.0M/416M [00:01<00:05, 60.7MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  23%|██▎       | 95.2M/416M [00:01<00:05, 61.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  24%|██▍       | 101M/416M [00:01<00:05, 59.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  15%|█▌        | 63.2M/416M [00:01<00:07, 49.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  16%|█▋        | 68.1M/416M [00:01<00:08, 43.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  18%|█▊        | 73.8M/416M [00:01<00:07, 47.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  19%|█▉        | 78.6M/416M [00:01<00:07, 48.1MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  21%|██        | 85.5M/416M [00:01<00:06, 54.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  22%|██▏       | 92.1M/416M [00:01<00:05, 58.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  24%|██▎       | 97.8M/416M [00:02<00:05, 58.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  25%|██▍       | 104M/416M [00:02<00:06, 51.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  26%|██▌       | 109M/416M [00:02<00:06, 52.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 96.1M/416M [00:01<00:04, 78.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 104M/416M [00:01<00:05, 63.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 110M/416M [00:01<00:05, 58.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 116M/416M [00:01<00:05, 58.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 122M/416M [00:01<00:05, 55.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 128M/416M [00:01<00:05, 57.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 134M/416M [00:01<00:05, 57.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▎      | 139M/416M [00:01<00:05, 57.5MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  26%|██▌       | 107M/416M [00:01<00:06, 52.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  27%|██▋       | 112M/416M [00:01<00:06, 50.2MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  28%|██▊       | 118M/416M [00:01<00:06, 51.1MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  29%|██▉       | 123M/416M [00:02<00:06, 50.6MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  31%|███       | 128M/416M [00:02<00:05, 51.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  32%|███▏      | 134M/416M [00:02<00:05, 55.6MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  34%|███▎      | 140M/416M [00:02<00:05, 54.7MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  35%|███▍      | 145M/416M [00:02<00:05, 50.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  27%|██▋       | 114M/416M [00:02<00:06, 51.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  29%|██▊       | 119M/416M [00:02<00:06, 50.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  30%|██▉       | 124M/416M [00:02<00:05, 51.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  31%|███       | 130M/416M [00:02<00:05, 53.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  33%|███▎      | 135M/416M [00:02<00:05, 54.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  34%|███▍      | 141M/416M [00:02<00:05, 54.1MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  35%|███▌      | 146M/416M [00:03<00:05, 47.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  36%|███▌      | 151M/416M [00:03<00:06, 44.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  37%|███▋      | 155M/416M [00:03<00:06, 43.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 145M/416M [00:02<00:05, 52.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 150M/416M [00:02<00:06, 43.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 155M/416M [00:02<00:06, 45.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 159M/416M [00:02<00:05, 45.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 166M/416M [00:02<00:05, 51.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 171M/416M [00:02<00:05, 50.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 178M/416M [00:02<00:04, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 184M/416M [00:02<00:04, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 191M/416M [00:03<00:03, 61.3MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  36%|███▌      | 150M/416M [00:02<00:06, 43.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  37%|███▋      | 155M/416M [00:02<00:06, 44.7MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  38%|███▊      | 159M/416M [00:02<00:05, 45.1MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  40%|███▉      | 166M/416M [00:02<00:05, 51.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  41%|████      | 171M/416M [00:03<00:05, 49.0MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  43%|████▎     | 179M/416M [00:03<00:04, 59.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  45%|████▍     | 185M/416M [00:03<00:04, 59.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  46%|████▌     | 191M/416M [00:03<00:03, 61.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  47%|████▋     | 197M/416M [00:03<00:04, 50.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  39%|███▊      | 161M/416M [00:03<00:05, 47.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  40%|████      | 167M/416M [00:03<00:04, 52.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  41%|████▏     | 172M/416M [00:03<00:04, 52.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  43%|████▎     | 180M/416M [00:03<00:04, 59.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  45%|████▍     | 186M/416M [00:03<00:04, 59.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  46%|████▌     | 192M/416M [00:03<00:03, 60.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  48%|████▊     | 197M/416M [00:04<00:04, 48.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  49%|████▉     | 203M/416M [00:04<00:04, 50.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  50%|█████     | 208M/416M [00:04<00:04, 50.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 196M/416M [00:03<00:04, 50.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▊     | 202M/416M [00:03<00:04, 50.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 207M/416M [00:03<00:04, 51.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 212M/416M [00:03<00:04, 52.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 217M/416M [00:03<00:04, 51.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 224M/416M [00:03<00:03, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 229M/416M [00:03<00:04, 47.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▋    | 234M/416M [00:04<00:05, 37.6MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  49%|████▊     | 203M/416M [00:03<00:04, 50.1MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  50%|█████     | 208M/416M [00:03<00:04, 51.3MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  51%|█████     | 213M/416M [00:03<00:04, 51.7MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  52%|█████▏    | 218M/416M [00:04<00:04, 50.5MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  54%|█████▍    | 224M/416M [00:04<00:03, 54.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  55%|█████▌    | 230M/416M [00:04<00:04, 45.3MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  56%|█████▋    | 234M/416M [00:04<00:05, 38.0MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  57%|█████▋    | 238M/416M [00:04<00:05, 34.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  51%|█████▏    | 214M/416M [00:04<00:03, 53.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  53%|█████▎    | 219M/416M [00:04<00:03, 52.1MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  54%|█████▍    | 225M/416M [00:04<00:03, 55.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  55%|█████▌    | 230M/416M [00:04<00:04, 42.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  57%|█████▋    | 235M/416M [00:05<00:04, 38.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  58%|█████▊    | 239M/416M [00:05<00:05, 34.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  59%|█████▉    | 245M/416M [00:05<00:04, 40.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 238M/416M [00:04<00:05, 33.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 244M/416M [00:04<00:04, 40.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 251M/416M [00:04<00:03, 46.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 256M/416M [00:04<00:03, 47.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 261M/416M [00:04<00:03, 46.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 267M/416M [00:04<00:03, 49.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 272M/416M [00:04<00:02, 50.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 278M/416M [00:04<00:02, 54.2MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  59%|█████▊    | 244M/416M [00:04<00:04, 39.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  60%|██████    | 250M/416M [00:04<00:03, 46.1MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  61%|██████▏   | 255M/416M [00:04<00:03, 43.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  63%|██████▎   | 261M/416M [00:05<00:03, 47.7MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  64%|██████▍   | 267M/416M [00:05<00:03, 50.7MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  65%|██████▌   | 272M/416M [00:05<00:02, 51.2MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  67%|██████▋   | 278M/416M [00:05<00:02, 54.3MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  68%|██████▊   | 283M/416M [00:05<00:02, 48.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  69%|██████▉   | 288M/416M [00:05<00:02, 48.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  60%|██████    | 251M/416M [00:05<00:03, 46.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  62%|██████▏   | 256M/416M [00:05<00:03, 47.1MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  63%|██████▎   | 261M/416M [00:05<00:03, 46.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  64%|██████▍   | 267M/416M [00:05<00:03, 49.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  65%|██████▌   | 272M/416M [00:05<00:02, 50.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  67%|██████▋   | 278M/416M [00:05<00:02, 54.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  68%|██████▊   | 283M/416M [00:06<00:02, 47.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  69%|██████▉   | 289M/416M [00:06<00:02, 49.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  71%|███████   | 294M/416M [00:06<00:02, 43.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 283M/416M [00:05<00:02, 48.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 288M/416M [00:05<00:02, 49.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 293M/416M [00:05<00:02, 44.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 298M/416M [00:05<00:02, 43.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 302M/416M [00:05<00:03, 33.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 306M/416M [00:05<00:03, 35.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 310M/416M [00:05<00:03, 34.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 313M/416M [00:06<00:03, 34.0MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  70%|███████   | 293M/416M [00:05<00:02, 45.1MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  72%|███████▏  | 298M/416M [00:05<00:02, 43.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  73%|███████▎  | 302M/416M [00:06<00:03, 33.7MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  74%|███████▎  | 306M/416M [00:06<00:03, 36.0MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  75%|███████▍  | 310M/416M [00:06<00:03, 34.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  75%|███████▌  | 313M/416M [00:06<00:03, 34.3MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  77%|███████▋  | 319M/416M [00:06<00:02, 39.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  72%|███████▏  | 298M/416M [00:06<00:02, 43.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  73%|███████▎  | 302M/416M [00:06<00:03, 34.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  74%|███████▎  | 306M/416M [00:06<00:03, 36.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  75%|███████▍  | 310M/416M [00:06<00:03, 35.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  76%|███████▌  | 314M/416M [00:06<00:03, 34.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  77%|███████▋  | 319M/416M [00:07<00:02, 38.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  78%|███████▊  | 325M/416M [00:07<00:02, 45.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  80%|███████▉  | 331M/416M [00:07<00:01, 50.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 319M/416M [00:06<00:02, 39.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 324M/416M [00:06<00:02, 44.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 330M/416M [00:06<00:01, 50.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 336M/416M [00:06<00:01, 52.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 342M/416M [00:06<00:01, 56.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 349M/416M [00:06<00:01, 59.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 355M/416M [00:06<00:01, 55.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 360M/416M [00:06<00:01, 56.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 365M/416M [00:06<00:00, 54.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 371M/416M [00:07<00:00, 56.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  78%|███████▊  | 324M/416M [00:06<00:02, 44.1MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  79%|███████▉  | 330M/416M [00:06<00:01, 50.0MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  81%|████████  | 336M/416M [00:06<00:01, 52.6MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  82%|████████▏ | 342M/416M [00:06<00:01, 56.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  84%|████████▍ | 349M/416M [00:07<00:01, 60.0MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  85%|████████▌ | 355M/416M [00:07<00:01, 57.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  87%|████████▋ | 360M/416M [00:07<00:01, 55.3MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  88%|████████▊ | 366M/416M [00:07<00:00, 54.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  89%|████████▉ | 371M/416M [00:07<00:00, 56.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  81%|████████  | 336M/416M [00:07<00:01, 52.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  83%|████████▎ | 343M/416M [00:07<00:01, 56.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  84%|████████▍ | 349M/416M [00:07<00:01, 60.1MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  85%|████████▌ | 355M/416M [00:07<00:01, 57.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  87%|████████▋ | 361M/416M [00:07<00:01, 55.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  88%|████████▊ | 366M/416M [00:07<00:00, 54.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  89%|████████▉ | 372M/416M [00:08<00:00, 55.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  91%|█████████ | 377M/416M [00:08<00:00, 40.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 377M/416M [00:07<00:00, 43.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 381M/416M [00:07<00:00, 36.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 387M/416M [00:07<00:00, 40.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 392M/416M [00:07<00:00, 42.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 396M/416M [00:07<00:00, 43.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 401M/416M [00:07<00:00, 45.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 406M/416M [00:08<00:00, 46.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  91%|█████████ | 377M/416M [00:07<00:00, 42.6MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  92%|█████████▏| 381M/416M [00:07<00:00, 36.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  93%|█████████▎| 387M/416M [00:07<00:00, 40.7MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  94%|█████████▍| 392M/416M [00:08<00:00, 41.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  95%|█████████▌| 397M/416M [00:08<00:00, 43.6MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  97%|█████████▋| 401M/416M [00:08<00:00, 45.3MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  98%|█████████▊| 406M/416M [00:08<00:00, 46.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading:  99%|█████████▉| 411M/416M [00:08<00:00, 37.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  92%|█████████▏| 382M/416M [00:08<00:00, 36.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  93%|█████████▎| 387M/416M [00:08<00:00, 41.1MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  94%|█████████▍| 392M/416M [00:08<00:00, 42.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  95%|█████████▌| 396M/416M [00:08<00:00, 42.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  96%|█████████▋| 401M/416M [00:08<00:00, 43.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  98%|█████████▊| 406M/416M [00:08<00:00, 47.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  99%|█████████▉| 411M/416M [00:09<00:00, 38.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|█████████▉| 415M/416M [00:09<00:00, 38.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 416M/416M [00:09<00:00, 47.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 411M/416M [00:08<00:00, 37.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 415M/416M [00:08<00:00, 38.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 416M/416M [00:08<00:00, 52.4MB/s]\u001b[0m\n",
      "\u001b[32mDownloading: 100%|█████████▉| 415M/416M [00:08<00:00, 37.9MB/s]\u001b[0m\n",
      "\u001b[32mDownloading: 100%|██████████| 416M/416M [00:08<00:00, 50.1MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[32m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[32m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[35mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[35m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[35m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f09c9b06eb0>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f09c9b06eb0>)`.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mMissing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fee7c4aaeb0>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fee7c4aaeb0>)`.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[32mGPU available: True, used: True\u001b[0m\n",
      "\u001b[32mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[32mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[32mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[32mMissing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f1b7bdfbeb0>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f1b7bdfbeb0>)`.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[35mGPU available: True, used: True\u001b[0m\n",
      "\u001b[35mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[35mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[35mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[35mMissing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[32mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[32m| Name       | Type      | Params\u001b[0m\n",
      "\u001b[32m-----------------------------------------\u001b[0m\n",
      "\u001b[32m0 | bert       | BertModel | 108 M \u001b[0m\n",
      "\u001b[32m1 | classifier | Linear    | 3.8 K \u001b[0m\n",
      "\u001b[32m2 | criterion  | BCELoss   | 0     \u001b[0m\n",
      "\u001b[32m-----------------------------------------\u001b[0m\n",
      "\u001b[32m108 M     Trainable params\u001b[0m\n",
      "\u001b[32m0         Non-trainable params\u001b[0m\n",
      "\u001b[32m108 M     Total params\u001b[0m\n",
      "\u001b[32m433.256   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[32mSanity Checking: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m| Name       | Type      | Params\u001b[0m\n",
      "\u001b[34m-----------------------------------------\u001b[0m\n",
      "\u001b[34m0 | bert       | BertModel | 108 M \u001b[0m\n",
      "\u001b[34m1 | classifier | Linear    | 3.8 K \u001b[0m\n",
      "\u001b[34m2 | criterion  | BCELoss   | 0     \u001b[0m\n",
      "\u001b[34m-----------------------------------------\u001b[0m\n",
      "\u001b[34m108 M     Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m108 M     Total params\u001b[0m\n",
      "\u001b[34m433.256   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34mSanity Checking: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.263 algo-1:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:50.504 algo-3:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:50.633 algo-3:57 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:50.634 algo-3:57 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:50.634 algo-3:57 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:50.635 algo-3:57 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:50.635 algo-3:57 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[32mSanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[32mSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.176 algo-3:57 INFO hook.py:560] name:bert.embeddings.word_embeddings.weight count_params:22268928\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.177 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.178 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.179 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.180 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.181 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.182 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:classifier.weight count_params:3840\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:560] name:classifier.bias count_params:5\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:562] Total Trainable Params: 108314117\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.183 algo-3:57 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[32m[2022-07-19 05:45:51.184 algo-3:57 INFO hook.py:485] Hook is writing from the hook with pid: 57\u001b[0m\n",
      "\u001b[32mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 10.96it/s]\u001b[0m\n",
      "\u001b[32mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 10.96it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.394 algo-1:57 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.396 algo-1:57 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.396 algo-1:57 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.396 algo-1:57 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.396 algo-1:57 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mSanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.embeddings.word_embeddings.weight count_params:22268928\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.770 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.771 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.772 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.773 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.774 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:classifier.weight count_params:3840\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:560] name:classifier.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:562] Total Trainable Params: 108314117\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.775 algo-1:57 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-07-19 05:45:51.776 algo-1:57 INFO hook.py:485] Hook is writing from the hook with pid: 57\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 11.23it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 11.23it/s]\u001b[0m\n",
      "\u001b[34mTraining: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining:   0%|          | 0/727 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/727 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[32mTraining: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[32mTraining:   0%|          | 0/727 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[32mEpoch 0:   0%|          | 0/727 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m| Name       | Type      | Params\u001b[0m\n",
      "\u001b[35m-----------------------------------------\u001b[0m\n",
      "\u001b[35m0 | bert       | BertModel | 108 M \u001b[0m\n",
      "\u001b[35m1 | classifier | Linear    | 3.8 K \u001b[0m\n",
      "\u001b[35m2 | criterion  | BCELoss   | 0     \u001b[0m\n",
      "\u001b[35m-----------------------------------------\u001b[0m\n",
      "\u001b[35m108 M     Trainable params\u001b[0m\n",
      "\u001b[35m0         Non-trainable params\u001b[0m\n",
      "\u001b[35m108 M     Total params\u001b[0m\n",
      "\u001b[35m433.256   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[35mSanity Checking: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:52.696 algo-2:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220716-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:52.828 algo-2:57 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:52.829 algo-2:57 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:52.829 algo-2:57 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:52.830 algo-2:57 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:52.830 algo-2:57 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35mSanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.embeddings.word_embeddings.weight count_params:22268928\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.212 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.213 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.214 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.215 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.216 algo-2:57 INFO hook.py:560] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.217 algo-2:57 INFO hook.py:560] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.217 algo-2:57 INFO hook.py:560] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.217 algo-2:57 INFO hook.py:560] name:classifier.weight count_params:3840\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.217 algo-2:57 INFO hook.py:560] name:classifier.bias count_params:5\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.217 algo-2:57 INFO hook.py:562] Total Trainable Params: 108314117\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.217 algo-2:57 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35m[2022-07-19 05:45:53.218 algo-2:57 INFO hook.py:485] Hook is writing from the hook with pid: 57\u001b[0m\n",
      "\u001b[35mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 11.82it/s]\u001b[0m\n",
      "\u001b[35mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 11.82it/s]\u001b[0m\n",
      "\u001b[35mTraining: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining:   0%|          | 0/727 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mEpoch 0:   0%|          | 0/727 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[32mEpoch 0:   4%|▍         | 30/727 [00:08<03:25,  3.39it/s]\u001b[0m\n",
      "\u001b[32mEpoch 0:   4%|▍         | 30/727 [00:08<03:25,  3.39it/s]\u001b[0m\n",
      "\u001b[32mEpoch 0:   4%|▍         | 30/727 [00:08<03:25,  3.39it/s, loss=0.751, v_num=0, train_loss=0.729]\u001b[0m\n",
      "\u001b[34mEpoch 0:   4%|▍         | 30/727 [00:08<03:20,  3.48it/s]#015Epoch 0:   4%|▍         | 30/727 [00:08<03:20,  3.48it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   4%|▍         | 30/727 [00:08<03:20,  3.47it/s, loss=0.695, v_num=0, train_loss=0.680]\u001b[0m\n",
      "\u001b[35mEpoch 0:   4%|▍         | 30/727 [00:08<03:25,  3.40it/s]\u001b[0m\n",
      "\u001b[35mEpoch 0:   4%|▍         | 30/727 [00:08<03:25,  3.40it/s]\u001b[0m\n",
      "\u001b[35mEpoch 0:   4%|▍         | 30/727 [00:08<03:25,  3.39it/s, loss=0.639, v_num=0, train_loss=0.617]\u001b[0m\n",
      "\u001b[34mEpoch 0:   8%|▊         | 60/727 [00:17<03:09,  3.53it/s, loss=0.695, v_num=0, train_loss=0.680]#015Epoch 0:   8%|▊         | 60/727 [00:17<03:09,  3.53it/s, loss=0.695, v_num=0, train_loss=0.680]\u001b[0m\n",
      "\u001b[34mEpoch 0:   8%|▊         | 60/727 [00:17<03:09,  3.52it/s, loss=0.656, v_num=0, train_loss=0.635]\u001b[0m\n",
      "\u001b[32mEpoch 0:   8%|▊         | 60/727 [00:17<03:13,  3.44it/s, loss=0.751, v_num=0, train_loss=0.729]\u001b[0m\n",
      "\u001b[32mEpoch 0:   8%|▊         | 60/727 [00:17<03:13,  3.44it/s, loss=0.751, v_num=0, train_loss=0.729]\u001b[0m\n",
      "\u001b[32mEpoch 0:   8%|▊         | 60/727 [00:17<03:13,  3.44it/s, loss=0.678, v_num=0, train_loss=0.665]\u001b[0m\n",
      "\u001b[35mEpoch 0:   8%|▊         | 60/727 [00:17<03:12,  3.46it/s, loss=0.639, v_num=0, train_loss=0.617]\u001b[0m\n",
      "\u001b[35mEpoch 0:   8%|▊         | 60/727 [00:17<03:12,  3.46it/s, loss=0.639, v_num=0, train_loss=0.617]\u001b[0m\n",
      "\u001b[35mEpoch 0:   8%|▊         | 60/727 [00:17<03:13,  3.45it/s, loss=0.59, v_num=0, train_loss=0.558]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 90/727 [00:25<02:59,  3.55it/s, loss=0.656, v_num=0, train_loss=0.635]#015Epoch 0:  12%|█▏        | 90/727 [00:25<02:59,  3.55it/s, loss=0.656, v_num=0, train_loss=0.635]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 90/727 [00:25<02:59,  3.54it/s, loss=0.556, v_num=0, train_loss=0.525]\u001b[0m\n",
      "\u001b[32mEpoch 0:  12%|█▏        | 90/727 [00:26<03:04,  3.45it/s, loss=0.678, v_num=0, train_loss=0.665]\u001b[0m\n",
      "\u001b[32mEpoch 0:  12%|█▏        | 90/727 [00:26<03:04,  3.45it/s, loss=0.678, v_num=0, train_loss=0.665]\u001b[0m\n",
      "\u001b[32mEpoch 0:  12%|█▏        | 90/727 [00:26<03:04,  3.45it/s, loss=0.567, v_num=0, train_loss=0.529]\u001b[0m\n",
      "\u001b[35mEpoch 0:  12%|█▏        | 90/727 [00:25<03:03,  3.47it/s, loss=0.59, v_num=0, train_loss=0.558]\u001b[0m\n",
      "\u001b[35mEpoch 0:  12%|█▏        | 90/727 [00:25<03:03,  3.47it/s, loss=0.59, v_num=0, train_loss=0.558]\u001b[0m\n",
      "\u001b[35mEpoch 0:  12%|█▏        | 90/727 [00:25<03:03,  3.47it/s, loss=0.498, v_num=0, train_loss=0.447]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 120/727 [00:33<02:50,  3.55it/s, loss=0.556, v_num=0, train_loss=0.525]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 120/727 [00:33<02:50,  3.55it/s, loss=0.556, v_num=0, train_loss=0.525]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 120/727 [00:33<02:51,  3.55it/s, loss=0.428, v_num=0, train_loss=0.397]\u001b[0m\n",
      "\u001b[32mEpoch 0:  17%|█▋        | 120/727 [00:34<02:55,  3.45it/s, loss=0.567, v_num=0, train_loss=0.529]\u001b[0m\n",
      "\u001b[32mEpoch 0:  17%|█▋        | 120/727 [00:34<02:55,  3.45it/s, loss=0.567, v_num=0, train_loss=0.529]\u001b[0m\n",
      "\u001b[32mEpoch 0:  17%|█▋        | 120/727 [00:34<02:55,  3.45it/s, loss=0.424, v_num=0, train_loss=0.367]\u001b[0m\n",
      "\u001b[35mEpoch 0:  17%|█▋        | 120/727 [00:34<02:54,  3.47it/s, loss=0.498, v_num=0, train_loss=0.447]\u001b[0m\n",
      "\u001b[35mEpoch 0:  17%|█▋        | 120/727 [00:34<02:54,  3.47it/s, loss=0.498, v_num=0, train_loss=0.447]\u001b[0m\n",
      "\u001b[35mEpoch 0:  17%|█▋        | 120/727 [00:34<02:54,  3.47it/s, loss=0.386, v_num=0, train_loss=0.340]\u001b[0m\n",
      "\u001b[34mEpoch 0:  21%|██        | 150/727 [00:42<02:42,  3.55it/s, loss=0.428, v_num=0, train_loss=0.397]\u001b[0m\n",
      "\u001b[34mEpoch 0:  21%|██        | 150/727 [00:42<02:42,  3.55it/s, loss=0.428, v_num=0, train_loss=0.397]\u001b[0m\n",
      "\u001b[34mEpoch 0:  21%|██        | 150/727 [00:42<02:42,  3.55it/s, loss=0.328, v_num=0, train_loss=0.303]\u001b[0m\n",
      "\u001b[32mEpoch 0:  21%|██        | 150/727 [00:43<02:47,  3.44it/s, loss=0.424, v_num=0, train_loss=0.367]\u001b[0m\n",
      "\u001b[32mEpoch 0:  21%|██        | 150/727 [00:43<02:47,  3.44it/s, loss=0.424, v_num=0, train_loss=0.367]\u001b[0m\n",
      "\u001b[32mEpoch 0:  21%|██        | 150/727 [00:43<02:47,  3.44it/s, loss=0.317, v_num=0, train_loss=0.309]\u001b[0m\n",
      "\u001b[35mEpoch 0:  21%|██        | 150/727 [00:43<02:46,  3.47it/s, loss=0.386, v_num=0, train_loss=0.340]\u001b[0m\n",
      "\u001b[35mEpoch 0:  21%|██        | 150/727 [00:43<02:46,  3.47it/s, loss=0.386, v_num=0, train_loss=0.340]\u001b[0m\n",
      "\u001b[35mEpoch 0:  21%|██        | 150/727 [00:43<02:46,  3.47it/s, loss=0.293, v_num=0, train_loss=0.301]\u001b[0m\n",
      "\u001b[34mEpoch 0:  25%|██▍       | 180/727 [00:50<02:33,  3.55it/s, loss=0.328, v_num=0, train_loss=0.303]\u001b[0m\n",
      "\u001b[34mEpoch 0:  25%|██▍       | 180/727 [00:50<02:33,  3.55it/s, loss=0.328, v_num=0, train_loss=0.303]\u001b[0m\n",
      "\u001b[34mEpoch 0:  25%|██▍       | 180/727 [00:50<02:34,  3.55it/s, loss=0.244, v_num=0, train_loss=0.215]\u001b[0m\n",
      "\u001b[32mEpoch 0:  25%|██▍       | 180/727 [00:52<02:39,  3.44it/s, loss=0.317, v_num=0, train_loss=0.309]\u001b[0m\n",
      "\u001b[32mEpoch 0:  25%|██▍       | 180/727 [00:52<02:39,  3.44it/s, loss=0.317, v_num=0, train_loss=0.309]\u001b[0m\n",
      "\u001b[32mEpoch 0:  25%|██▍       | 180/727 [00:52<02:39,  3.43it/s, loss=0.258, v_num=0, train_loss=0.252]\u001b[0m\n",
      "\u001b[35mEpoch 0:  25%|██▍       | 180/727 [00:51<02:37,  3.47it/s, loss=0.293, v_num=0, train_loss=0.301]\u001b[0m\n",
      "\u001b[35mEpoch 0:  25%|██▍       | 180/727 [00:51<02:37,  3.47it/s, loss=0.293, v_num=0, train_loss=0.301]\u001b[0m\n",
      "\u001b[35mEpoch 0:  25%|██▍       | 180/727 [00:51<02:37,  3.47it/s, loss=0.238, v_num=0, train_loss=0.207]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 210/727 [00:59<02:25,  3.55it/s, loss=0.244, v_num=0, train_loss=0.215]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 210/727 [00:59<02:25,  3.55it/s, loss=0.244, v_num=0, train_loss=0.215]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 210/727 [00:59<02:25,  3.55it/s, loss=0.186, v_num=0, train_loss=0.170]\u001b[0m\n",
      "\u001b[32mEpoch 0:  29%|██▉       | 210/727 [01:01<02:30,  3.42it/s, loss=0.258, v_num=0, train_loss=0.252]\u001b[0m\n",
      "\u001b[32mEpoch 0:  29%|██▉       | 210/727 [01:01<02:30,  3.42it/s, loss=0.258, v_num=0, train_loss=0.252]\u001b[0m\n",
      "\u001b[32mEpoch 0:  29%|██▉       | 210/727 [01:01<02:31,  3.42it/s, loss=0.226, v_num=0, train_loss=0.246]\u001b[0m\n",
      "\u001b[35mEpoch 0:  29%|██▉       | 210/727 [01:00<02:29,  3.47it/s, loss=0.238, v_num=0, train_loss=0.207]\u001b[0m\n",
      "\u001b[35mEpoch 0:  29%|██▉       | 210/727 [01:00<02:29,  3.47it/s, loss=0.238, v_num=0, train_loss=0.207]\u001b[0m\n",
      "\u001b[35mEpoch 0:  29%|██▉       | 210/727 [01:00<02:29,  3.46it/s, loss=0.185, v_num=0, train_loss=0.193]\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 240/727 [01:07<02:17,  3.55it/s, loss=0.186, v_num=0, train_loss=0.170]\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 240/727 [01:07<02:17,  3.55it/s, loss=0.186, v_num=0, train_loss=0.170]\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 240/727 [01:07<02:17,  3.55it/s, loss=0.165, v_num=0, train_loss=0.135]\u001b[0m\n",
      "\u001b[32mEpoch 0:  33%|███▎      | 240/727 [01:10<02:22,  3.41it/s, loss=0.226, v_num=0, train_loss=0.246]\u001b[0m\n",
      "\u001b[32mEpoch 0:  33%|███▎      | 240/727 [01:10<02:22,  3.41it/s, loss=0.226, v_num=0, train_loss=0.246]\u001b[0m\n",
      "\u001b[32mEpoch 0:  33%|███▎      | 240/727 [01:10<02:22,  3.41it/s, loss=0.182, v_num=0, train_loss=0.134]\u001b[0m\n",
      "\u001b[35mEpoch 0:  33%|███▎      | 240/727 [01:09<02:20,  3.46it/s, loss=0.185, v_num=0, train_loss=0.193]\u001b[0m\n",
      "\u001b[35mEpoch 0:  33%|███▎      | 240/727 [01:09<02:20,  3.46it/s, loss=0.185, v_num=0, train_loss=0.193]\u001b[0m\n",
      "\u001b[35mEpoch 0:  33%|███▎      | 240/727 [01:09<02:20,  3.46it/s, loss=0.174, v_num=0, train_loss=0.138]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 270/727 [01:16<02:08,  3.55it/s, loss=0.165, v_num=0, train_loss=0.135]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 270/727 [01:16<02:08,  3.55it/s, loss=0.165, v_num=0, train_loss=0.135]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 270/727 [01:16<02:08,  3.55it/s, loss=0.132, v_num=0, train_loss=0.122]\u001b[0m\n",
      "\u001b[32mEpoch 0:  37%|███▋      | 270/727 [01:19<02:14,  3.40it/s, loss=0.182, v_num=0, train_loss=0.134]\u001b[0m\n",
      "\u001b[32mEpoch 0:  37%|███▋      | 270/727 [01:19<02:14,  3.40it/s, loss=0.182, v_num=0, train_loss=0.134]\u001b[0m\n",
      "\u001b[32mEpoch 0:  37%|███▋      | 270/727 [01:19<02:14,  3.40it/s, loss=0.147, v_num=0, train_loss=0.116]\u001b[0m\n",
      "\u001b[35mEpoch 0:  37%|███▋      | 270/727 [01:18<02:12,  3.45it/s, loss=0.174, v_num=0, train_loss=0.138]\u001b[0m\n",
      "\u001b[35mEpoch 0:  37%|███▋      | 270/727 [01:18<02:12,  3.45it/s, loss=0.174, v_num=0, train_loss=0.138]\u001b[0m\n",
      "\u001b[35mEpoch 0:  37%|███▋      | 270/727 [01:18<02:12,  3.45it/s, loss=0.129, v_num=0, train_loss=0.115]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 300/727 [01:24<02:00,  3.55it/s, loss=0.132, v_num=0, train_loss=0.122]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 300/727 [01:24<02:00,  3.55it/s, loss=0.132, v_num=0, train_loss=0.122]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 300/727 [01:24<02:00,  3.55it/s, loss=0.1, v_num=0, train_loss=0.121]\u001b[0m\n",
      "\u001b[32mEpoch 0:  41%|████▏     | 300/727 [01:28<02:06,  3.39it/s, loss=0.147, v_num=0, train_loss=0.116]\u001b[0m\n",
      "\u001b[32mEpoch 0:  41%|████▏     | 300/727 [01:28<02:06,  3.39it/s, loss=0.147, v_num=0, train_loss=0.116]\u001b[0m\n",
      "\u001b[32mEpoch 0:  41%|████▏     | 300/727 [01:28<02:06,  3.39it/s, loss=0.125, v_num=0, train_loss=0.135]\u001b[0m\n",
      "\u001b[35mEpoch 0:  41%|████▏     | 300/727 [01:27<02:03,  3.45it/s, loss=0.129, v_num=0, train_loss=0.115]\u001b[0m\n",
      "\u001b[35mEpoch 0:  41%|████▏     | 300/727 [01:27<02:03,  3.45it/s, loss=0.129, v_num=0, train_loss=0.115]\u001b[0m\n",
      "\u001b[35mEpoch 0:  41%|████▏     | 300/727 [01:27<02:03,  3.45it/s, loss=0.113, v_num=0, train_loss=0.113]\u001b[0m\n",
      "\u001b[34mEpoch 0:  45%|████▌     | 330/727 [01:32<01:51,  3.55it/s, loss=0.1, v_num=0, train_loss=0.121]\u001b[0m\n",
      "\u001b[34mEpoch 0:  45%|████▌     | 330/727 [01:32<01:51,  3.55it/s, loss=0.1, v_num=0, train_loss=0.121]\u001b[0m\n",
      "\u001b[34mEpoch 0:  45%|████▌     | 330/727 [01:32<01:51,  3.55it/s, loss=0.0917, v_num=0, train_loss=0.0951]\u001b[0m\n",
      "\u001b[32mEpoch 0:  45%|████▌     | 330/727 [01:37<01:57,  3.37it/s, loss=0.125, v_num=0, train_loss=0.135]\u001b[0m\n",
      "\u001b[32mEpoch 0:  45%|████▌     | 330/727 [01:37<01:57,  3.37it/s, loss=0.125, v_num=0, train_loss=0.135]\u001b[0m\n",
      "\u001b[32mEpoch 0:  45%|████▌     | 330/727 [01:37<01:57,  3.37it/s, loss=0.0948, v_num=0, train_loss=0.0792]\u001b[0m\n",
      "\u001b[35mEpoch 0:  45%|████▌     | 330/727 [01:35<01:55,  3.44it/s, loss=0.113, v_num=0, train_loss=0.113]\u001b[0m\n",
      "\u001b[35mEpoch 0:  45%|████▌     | 330/727 [01:35<01:55,  3.44it/s, loss=0.113, v_num=0, train_loss=0.113]\u001b[0m\n",
      "\u001b[35mEpoch 0:  45%|████▌     | 330/727 [01:35<01:55,  3.44it/s, loss=0.0953, v_num=0, train_loss=0.124]\u001b[0m\n",
      "\u001b[34mEpoch 0:  50%|████▉     | 360/727 [01:41<01:43,  3.55it/s, loss=0.0917, v_num=0, train_loss=0.0951]\u001b[0m\n",
      "\u001b[34mEpoch 0:  50%|████▉     | 360/727 [01:41<01:43,  3.55it/s, loss=0.0917, v_num=0, train_loss=0.0951]\u001b[0m\n",
      "\u001b[34mEpoch 0:  50%|████▉     | 360/727 [01:41<01:43,  3.55it/s, loss=0.0812, v_num=0, train_loss=0.0696]\u001b[0m\n",
      "\u001b[35mEpoch 0:  50%|████▉     | 360/727 [01:44<01:46,  3.43it/s, loss=0.0953, v_num=0, train_loss=0.124]\u001b[0m\n",
      "\u001b[35mEpoch 0:  50%|████▉     | 360/727 [01:44<01:46,  3.43it/s, loss=0.0953, v_num=0, train_loss=0.124]\u001b[0m\n",
      "\u001b[35mEpoch 0:  50%|████▉     | 360/727 [01:44<01:46,  3.43it/s, loss=0.0741, v_num=0, train_loss=0.0554]\u001b[0m\n",
      "\u001b[32mEpoch 0:  50%|████▉     | 360/727 [01:47<01:49,  3.36it/s, loss=0.0948, v_num=0, train_loss=0.0792]\u001b[0m\n",
      "\u001b[32mEpoch 0:  50%|████▉     | 360/727 [01:47<01:49,  3.36it/s, loss=0.0948, v_num=0, train_loss=0.0792]\u001b[0m\n",
      "\u001b[32mEpoch 0:  50%|████▉     | 360/727 [01:47<01:49,  3.36it/s, loss=0.0884, v_num=0, train_loss=0.0844]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 390/727 [01:49<01:34,  3.55it/s, loss=0.0812, v_num=0, train_loss=0.0696]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 390/727 [01:49<01:34,  3.55it/s, loss=0.0812, v_num=0, train_loss=0.0696]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 390/727 [01:49<01:35,  3.55it/s, loss=0.0666, v_num=0, train_loss=0.0722]\u001b[0m\n",
      "\u001b[35mEpoch 0:  54%|█████▎    | 390/727 [01:53<01:38,  3.43it/s, loss=0.0741, v_num=0, train_loss=0.0554]\u001b[0m\n",
      "\u001b[35mEpoch 0:  54%|█████▎    | 390/727 [01:53<01:38,  3.43it/s, loss=0.0741, v_num=0, train_loss=0.0554]\u001b[0m\n",
      "\u001b[35mEpoch 0:  54%|█████▎    | 390/727 [01:53<01:38,  3.42it/s, loss=0.0635, v_num=0, train_loss=0.0684]\u001b[0m\n",
      "\u001b[32mEpoch 0:  54%|█████▎    | 390/727 [01:56<01:40,  3.34it/s, loss=0.0884, v_num=0, train_loss=0.0844]\u001b[0m\n",
      "\u001b[32mEpoch 0:  54%|█████▎    | 390/727 [01:56<01:40,  3.34it/s, loss=0.0884, v_num=0, train_loss=0.0844]\u001b[0m\n",
      "\u001b[32mEpoch 0:  54%|█████▎    | 390/727 [01:56<01:40,  3.34it/s, loss=0.072, v_num=0, train_loss=0.0339]\u001b[0m\n",
      "\u001b[34mEpoch 0:  58%|█████▊    | 420/727 [01:58<01:26,  3.55it/s, loss=0.0666, v_num=0, train_loss=0.0722]\u001b[0m\n",
      "\u001b[34mEpoch 0:  58%|█████▊    | 420/727 [01:58<01:26,  3.55it/s, loss=0.0666, v_num=0, train_loss=0.0722]\u001b[0m\n",
      "\u001b[34mEpoch 0:  58%|█████▊    | 420/727 [01:58<01:26,  3.55it/s, loss=0.0551, v_num=0, train_loss=0.0653]\u001b[0m\n",
      "\u001b[35mEpoch 0:  58%|█████▊    | 420/727 [02:02<01:29,  3.42it/s, loss=0.0635, v_num=0, train_loss=0.0684]\u001b[0m\n",
      "\u001b[35mEpoch 0:  58%|█████▊    | 420/727 [02:02<01:29,  3.42it/s, loss=0.0635, v_num=0, train_loss=0.0684]\u001b[0m\n",
      "\u001b[35mEpoch 0:  58%|█████▊    | 420/727 [02:02<01:29,  3.42it/s, loss=0.0578, v_num=0, train_loss=0.0478]\u001b[0m\n",
      "\u001b[32mEpoch 0:  58%|█████▊    | 420/727 [02:06<01:32,  3.33it/s, loss=0.072, v_num=0, train_loss=0.0339]\u001b[0m\n",
      "\u001b[32mEpoch 0:  58%|█████▊    | 420/727 [02:06<01:32,  3.33it/s, loss=0.072, v_num=0, train_loss=0.0339]\u001b[0m\n",
      "\u001b[32mEpoch 0:  58%|█████▊    | 420/727 [02:06<01:32,  3.33it/s, loss=0.0613, v_num=0, train_loss=0.0988]\u001b[0m\n",
      "\u001b[34mEpoch 0:  62%|██████▏   | 450/727 [02:06<01:18,  3.54it/s, loss=0.0551, v_num=0, train_loss=0.0653]\u001b[0m\n",
      "\u001b[34mEpoch 0:  62%|██████▏   | 450/727 [02:06<01:18,  3.54it/s, loss=0.0551, v_num=0, train_loss=0.0653]\u001b[0m\n",
      "\u001b[34mEpoch 0:  62%|██████▏   | 450/727 [02:06<01:18,  3.54it/s, loss=0.0466, v_num=0, train_loss=0.0457]\u001b[0m\n",
      "\u001b[35mEpoch 0:  62%|██████▏   | 450/727 [02:11<01:21,  3.41it/s, loss=0.0578, v_num=0, train_loss=0.0478]\u001b[0m\n",
      "\u001b[35mEpoch 0:  62%|██████▏   | 450/727 [02:11<01:21,  3.41it/s, loss=0.0578, v_num=0, train_loss=0.0478]\u001b[0m\n",
      "\u001b[35mEpoch 0:  62%|██████▏   | 450/727 [02:11<01:21,  3.41it/s, loss=0.0569, v_num=0, train_loss=0.058]\u001b[0m\n",
      "\u001b[32mEpoch 0:  62%|██████▏   | 450/727 [02:15<01:23,  3.31it/s, loss=0.0613, v_num=0, train_loss=0.0988]\u001b[0m\n",
      "\u001b[32mEpoch 0:  62%|██████▏   | 450/727 [02:15<01:23,  3.31it/s, loss=0.0613, v_num=0, train_loss=0.0988]\u001b[0m\n",
      "\u001b[32mEpoch 0:  62%|██████▏   | 450/727 [02:15<01:23,  3.31it/s, loss=0.0503, v_num=0, train_loss=0.0325]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 480/727 [02:15<01:09,  3.54it/s, loss=0.0466, v_num=0, train_loss=0.0457]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 480/727 [02:15<01:09,  3.54it/s, loss=0.0466, v_num=0, train_loss=0.0457]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 480/727 [02:15<01:09,  3.54it/s, loss=0.0459, v_num=0, train_loss=0.0336]\u001b[0m\n",
      "\u001b[35mEpoch 0:  66%|██████▌   | 480/727 [02:21<01:12,  3.40it/s, loss=0.0569, v_num=0, train_loss=0.058]\u001b[0m\n",
      "\u001b[35mEpoch 0:  66%|██████▌   | 480/727 [02:21<01:12,  3.40it/s, loss=0.0569, v_num=0, train_loss=0.058]\u001b[0m\n",
      "\u001b[35mEpoch 0:  66%|██████▌   | 480/727 [02:21<01:12,  3.40it/s, loss=0.0445, v_num=0, train_loss=0.0187]\u001b[0m\n",
      "\u001b[34mEpoch 0:  70%|███████   | 510/727 [02:24<01:01,  3.54it/s, loss=0.0459, v_num=0, train_loss=0.0336]\u001b[0m\n",
      "\u001b[34mEpoch 0:  70%|███████   | 510/727 [02:24<01:01,  3.54it/s, loss=0.0459, v_num=0, train_loss=0.0336]\u001b[0m\n",
      "\u001b[34mEpoch 0:  70%|███████   | 510/727 [02:24<01:01,  3.54it/s, loss=0.0357, v_num=0, train_loss=0.0275]\u001b[0m\n",
      "\u001b[32mEpoch 0:  66%|██████▌   | 480/727 [02:25<01:14,  3.30it/s, loss=0.0503, v_num=0, train_loss=0.0325]\u001b[0m\n",
      "\u001b[32mEpoch 0:  66%|██████▌   | 480/727 [02:25<01:14,  3.30it/s, loss=0.0503, v_num=0, train_loss=0.0325]\u001b[0m\n",
      "\u001b[32mEpoch 0:  66%|██████▌   | 480/727 [02:25<01:14,  3.30it/s, loss=0.0499, v_num=0, train_loss=0.0522]\u001b[0m\n",
      "\u001b[35mEpoch 0:  70%|███████   | 510/727 [02:30<01:03,  3.39it/s, loss=0.0445, v_num=0, train_loss=0.0187]\u001b[0m\n",
      "\u001b[35mEpoch 0:  70%|███████   | 510/727 [02:30<01:03,  3.39it/s, loss=0.0445, v_num=0, train_loss=0.0187]\u001b[0m\n",
      "\u001b[35mEpoch 0:  70%|███████   | 510/727 [02:30<01:03,  3.39it/s, loss=0.034, v_num=0, train_loss=0.0245]\u001b[0m\n",
      "\u001b[34mEpoch 0:  74%|███████▍  | 540/727 [02:32<00:52,  3.54it/s, loss=0.0357, v_num=0, train_loss=0.0275]\u001b[0m\n",
      "\u001b[34mEpoch 0:  74%|███████▍  | 540/727 [02:32<00:52,  3.54it/s, loss=0.0357, v_num=0, train_loss=0.0275]\u001b[0m\n",
      "\u001b[34mEpoch 0:  74%|███████▍  | 540/727 [02:32<00:52,  3.54it/s, loss=0.0365, v_num=0, train_loss=0.0191]\u001b[0m\n",
      "\u001b[32mEpoch 0:  70%|███████   | 510/727 [02:35<01:05,  3.29it/s, loss=0.0499, v_num=0, train_loss=0.0522]\u001b[0m\n",
      "\u001b[32mEpoch 0:  70%|███████   | 510/727 [02:35<01:05,  3.29it/s, loss=0.0499, v_num=0, train_loss=0.0522]\u001b[0m\n",
      "\u001b[32mEpoch 0:  70%|███████   | 510/727 [02:35<01:05,  3.29it/s, loss=0.0438, v_num=0, train_loss=0.0215]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 570/727 [02:41<00:44,  3.54it/s, loss=0.0365, v_num=0, train_loss=0.0191]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 570/727 [02:41<00:44,  3.54it/s, loss=0.0365, v_num=0, train_loss=0.0191]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 570/727 [02:41<00:44,  3.54it/s, loss=0.0328, v_num=0, train_loss=0.0525]\u001b[0m\n",
      "\u001b[35mEpoch 0:  74%|███████▍  | 540/727 [02:39<00:55,  3.39it/s, loss=0.034, v_num=0, train_loss=0.0245]\u001b[0m\n",
      "\u001b[35mEpoch 0:  74%|███████▍  | 540/727 [02:39<00:55,  3.39it/s, loss=0.034, v_num=0, train_loss=0.0245]\u001b[0m\n",
      "\u001b[35mEpoch 0:  74%|███████▍  | 540/727 [02:39<00:55,  3.38it/s, loss=0.0252, v_num=0, train_loss=0.0567]\u001b[0m\n",
      "\u001b[32mEpoch 0:  74%|███████▍  | 540/727 [02:44<00:57,  3.28it/s, loss=0.0438, v_num=0, train_loss=0.0215]\u001b[0m\n",
      "\u001b[32mEpoch 0:  74%|███████▍  | 540/727 [02:44<00:57,  3.28it/s, loss=0.0438, v_num=0, train_loss=0.0215]\u001b[0m\n",
      "\u001b[32mEpoch 0:  74%|███████▍  | 540/727 [02:44<00:57,  3.28it/s, loss=0.0468, v_num=0, train_loss=0.0416]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 600/727 [02:49<00:35,  3.53it/s, loss=0.0328, v_num=0, train_loss=0.0525]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 600/727 [02:49<00:35,  3.53it/s, loss=0.0328, v_num=0, train_loss=0.0525]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 600/727 [02:49<00:35,  3.53it/s, loss=0.0298, v_num=0, train_loss=0.0536]\u001b[0m\n",
      "\u001b[35mEpoch 0:  78%|███████▊  | 570/727 [02:48<00:46,  3.38it/s, loss=0.0252, v_num=0, train_loss=0.0567]\u001b[0m\n",
      "\u001b[35mEpoch 0:  78%|███████▊  | 570/727 [02:48<00:46,  3.38it/s, loss=0.0252, v_num=0, train_loss=0.0567]\u001b[0m\n",
      "\u001b[35mEpoch 0:  78%|███████▊  | 570/727 [02:48<00:46,  3.38it/s, loss=0.0298, v_num=0, train_loss=0.0214]\u001b[0m\n",
      "\u001b[32mEpoch 0:  78%|███████▊  | 570/727 [02:54<00:48,  3.26it/s, loss=0.0468, v_num=0, train_loss=0.0416]\u001b[0m\n",
      "\u001b[32mEpoch 0:  78%|███████▊  | 570/727 [02:54<00:48,  3.26it/s, loss=0.0468, v_num=0, train_loss=0.0416]\u001b[0m\n",
      "\u001b[32mEpoch 0:  78%|███████▊  | 570/727 [02:54<00:48,  3.26it/s, loss=0.0378, v_num=0, train_loss=0.024]\u001b[0m\n",
      "\u001b[34mEpoch 0:  87%|████████▋ | 630/727 [02:58<00:27,  3.53it/s, loss=0.0298, v_num=0, train_loss=0.0536]\u001b[0m\n",
      "\u001b[34mEpoch 0:  87%|████████▋ | 630/727 [02:58<00:27,  3.53it/s, loss=0.0298, v_num=0, train_loss=0.0536]\u001b[0m\n",
      "\u001b[34mEpoch 0:  87%|████████▋ | 630/727 [02:58<00:27,  3.53it/s, loss=0.021, v_num=0, train_loss=0.028]\u001b[0m\n",
      "\u001b[35mEpoch 0:  83%|████████▎ | 600/727 [02:58<00:37,  3.37it/s, loss=0.0298, v_num=0, train_loss=0.0214]\u001b[0m\n",
      "\u001b[35mEpoch 0:  83%|████████▎ | 600/727 [02:58<00:37,  3.37it/s, loss=0.0298, v_num=0, train_loss=0.0214]\u001b[0m\n",
      "\u001b[35mEpoch 0:  83%|████████▎ | 600/727 [02:58<00:37,  3.37it/s, loss=0.0312, v_num=0, train_loss=0.0263]\u001b[0m\n",
      "\u001b[32mEpoch 0:  83%|████████▎ | 600/727 [03:04<00:39,  3.25it/s, loss=0.0378, v_num=0, train_loss=0.024]\u001b[0m\n",
      "\u001b[32mEpoch 0:  83%|████████▎ | 600/727 [03:04<00:39,  3.25it/s, loss=0.0378, v_num=0, train_loss=0.024]\u001b[0m\n",
      "\u001b[32mEpoch 0:  83%|████████▎ | 600/727 [03:04<00:39,  3.25it/s, loss=0.0295, v_num=0, train_loss=0.0228]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  91%|█████████ | 660/727 [03:05<00:18,  3.55it/s, loss=0.021, v_num=0, train_loss=0.028]\u001b[0m\n",
      "\u001b[34mEpoch 0:  91%|█████████ | 660/727 [03:05<00:18,  3.55it/s, loss=0.021, v_num=0, train_loss=0.028]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:03, 11.23it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:03, 11.23it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▍| 690/727 [03:08<00:10,  3.66it/s, loss=0.021, v_num=0, train_loss=0.028]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▍| 690/727 [03:08<00:10,  3.66it/s, loss=0.021, v_num=0, train_loss=0.028]\u001b[0m\n",
      "\u001b[35mEpoch 0:  87%|████████▋ | 630/727 [03:07<00:28,  3.36it/s, loss=0.0312, v_num=0, train_loss=0.0263]\u001b[0m\n",
      "\u001b[35mEpoch 0:  87%|████████▋ | 630/727 [03:07<00:28,  3.36it/s, loss=0.0312, v_num=0, train_loss=0.0263]\u001b[0m\n",
      "\u001b[35mEpoch 0:  87%|████████▋ | 630/727 [03:07<00:28,  3.36it/s, loss=0.0238, v_num=0, train_loss=0.0199]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01, 10.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01, 10.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  99%|█████████▉| 720/727 [03:11<00:01,  3.76it/s, loss=0.021, v_num=0, train_loss=0.028]\u001b[0m\n",
      "\u001b[34mEpoch 0:  99%|█████████▉| 720/727 [03:11<00:01,  3.76it/s, loss=0.021, v_num=0, train_loss=0.028]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 73/73 [00:06<00:00, 10.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 73/73 [00:06<00:00, 10.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 727/727 [03:12<00:00,  3.78it/s, loss=0.021, v_num=0, train_loss=0.028]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 727/727 [03:12<00:00,  3.78it/s, loss=0.0297, v_num=0, train_loss=0.0127, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 727/727 [03:13<00:00,  3.76it/s, loss=0.0297, v_num=0, train_loss=0.0127, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 0:  87%|████████▋ | 630/727 [03:14<00:29,  3.24it/s, loss=0.0295, v_num=0, train_loss=0.0228]\u001b[0m\n",
      "\u001b[32mEpoch 0:  87%|████████▋ | 630/727 [03:14<00:29,  3.24it/s, loss=0.0295, v_num=0, train_loss=0.0228]\u001b[0m\n",
      "\u001b[32mEpoch 0:  87%|████████▋ | 630/727 [03:14<00:29,  3.24it/s, loss=0.0341, v_num=0, train_loss=0.0345]\u001b[0m\n",
      "\u001b[35mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 0:  91%|█████████ | 660/727 [03:15<00:19,  3.38it/s, loss=0.0238, v_num=0, train_loss=0.0199]\u001b[0m\n",
      "\u001b[35mEpoch 0:  91%|█████████ | 660/727 [03:15<00:19,  3.38it/s, loss=0.0238, v_num=0, train_loss=0.0199]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:04, 10.36it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:04, 10.36it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 0:  95%|█████████▍| 690/727 [03:18<00:10,  3.47it/s, loss=0.0238, v_num=0, train_loss=0.0199]\u001b[0m\n",
      "\u001b[35mEpoch 0:  95%|█████████▍| 690/727 [03:18<00:10,  3.47it/s, loss=0.0238, v_num=0, train_loss=0.0199]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0297, v_num=0, train_loss=0.0127, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0297, v_num=0, train_loss=0.0127, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 0:  91%|█████████ | 660/727 [03:22<00:20,  3.26it/s, loss=0.0341, v_num=0, train_loss=0.0345]\u001b[0m\n",
      "\u001b[32mEpoch 0:  91%|█████████ | 660/727 [03:22<00:20,  3.26it/s, loss=0.0341, v_num=0, train_loss=0.0345]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.76it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.76it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 0:  99%|█████████▉| 720/727 [03:21<00:01,  3.57it/s, loss=0.0238, v_num=0, train_loss=0.0199]\u001b[0m\n",
      "\u001b[35mEpoch 0:  99%|█████████▉| 720/727 [03:21<00:01,  3.57it/s, loss=0.0238, v_num=0, train_loss=0.0199]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.69it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.69it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 0: 100%|██████████| 727/727 [03:22<00:00,  3.59it/s, loss=0.0238, v_num=0, train_loss=0.0199]\u001b[0m\n",
      "\u001b[35mEpoch 0: 100%|██████████| 727/727 [03:22<00:00,  3.59it/s, loss=0.0241, v_num=0, train_loss=0.00904, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mEpoch 1:   4%|▍         | 30/727 [00:08<03:28,  3.34it/s, loss=0.0297, v_num=0, train_loss=0.0127, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:   4%|▍         | 30/727 [00:08<03:28,  3.34it/s, loss=0.0297, v_num=0, train_loss=0.0127, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:   4%|▍         | 30/727 [00:09<03:29,  3.33it/s, loss=0.0299, v_num=0, train_loss=0.0126, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  41%|████      | 30/73 [00:03<00:04,  9.68it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  41%|████      | 30/73 [00:03<00:04,  9.68it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 0:  95%|█████████▍| 690/727 [03:25<00:11,  3.35it/s, loss=0.0341, v_num=0, train_loss=0.0345]\u001b[0m\n",
      "\u001b[32mEpoch 0:  95%|█████████▍| 690/727 [03:25<00:11,  3.35it/s, loss=0.0341, v_num=0, train_loss=0.0345]\u001b[0m\n",
      "\u001b[35mEpoch 0: 100%|██████████| 727/727 [03:23<00:00,  3.58it/s, loss=0.0241, v_num=0, train_loss=0.00904, val_loss=0.0246]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.18it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.18it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 0:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0241, v_num=0, train_loss=0.00904, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0241, v_num=0, train_loss=0.00904, val_loss=0.0246]\u001b[0m\n",
      "\u001b[32mEpoch 0:  99%|█████████▉| 720/727 [03:29<00:02,  3.44it/s, loss=0.0341, v_num=0, train_loss=0.0345]\u001b[0m\n",
      "\u001b[32mEpoch 0:  99%|█████████▉| 720/727 [03:29<00:02,  3.44it/s, loss=0.0341, v_num=0, train_loss=0.0345]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.10it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.10it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 0: 100%|██████████| 727/727 [03:30<00:00,  3.46it/s, loss=0.0341, v_num=0, train_loss=0.0345]\u001b[0m\n",
      "\u001b[32mEpoch 0: 100%|██████████| 727/727 [03:30<00:00,  3.46it/s, loss=0.0279, v_num=0, train_loss=0.0156, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32m#033[A\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\u001b[0m\n",
      "\u001b[32mEpoch 0: 100%|██████████| 727/727 [03:30<00:00,  3.45it/s, loss=0.0279, v_num=0, train_loss=0.0156, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 0:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0279, v_num=0, train_loss=0.0156, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0279, v_num=0, train_loss=0.0156, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mEpoch 1:   8%|▊         | 60/727 [00:17<03:16,  3.40it/s, loss=0.0299, v_num=0, train_loss=0.0126, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:   8%|▊         | 60/727 [00:17<03:16,  3.40it/s, loss=0.0299, v_num=0, train_loss=0.0126, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:   8%|▊         | 60/727 [00:17<03:16,  3.40it/s, loss=0.0278, v_num=0, train_loss=0.0153, val_loss=0.0294]\u001b[0m\n",
      "\u001b[35mEpoch 1:   4%|▍         | 30/727 [00:09<03:47,  3.07it/s, loss=0.0241, v_num=0, train_loss=0.00904, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:   4%|▍         | 30/727 [00:09<03:47,  3.07it/s, loss=0.0241, v_num=0, train_loss=0.00904, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:   4%|▍         | 30/727 [00:09<03:47,  3.06it/s, loss=0.0303, v_num=0, train_loss=0.0313, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 90/727 [00:26<03:06,  3.42it/s, loss=0.0278, v_num=0, train_loss=0.0153, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 90/727 [00:26<03:06,  3.42it/s, loss=0.0278, v_num=0, train_loss=0.0153, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 90/727 [00:26<03:06,  3.42it/s, loss=0.0315, v_num=0, train_loss=0.044, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:   4%|▍         | 30/727 [00:09<03:49,  3.04it/s, loss=0.0279, v_num=0, train_loss=0.0156, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:   4%|▍         | 30/727 [00:09<03:49,  3.04it/s, loss=0.0279, v_num=0, train_loss=0.0156, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:   4%|▍         | 30/727 [00:09<03:49,  3.03it/s, loss=0.0276, v_num=0, train_loss=0.0182, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:   8%|▊         | 60/727 [00:19<03:33,  3.13it/s, loss=0.0303, v_num=0, train_loss=0.0313, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:   8%|▊         | 60/727 [00:19<03:33,  3.13it/s, loss=0.0303, v_num=0, train_loss=0.0313, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:   8%|▊         | 60/727 [00:19<03:33,  3.13it/s, loss=0.0232, v_num=0, train_loss=0.0135, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 120/727 [00:35<02:57,  3.42it/s, loss=0.0315, v_num=0, train_loss=0.044, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 120/727 [00:35<02:57,  3.42it/s, loss=0.0315, v_num=0, train_loss=0.044, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 120/727 [00:35<02:57,  3.42it/s, loss=0.0253, v_num=0, train_loss=0.0101, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:   8%|▊         | 60/727 [00:19<03:35,  3.09it/s, loss=0.0276, v_num=0, train_loss=0.0182, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:   8%|▊         | 60/727 [00:19<03:35,  3.09it/s, loss=0.0276, v_num=0, train_loss=0.0182, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:   8%|▊         | 60/727 [00:19<03:35,  3.09it/s, loss=0.0208, v_num=0, train_loss=0.0401, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  12%|█▏        | 90/727 [00:28<03:22,  3.15it/s, loss=0.0232, v_num=0, train_loss=0.0135, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  12%|█▏        | 90/727 [00:28<03:22,  3.15it/s, loss=0.0232, v_num=0, train_loss=0.0135, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  12%|█▏        | 90/727 [00:28<03:22,  3.14it/s, loss=0.0296, v_num=0, train_loss=0.0133, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  21%|██        | 150/727 [00:43<02:48,  3.42it/s, loss=0.0253, v_num=0, train_loss=0.0101, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  21%|██        | 150/727 [00:43<02:48,  3.42it/s, loss=0.0253, v_num=0, train_loss=0.0101, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  21%|██        | 150/727 [00:43<02:49,  3.41it/s, loss=0.0236, v_num=0, train_loss=0.0115, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  12%|█▏        | 90/727 [00:28<03:24,  3.11it/s, loss=0.0208, v_num=0, train_loss=0.0401, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  12%|█▏        | 90/727 [00:28<03:24,  3.11it/s, loss=0.0208, v_num=0, train_loss=0.0401, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  12%|█▏        | 90/727 [00:28<03:25,  3.11it/s, loss=0.0243, v_num=0, train_loss=0.032, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  17%|█▋        | 120/727 [00:38<03:12,  3.15it/s, loss=0.0296, v_num=0, train_loss=0.0133, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  17%|█▋        | 120/727 [00:38<03:12,  3.15it/s, loss=0.0296, v_num=0, train_loss=0.0133, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  17%|█▋        | 120/727 [00:38<03:12,  3.15it/s, loss=0.0185, v_num=0, train_loss=0.0103, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  25%|██▍       | 180/727 [00:52<02:40,  3.41it/s, loss=0.0236, v_num=0, train_loss=0.0115, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  25%|██▍       | 180/727 [00:52<02:40,  3.41it/s, loss=0.0236, v_num=0, train_loss=0.0115, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  25%|██▍       | 180/727 [00:52<02:40,  3.41it/s, loss=0.0222, v_num=0, train_loss=0.0118, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  17%|█▋        | 120/727 [00:38<03:15,  3.11it/s, loss=0.0243, v_num=0, train_loss=0.032, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  17%|█▋        | 120/727 [00:38<03:15,  3.11it/s, loss=0.0243, v_num=0, train_loss=0.032, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  17%|█▋        | 120/727 [00:38<03:15,  3.11it/s, loss=0.025, v_num=0, train_loss=0.0105, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  21%|██        | 150/727 [00:47<03:02,  3.16it/s, loss=0.0185, v_num=0, train_loss=0.0103, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  21%|██        | 150/727 [00:47<03:02,  3.16it/s, loss=0.0185, v_num=0, train_loss=0.0103, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  21%|██        | 150/727 [00:47<03:02,  3.16it/s, loss=0.0224, v_num=0, train_loss=0.010, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 210/727 [01:01<02:31,  3.41it/s, loss=0.0222, v_num=0, train_loss=0.0118, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 210/727 [01:01<02:31,  3.41it/s, loss=0.0222, v_num=0, train_loss=0.0118, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 210/727 [01:01<02:31,  3.41it/s, loss=0.0165, v_num=0, train_loss=0.00948, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  21%|██        | 150/727 [00:48<03:06,  3.10it/s, loss=0.025, v_num=0, train_loss=0.0105, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  21%|██        | 150/727 [00:48<03:06,  3.10it/s, loss=0.025, v_num=0, train_loss=0.0105, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  21%|██        | 150/727 [00:48<03:06,  3.10it/s, loss=0.0224, v_num=0, train_loss=0.0354, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  25%|██▍       | 180/727 [00:56<02:52,  3.16it/s, loss=0.0224, v_num=0, train_loss=0.010, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  25%|██▍       | 180/727 [00:56<02:52,  3.16it/s, loss=0.0224, v_num=0, train_loss=0.010, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  25%|██▍       | 180/727 [00:56<02:52,  3.16it/s, loss=0.0223, v_num=0, train_loss=0.0404, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  33%|███▎      | 240/727 [01:10<02:22,  3.41it/s, loss=0.0165, v_num=0, train_loss=0.00948, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  33%|███▎      | 240/727 [01:10<02:22,  3.41it/s, loss=0.0165, v_num=0, train_loss=0.00948, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  33%|███▎      | 240/727 [01:10<02:22,  3.41it/s, loss=0.0181, v_num=0, train_loss=0.00802, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  25%|██▍       | 180/727 [00:58<02:56,  3.10it/s, loss=0.0224, v_num=0, train_loss=0.0354, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  25%|██▍       | 180/727 [00:58<02:56,  3.10it/s, loss=0.0224, v_num=0, train_loss=0.0354, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  25%|██▍       | 180/727 [00:58<02:56,  3.10it/s, loss=0.0202, v_num=0, train_loss=0.0298, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  29%|██▉       | 210/727 [01:06<02:43,  3.17it/s, loss=0.0223, v_num=0, train_loss=0.0404, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  29%|██▉       | 210/727 [01:06<02:43,  3.17it/s, loss=0.0223, v_num=0, train_loss=0.0404, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  29%|██▉       | 210/727 [01:06<02:43,  3.16it/s, loss=0.0201, v_num=0, train_loss=0.0106, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 270/727 [01:19<02:13,  3.42it/s, loss=0.0181, v_num=0, train_loss=0.00802, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 270/727 [01:19<02:13,  3.42it/s, loss=0.0181, v_num=0, train_loss=0.00802, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 270/727 [01:19<02:13,  3.42it/s, loss=0.016, v_num=0, train_loss=0.0337, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  29%|██▉       | 210/727 [01:07<02:47,  3.09it/s, loss=0.0202, v_num=0, train_loss=0.0298, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  29%|██▉       | 210/727 [01:07<02:47,  3.09it/s, loss=0.0202, v_num=0, train_loss=0.0298, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  29%|██▉       | 210/727 [01:07<02:47,  3.09it/s, loss=0.0182, v_num=0, train_loss=0.0118, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  33%|███▎      | 240/727 [01:15<02:33,  3.17it/s, loss=0.0201, v_num=0, train_loss=0.0106, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  33%|███▎      | 240/727 [01:15<02:33,  3.17it/s, loss=0.0201, v_num=0, train_loss=0.0106, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  33%|███▎      | 240/727 [01:15<02:33,  3.17it/s, loss=0.0145, v_num=0, train_loss=0.0107, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 300/727 [01:27<02:04,  3.42it/s, loss=0.016, v_num=0, train_loss=0.0337, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 300/727 [01:27<02:04,  3.42it/s, loss=0.016, v_num=0, train_loss=0.0337, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 300/727 [01:27<02:04,  3.42it/s, loss=0.0185, v_num=0, train_loss=0.00757, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  33%|███▎      | 240/727 [01:17<02:37,  3.09it/s, loss=0.0182, v_num=0, train_loss=0.0118, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  33%|███▎      | 240/727 [01:17<02:37,  3.09it/s, loss=0.0182, v_num=0, train_loss=0.0118, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  33%|███▎      | 240/727 [01:17<02:37,  3.09it/s, loss=0.021, v_num=0, train_loss=0.0402, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mEpoch 1:  45%|████▌     | 330/727 [01:36<01:55,  3.43it/s, loss=0.0185, v_num=0, train_loss=0.00757, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  45%|████▌     | 330/727 [01:36<01:55,  3.43it/s, loss=0.0185, v_num=0, train_loss=0.00757, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  45%|████▌     | 330/727 [01:36<01:55,  3.43it/s, loss=0.0203, v_num=0, train_loss=0.0112, val_loss=0.0294]\u001b[0m\n",
      "\u001b[35mEpoch 1:  37%|███▋      | 270/727 [01:25<02:24,  3.17it/s, loss=0.0145, v_num=0, train_loss=0.0107, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  37%|███▋      | 270/727 [01:25<02:24,  3.17it/s, loss=0.0145, v_num=0, train_loss=0.0107, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  37%|███▋      | 270/727 [01:25<02:24,  3.17it/s, loss=0.0192, v_num=0, train_loss=0.0211, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  50%|████▉     | 360/727 [01:44<01:46,  3.44it/s, loss=0.0203, v_num=0, train_loss=0.0112, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  50%|████▉     | 360/727 [01:44<01:46,  3.44it/s, loss=0.0203, v_num=0, train_loss=0.0112, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  50%|████▉     | 360/727 [01:44<01:46,  3.44it/s, loss=0.0158, v_num=0, train_loss=0.0391, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  37%|███▋      | 270/727 [01:27<02:27,  3.09it/s, loss=0.021, v_num=0, train_loss=0.0402, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  37%|███▋      | 270/727 [01:27<02:27,  3.09it/s, loss=0.021, v_num=0, train_loss=0.0402, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  37%|███▋      | 270/727 [01:27<02:28,  3.09it/s, loss=0.0158, v_num=0, train_loss=0.0119, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  41%|████▏     | 300/727 [01:34<02:14,  3.17it/s, loss=0.0192, v_num=0, train_loss=0.0211, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  41%|████▏     | 300/727 [01:34<02:14,  3.17it/s, loss=0.0192, v_num=0, train_loss=0.0211, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  41%|████▏     | 300/727 [01:34<02:14,  3.17it/s, loss=0.0169, v_num=0, train_loss=0.013, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 390/727 [01:53<01:37,  3.45it/s, loss=0.0158, v_num=0, train_loss=0.0391, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 390/727 [01:53<01:37,  3.45it/s, loss=0.0158, v_num=0, train_loss=0.0391, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 390/727 [01:53<01:37,  3.44it/s, loss=0.0141, v_num=0, train_loss=0.0351, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  41%|████▏     | 300/727 [01:37<02:18,  3.09it/s, loss=0.0158, v_num=0, train_loss=0.0119, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  41%|████▏     | 300/727 [01:37<02:18,  3.09it/s, loss=0.0158, v_num=0, train_loss=0.0119, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  41%|████▏     | 300/727 [01:37<02:18,  3.09it/s, loss=0.0164, v_num=0, train_loss=0.011, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  45%|████▌     | 330/727 [01:43<02:04,  3.18it/s, loss=0.0169, v_num=0, train_loss=0.013, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  45%|████▌     | 330/727 [01:43<02:04,  3.18it/s, loss=0.0169, v_num=0, train_loss=0.013, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  45%|████▌     | 330/727 [01:43<02:04,  3.18it/s, loss=0.0142, v_num=0, train_loss=0.0104, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  58%|█████▊    | 420/727 [02:01<01:28,  3.45it/s, loss=0.0141, v_num=0, train_loss=0.0351, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  58%|█████▊    | 420/727 [02:01<01:28,  3.45it/s, loss=0.0141, v_num=0, train_loss=0.0351, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  58%|█████▊    | 420/727 [02:01<01:28,  3.45it/s, loss=0.011, v_num=0, train_loss=0.00801, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  45%|████▌     | 330/727 [01:46<02:08,  3.09it/s, loss=0.0164, v_num=0, train_loss=0.011, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  45%|████▌     | 330/727 [01:46<02:08,  3.09it/s, loss=0.0164, v_num=0, train_loss=0.011, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  45%|████▌     | 330/727 [01:46<02:08,  3.09it/s, loss=0.0219, v_num=0, train_loss=0.0181, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  50%|████▉     | 360/727 [01:53<01:55,  3.18it/s, loss=0.0142, v_num=0, train_loss=0.0104, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  50%|████▉     | 360/727 [01:53<01:55,  3.18it/s, loss=0.0142, v_num=0, train_loss=0.0104, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  50%|████▉     | 360/727 [01:53<01:55,  3.18it/s, loss=0.0259, v_num=0, train_loss=0.0508, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  62%|██████▏   | 450/727 [02:10<01:20,  3.46it/s, loss=0.011, v_num=0, train_loss=0.00801, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  62%|██████▏   | 450/727 [02:10<01:20,  3.46it/s, loss=0.011, v_num=0, train_loss=0.00801, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  62%|██████▏   | 450/727 [02:10<01:20,  3.46it/s, loss=0.0125, v_num=0, train_loss=0.00837, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  50%|████▉     | 360/727 [01:56<01:58,  3.09it/s, loss=0.0219, v_num=0, train_loss=0.0181, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  50%|████▉     | 360/727 [01:56<01:58,  3.09it/s, loss=0.0219, v_num=0, train_loss=0.0181, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  50%|████▉     | 360/727 [01:56<01:58,  3.09it/s, loss=0.0218, v_num=0, train_loss=0.0588, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  54%|█████▎    | 390/727 [02:02<01:45,  3.18it/s, loss=0.0259, v_num=0, train_loss=0.0508, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  54%|█████▎    | 390/727 [02:02<01:45,  3.18it/s, loss=0.0259, v_num=0, train_loss=0.0508, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  54%|█████▎    | 390/727 [02:02<01:45,  3.18it/s, loss=0.0173, v_num=0, train_loss=0.0329, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 480/727 [02:18<01:11,  3.46it/s, loss=0.0125, v_num=0, train_loss=0.00837, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 480/727 [02:18<01:11,  3.46it/s, loss=0.0125, v_num=0, train_loss=0.00837, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 480/727 [02:18<01:11,  3.46it/s, loss=0.00976, v_num=0, train_loss=0.00904, val_loss=0.0294]\u001b[0m\n",
      "\u001b[35mEpoch 1:  58%|█████▊    | 420/727 [02:11<01:36,  3.18it/s, loss=0.0173, v_num=0, train_loss=0.0329, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  58%|█████▊    | 420/727 [02:11<01:36,  3.18it/s, loss=0.0173, v_num=0, train_loss=0.0329, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  58%|█████▊    | 420/727 [02:11<01:36,  3.18it/s, loss=0.0152, v_num=0, train_loss=0.00655, val_loss=0.0246]\u001b[0m\n",
      "\u001b[32mEpoch 1:  54%|█████▎    | 390/727 [02:06<01:49,  3.09it/s, loss=0.0218, v_num=0, train_loss=0.0588, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  54%|█████▎    | 390/727 [02:06<01:49,  3.09it/s, loss=0.0218, v_num=0, train_loss=0.0588, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  54%|█████▎    | 390/727 [02:06<01:49,  3.09it/s, loss=0.0181, v_num=0, train_loss=0.00896, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mEpoch 1:  70%|███████   | 510/727 [02:27<01:02,  3.46it/s, loss=0.00976, v_num=0, train_loss=0.00904, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  70%|███████   | 510/727 [02:27<01:02,  3.46it/s, loss=0.00976, v_num=0, train_loss=0.00904, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  70%|███████   | 510/727 [02:27<01:02,  3.46it/s, loss=0.0173, v_num=0, train_loss=0.038, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  58%|█████▊    | 420/727 [02:15<01:39,  3.09it/s, loss=0.0181, v_num=0, train_loss=0.00896, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  58%|█████▊    | 420/727 [02:15<01:39,  3.09it/s, loss=0.0181, v_num=0, train_loss=0.00896, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  58%|█████▊    | 420/727 [02:15<01:39,  3.09it/s, loss=0.0192, v_num=0, train_loss=0.0137, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  62%|██████▏   | 450/727 [02:21<01:26,  3.19it/s, loss=0.0152, v_num=0, train_loss=0.00655, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  62%|██████▏   | 450/727 [02:21<01:26,  3.19it/s, loss=0.0152, v_num=0, train_loss=0.00655, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  62%|██████▏   | 450/727 [02:21<01:26,  3.19it/s, loss=0.015, v_num=0, train_loss=0.00658, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mEpoch 1:  74%|███████▍  | 540/727 [02:35<00:53,  3.47it/s, loss=0.0173, v_num=0, train_loss=0.038, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  74%|███████▍  | 540/727 [02:35<00:53,  3.47it/s, loss=0.0173, v_num=0, train_loss=0.038, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  74%|███████▍  | 540/727 [02:35<00:53,  3.47it/s, loss=0.009, v_num=0, train_loss=0.00618, val_loss=0.0294]\u001b[0m\n",
      "\u001b[35mEpoch 1:  66%|██████▌   | 480/727 [02:30<01:17,  3.19it/s, loss=0.015, v_num=0, train_loss=0.00658, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  66%|██████▌   | 480/727 [02:30<01:17,  3.19it/s, loss=0.015, v_num=0, train_loss=0.00658, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  66%|██████▌   | 480/727 [02:30<01:17,  3.19it/s, loss=0.0132, v_num=0, train_loss=0.0345, val_loss=0.0246]\u001b[0m\n",
      "\u001b[32mEpoch 1:  62%|██████▏   | 450/727 [02:25<01:29,  3.09it/s, loss=0.0192, v_num=0, train_loss=0.0137, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  62%|██████▏   | 450/727 [02:25<01:29,  3.09it/s, loss=0.0192, v_num=0, train_loss=0.0137, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  62%|██████▏   | 450/727 [02:25<01:29,  3.09it/s, loss=0.0148, v_num=0, train_loss=0.0105, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 570/727 [02:44<00:45,  3.47it/s, loss=0.009, v_num=0, train_loss=0.00618, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 570/727 [02:44<00:45,  3.47it/s, loss=0.009, v_num=0, train_loss=0.00618, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 570/727 [02:44<00:45,  3.47it/s, loss=0.00972, v_num=0, train_loss=0.0147, val_loss=0.0294]\u001b[0m\n",
      "\u001b[35mEpoch 1:  70%|███████   | 510/727 [02:39<01:08,  3.19it/s, loss=0.0132, v_num=0, train_loss=0.0345, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  70%|███████   | 510/727 [02:39<01:08,  3.19it/s, loss=0.0132, v_num=0, train_loss=0.0345, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  70%|███████   | 510/727 [02:39<01:08,  3.19it/s, loss=0.0134, v_num=0, train_loss=0.00761, val_loss=0.0246]\u001b[0m\n",
      "\u001b[32mEpoch 1:  66%|██████▌   | 480/727 [02:35<01:19,  3.10it/s, loss=0.0148, v_num=0, train_loss=0.0105, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  66%|██████▌   | 480/727 [02:35<01:19,  3.10it/s, loss=0.0148, v_num=0, train_loss=0.0105, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  66%|██████▌   | 480/727 [02:35<01:19,  3.10it/s, loss=0.0185, v_num=0, train_loss=0.0133, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 600/727 [02:52<00:36,  3.48it/s, loss=0.00972, v_num=0, train_loss=0.0147, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 600/727 [02:52<00:36,  3.48it/s, loss=0.00972, v_num=0, train_loss=0.0147, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 600/727 [02:52<00:36,  3.47it/s, loss=0.0136, v_num=0, train_loss=0.00968, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  87%|████████▋ | 630/727 [03:01<00:27,  3.48it/s, loss=0.0136, v_num=0, train_loss=0.00968, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  87%|████████▋ | 630/727 [03:01<00:27,  3.48it/s, loss=0.0136, v_num=0, train_loss=0.00968, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  87%|████████▋ | 630/727 [03:01<00:27,  3.48it/s, loss=0.0124, v_num=0, train_loss=0.0342, val_loss=0.0294]\u001b[0m\n",
      "\u001b[35mEpoch 1:  74%|███████▍  | 540/727 [02:49<00:58,  3.19it/s, loss=0.0134, v_num=0, train_loss=0.00761, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  74%|███████▍  | 540/727 [02:49<00:58,  3.19it/s, loss=0.0134, v_num=0, train_loss=0.00761, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  74%|███████▍  | 540/727 [02:49<00:58,  3.19it/s, loss=0.0126, v_num=0, train_loss=0.00538, val_loss=0.0246]\u001b[0m\n",
      "\u001b[32mEpoch 1:  70%|███████   | 510/727 [02:44<01:10,  3.10it/s, loss=0.0185, v_num=0, train_loss=0.0133, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  70%|███████   | 510/727 [02:44<01:10,  3.10it/s, loss=0.0185, v_num=0, train_loss=0.0133, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  70%|███████   | 510/727 [02:44<01:10,  3.10it/s, loss=0.0158, v_num=0, train_loss=0.0112, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  91%|█████████ | 660/727 [03:08<00:19,  3.50it/s, loss=0.0124, v_num=0, train_loss=0.0342, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  91%|█████████ | 660/727 [03:08<00:19,  3.50it/s, loss=0.0124, v_num=0, train_loss=0.0342, val_loss=0.0294]\u001b[0m\n",
      "\u001b[35mEpoch 1:  78%|███████▊  | 570/727 [02:58<00:49,  3.19it/s, loss=0.0126, v_num=0, train_loss=0.00538, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  78%|███████▊  | 570/727 [02:58<00:49,  3.19it/s, loss=0.0126, v_num=0, train_loss=0.00538, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  78%|███████▊  | 570/727 [02:58<00:49,  3.19it/s, loss=0.0111, v_num=0, train_loss=0.00864, val_loss=0.0246]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:03, 11.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:03, 11.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▍| 690/727 [03:11<00:10,  3.61it/s, loss=0.0124, v_num=0, train_loss=0.0342, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▍| 690/727 [03:11<00:10,  3.61it/s, loss=0.0124, v_num=0, train_loss=0.0342, val_loss=0.0294]\u001b[0m\n",
      "\u001b[32mEpoch 1:  74%|███████▍  | 540/727 [02:54<01:00,  3.10it/s, loss=0.0158, v_num=0, train_loss=0.0112, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  74%|███████▍  | 540/727 [02:54<01:00,  3.10it/s, loss=0.0158, v_num=0, train_loss=0.0112, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  74%|███████▍  | 540/727 [02:54<01:00,  3.10it/s, loss=0.00865, v_num=0, train_loss=0.00757, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01, 10.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01, 10.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  99%|█████████▉| 720/727 [03:14<00:01,  3.71it/s, loss=0.0124, v_num=0, train_loss=0.0342, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1:  99%|█████████▉| 720/727 [03:14<00:01,  3.71it/s, loss=0.0124, v_num=0, train_loss=0.0342, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 73/73 [00:06<00:00, 10.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 73/73 [00:06<00:00, 10.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 727/727 [03:14<00:00,  3.73it/s, loss=0.0124, v_num=0, train_loss=0.0342, val_loss=0.0294]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 727/727 [03:15<00:00,  3.73it/s, loss=0.0112, v_num=0, train_loss=0.0074, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 727/727 [03:15<00:00,  3.72it/s, loss=0.0112, v_num=0, train_loss=0.0074, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0112, v_num=0, train_loss=0.0074, val_loss=0.0142]          #015Epoch 2:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0112, v_num=0, train_loss=0.0074, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 1:  83%|████████▎ | 600/727 [03:07<00:39,  3.20it/s, loss=0.0111, v_num=0, train_loss=0.00864, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  83%|████████▎ | 600/727 [03:07<00:39,  3.20it/s, loss=0.0111, v_num=0, train_loss=0.00864, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  83%|████████▎ | 600/727 [03:07<00:39,  3.19it/s, loss=0.011, v_num=0, train_loss=0.00713, val_loss=0.0246]\u001b[0m\n",
      "\u001b[32mEpoch 1:  78%|███████▊  | 570/727 [03:03<00:50,  3.10it/s, loss=0.00865, v_num=0, train_loss=0.00757, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  78%|███████▊  | 570/727 [03:03<00:50,  3.10it/s, loss=0.00865, v_num=0, train_loss=0.00757, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  78%|███████▊  | 570/727 [03:03<00:50,  3.10it/s, loss=0.0164, v_num=0, train_loss=0.0086, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mEpoch 2:   4%|▍         | 30/727 [00:08<03:26,  3.38it/s, loss=0.0112, v_num=0, train_loss=0.0074, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:   4%|▍         | 30/727 [00:08<03:26,  3.38it/s, loss=0.0112, v_num=0, train_loss=0.0074, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:   4%|▍         | 30/727 [00:08<03:26,  3.37it/s, loss=0.018, v_num=0, train_loss=0.00948, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 1:  87%|████████▋ | 630/727 [03:17<00:30,  3.20it/s, loss=0.011, v_num=0, train_loss=0.00713, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  87%|████████▋ | 630/727 [03:17<00:30,  3.20it/s, loss=0.011, v_num=0, train_loss=0.00713, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  87%|████████▋ | 630/727 [03:17<00:30,  3.20it/s, loss=0.0127, v_num=0, train_loss=0.0367, val_loss=0.0246]\u001b[0m\n",
      "\u001b[32mEpoch 1:  83%|████████▎ | 600/727 [03:13<00:40,  3.10it/s, loss=0.0164, v_num=0, train_loss=0.0086, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  83%|████████▎ | 600/727 [03:13<00:40,  3.10it/s, loss=0.0164, v_num=0, train_loss=0.0086, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  83%|████████▎ | 600/727 [03:13<00:40,  3.10it/s, loss=0.0194, v_num=0, train_loss=0.0268, val_loss=0.0272]\u001b[0m\n",
      "\u001b[34mEpoch 2:   8%|▊         | 60/727 [00:17<03:13,  3.45it/s, loss=0.018, v_num=0, train_loss=0.00948, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:   8%|▊         | 60/727 [00:17<03:13,  3.45it/s, loss=0.018, v_num=0, train_loss=0.00948, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:   8%|▊         | 60/727 [00:17<03:13,  3.45it/s, loss=0.00736, v_num=0, train_loss=0.00675, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 1:  91%|█████████ | 660/727 [03:25<00:20,  3.22it/s, loss=0.0127, v_num=0, train_loss=0.0367, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  91%|█████████ | 660/727 [03:25<00:20,  3.22it/s, loss=0.0127, v_num=0, train_loss=0.0367, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:04, 10.49it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:04, 10.49it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 1:  87%|████████▋ | 630/727 [03:22<00:31,  3.10it/s, loss=0.0194, v_num=0, train_loss=0.0268, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  87%|████████▋ | 630/727 [03:22<00:31,  3.10it/s, loss=0.0194, v_num=0, train_loss=0.0268, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  87%|████████▋ | 630/727 [03:22<00:31,  3.10it/s, loss=0.0116, v_num=0, train_loss=0.00879, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:  95%|█████████▍| 690/727 [03:28<00:11,  3.31it/s, loss=0.0127, v_num=0, train_loss=0.0367, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  95%|█████████▍| 690/727 [03:28<00:11,  3.31it/s, loss=0.0127, v_num=0, train_loss=0.0367, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.91it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.91it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 1:  99%|█████████▉| 720/727 [03:31<00:02,  3.41it/s, loss=0.0127, v_num=0, train_loss=0.0367, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1:  99%|█████████▉| 720/727 [03:31<00:02,  3.41it/s, loss=0.0127, v_num=0, train_loss=0.0367, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.85it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.85it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 1: 100%|██████████| 727/727 [03:32<00:00,  3.43it/s, loss=0.0127, v_num=0, train_loss=0.0367, val_loss=0.0246]\u001b[0m\n",
      "\u001b[35mEpoch 1: 100%|██████████| 727/727 [03:32<00:00,  3.42it/s, loss=0.0156, v_num=0, train_loss=0.00632, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  12%|█▏        | 90/727 [00:25<03:03,  3.48it/s, loss=0.00736, v_num=0, train_loss=0.00675, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  12%|█▏        | 90/727 [00:25<03:03,  3.48it/s, loss=0.00736, v_num=0, train_loss=0.00675, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  12%|█▏        | 90/727 [00:25<03:03,  3.48it/s, loss=0.00913, v_num=0, train_loss=0.00818, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 1: 100%|██████████| 727/727 [03:32<00:00,  3.41it/s, loss=0.0156, v_num=0, train_loss=0.00632, val_loss=0.0124]\u001b[0m\n",
      "\u001b[32mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 1:  91%|█████████ | 660/727 [03:31<00:21,  3.13it/s, loss=0.0116, v_num=0, train_loss=0.00879, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  91%|█████████ | 660/727 [03:31<00:21,  3.13it/s, loss=0.0116, v_num=0, train_loss=0.00879, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  41%|████      | 30/73 [00:03<00:04,  9.79it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  41%|████      | 30/73 [00:03<00:04,  9.79it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 1:  95%|█████████▍| 690/727 [03:34<00:11,  3.22it/s, loss=0.0116, v_num=0, train_loss=0.00879, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  95%|█████████▍| 690/727 [03:34<00:11,  3.22it/s, loss=0.0116, v_num=0, train_loss=0.00879, val_loss=0.0272]\u001b[0m\n",
      "\u001b[35mEpoch 1:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0156, v_num=0, train_loss=0.00632, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0156, v_num=0, train_loss=0.00632, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 120/727 [00:34<02:53,  3.49it/s, loss=0.00913, v_num=0, train_loss=0.00818, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 120/727 [00:34<02:53,  3.49it/s, loss=0.00913, v_num=0, train_loss=0.00818, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 120/727 [00:34<02:53,  3.49it/s, loss=0.0143, v_num=0, train_loss=0.00729, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.27it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.27it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 1:  99%|█████████▉| 720/727 [03:37<00:02,  3.30it/s, loss=0.0116, v_num=0, train_loss=0.00879, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1:  99%|█████████▉| 720/727 [03:37<00:02,  3.30it/s, loss=0.0116, v_num=0, train_loss=0.00879, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.19it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.19it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 1: 100%|██████████| 727/727 [03:38<00:00,  3.32it/s, loss=0.0116, v_num=0, train_loss=0.00879, val_loss=0.0272]\u001b[0m\n",
      "\u001b[32mEpoch 1: 100%|██████████| 727/727 [03:38<00:00,  3.32it/s, loss=0.0114, v_num=0, train_loss=0.0116, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32m#033[A\u001b[0m\n",
      "\u001b[32mEpoch 1: 100%|██████████| 727/727 [03:39<00:00,  3.31it/s, loss=0.0114, v_num=0, train_loss=0.0116, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:   4%|▍         | 30/727 [00:09<03:44,  3.11it/s, loss=0.0156, v_num=0, train_loss=0.00632, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:   4%|▍         | 30/727 [00:09<03:44,  3.11it/s, loss=0.0156, v_num=0, train_loss=0.00632, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:   4%|▍         | 30/727 [00:09<03:44,  3.10it/s, loss=0.00782, v_num=0, train_loss=0.00671, val_loss=0.0124]\u001b[0m\n",
      "\u001b[32mEpoch 1:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0114, v_num=0, train_loss=0.0116, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0114, v_num=0, train_loss=0.0116, val_loss=0.0156]\u001b[0m\n",
      "\u001b[34mEpoch 2:  21%|██        | 150/727 [00:42<02:44,  3.50it/s, loss=0.0143, v_num=0, train_loss=0.00729, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  21%|██        | 150/727 [00:42<02:44,  3.50it/s, loss=0.0143, v_num=0, train_loss=0.00729, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  21%|██        | 150/727 [00:42<02:44,  3.50it/s, loss=0.00804, v_num=0, train_loss=0.00515, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 2:   8%|▊         | 60/727 [00:18<03:30,  3.17it/s, loss=0.00782, v_num=0, train_loss=0.00671, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:   8%|▊         | 60/727 [00:18<03:30,  3.17it/s, loss=0.00782, v_num=0, train_loss=0.00671, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:   8%|▊         | 60/727 [00:18<03:30,  3.17it/s, loss=0.0111, v_num=0, train_loss=0.00699, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  25%|██▍       | 180/727 [00:51<02:36,  3.50it/s, loss=0.00804, v_num=0, train_loss=0.00515, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  25%|██▍       | 180/727 [00:51<02:36,  3.50it/s, loss=0.00804, v_num=0, train_loss=0.00515, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  25%|██▍       | 180/727 [00:51<02:36,  3.50it/s, loss=0.0176, v_num=0, train_loss=0.040, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:   4%|▍         | 30/727 [00:09<03:49,  3.04it/s, loss=0.0114, v_num=0, train_loss=0.0116, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:   4%|▍         | 30/727 [00:09<03:49,  3.04it/s, loss=0.0114, v_num=0, train_loss=0.0116, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:   4%|▍         | 30/727 [00:09<03:49,  3.03it/s, loss=0.0145, v_num=0, train_loss=0.00681, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  12%|█▏        | 90/727 [00:28<03:19,  3.19it/s, loss=0.0111, v_num=0, train_loss=0.00699, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  12%|█▏        | 90/727 [00:28<03:19,  3.19it/s, loss=0.0111, v_num=0, train_loss=0.00699, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  12%|█▏        | 90/727 [00:28<03:19,  3.19it/s, loss=0.00772, v_num=0, train_loss=0.00624, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  29%|██▉       | 210/727 [00:59<02:27,  3.51it/s, loss=0.0176, v_num=0, train_loss=0.040, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  29%|██▉       | 210/727 [00:59<02:27,  3.51it/s, loss=0.0176, v_num=0, train_loss=0.040, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  29%|██▉       | 210/727 [00:59<02:27,  3.51it/s, loss=0.0106, v_num=0, train_loss=0.00552, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:   8%|▊         | 60/727 [00:19<03:35,  3.09it/s, loss=0.0145, v_num=0, train_loss=0.00681, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:   8%|▊         | 60/727 [00:19<03:35,  3.09it/s, loss=0.0145, v_num=0, train_loss=0.00681, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:   8%|▊         | 60/727 [00:19<03:35,  3.09it/s, loss=0.0186, v_num=0, train_loss=0.00777, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  17%|█▋        | 120/727 [00:37<03:09,  3.20it/s, loss=0.00772, v_num=0, train_loss=0.00624, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  17%|█▋        | 120/727 [00:37<03:09,  3.20it/s, loss=0.00772, v_num=0, train_loss=0.00624, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  17%|█▋        | 120/727 [00:37<03:09,  3.20it/s, loss=0.0166, v_num=0, train_loss=0.039, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  33%|███▎      | 240/727 [01:08<02:18,  3.51it/s, loss=0.0106, v_num=0, train_loss=0.00552, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  33%|███▎      | 240/727 [01:08<02:18,  3.51it/s, loss=0.0106, v_num=0, train_loss=0.00552, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  33%|███▎      | 240/727 [01:08<02:18,  3.51it/s, loss=0.0141, v_num=0, train_loss=0.00667, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  12%|█▏        | 90/727 [00:29<03:25,  3.10it/s, loss=0.0186, v_num=0, train_loss=0.00777, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  12%|█▏        | 90/727 [00:29<03:25,  3.10it/s, loss=0.0186, v_num=0, train_loss=0.00777, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  12%|█▏        | 90/727 [00:29<03:25,  3.10it/s, loss=0.0198, v_num=0, train_loss=0.00825, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  21%|██        | 150/727 [00:46<03:00,  3.20it/s, loss=0.0166, v_num=0, train_loss=0.039, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  21%|██        | 150/727 [00:46<03:00,  3.20it/s, loss=0.0166, v_num=0, train_loss=0.039, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  21%|██        | 150/727 [00:46<03:00,  3.20it/s, loss=0.0134, v_num=0, train_loss=0.00523, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  37%|███▋      | 270/727 [01:16<02:09,  3.52it/s, loss=0.0141, v_num=0, train_loss=0.00667, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  37%|███▋      | 270/727 [01:16<02:09,  3.52it/s, loss=0.0141, v_num=0, train_loss=0.00667, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  37%|███▋      | 270/727 [01:16<02:10,  3.52it/s, loss=0.0114, v_num=0, train_loss=0.0109, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  17%|█▋        | 120/727 [00:38<03:15,  3.10it/s, loss=0.0198, v_num=0, train_loss=0.00825, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  17%|█▋        | 120/727 [00:38<03:15,  3.10it/s, loss=0.0198, v_num=0, train_loss=0.00825, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  17%|█▋        | 120/727 [00:38<03:15,  3.10it/s, loss=0.0118, v_num=0, train_loss=0.0394, val_loss=0.0156]\u001b[0m\n",
      "\u001b[34mEpoch 2:  41%|████▏     | 300/727 [01:25<02:01,  3.52it/s, loss=0.0114, v_num=0, train_loss=0.0109, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  41%|████▏     | 300/727 [01:25<02:01,  3.52it/s, loss=0.0114, v_num=0, train_loss=0.0109, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  41%|████▏     | 300/727 [01:25<02:01,  3.52it/s, loss=0.0136, v_num=0, train_loss=0.0378, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 2:  25%|██▍       | 180/727 [00:56<02:51,  3.20it/s, loss=0.0134, v_num=0, train_loss=0.00523, val_loss=0.0124]#015Epoch 2:  25%|██▍       | 180/727 [00:56<02:51,  3.20it/s, loss=0.0134, v_num=0, train_loss=0.00523, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  25%|██▍       | 180/727 [00:56<02:51,  3.20it/s, loss=0.0112, v_num=0, train_loss=0.00711, val_loss=0.0124]\u001b[0m\n",
      "\u001b[32mEpoch 2:  21%|██        | 150/727 [00:48<03:06,  3.09it/s, loss=0.0118, v_num=0, train_loss=0.0394, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  21%|██        | 150/727 [00:48<03:06,  3.09it/s, loss=0.0118, v_num=0, train_loss=0.0394, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  21%|██        | 150/727 [00:48<03:06,  3.09it/s, loss=0.00972, v_num=0, train_loss=0.00651, val_loss=0.0156]\u001b[0m\n",
      "\u001b[34mEpoch 2:  45%|████▌     | 330/727 [01:33<01:52,  3.52it/s, loss=0.0136, v_num=0, train_loss=0.0378, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  45%|████▌     | 330/727 [01:33<01:52,  3.52it/s, loss=0.0136, v_num=0, train_loss=0.0378, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  45%|████▌     | 330/727 [01:33<01:52,  3.52it/s, loss=0.0103, v_num=0, train_loss=0.00529, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 2:  29%|██▉       | 210/727 [01:05<02:41,  3.20it/s, loss=0.0112, v_num=0, train_loss=0.00711, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  29%|██▉       | 210/727 [01:05<02:41,  3.20it/s, loss=0.0112, v_num=0, train_loss=0.00711, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  29%|██▉       | 210/727 [01:05<02:41,  3.19it/s, loss=0.013, v_num=0, train_loss=0.00709, val_loss=0.0124]\u001b[0m\n",
      "\u001b[32mEpoch 2:  25%|██▍       | 180/727 [00:58<02:57,  3.09it/s, loss=0.00972, v_num=0, train_loss=0.00651, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  25%|██▍       | 180/727 [00:58<02:57,  3.09it/s, loss=0.00972, v_num=0, train_loss=0.00651, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  25%|██▍       | 180/727 [00:58<02:57,  3.09it/s, loss=0.0149, v_num=0, train_loss=0.0352, val_loss=0.0156]\u001b[0m\n",
      "\u001b[34mEpoch 2:  50%|████▉     | 360/727 [01:42<01:44,  3.52it/s, loss=0.0103, v_num=0, train_loss=0.00529, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  50%|████▉     | 360/727 [01:42<01:44,  3.52it/s, loss=0.0103, v_num=0, train_loss=0.00529, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  50%|████▉     | 360/727 [01:42<01:44,  3.52it/s, loss=0.0127, v_num=0, train_loss=0.00534, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 2:  33%|███▎      | 240/727 [01:15<02:32,  3.20it/s, loss=0.013, v_num=0, train_loss=0.00709, val_loss=0.0124]#015Epoch 2:  33%|███▎      | 240/727 [01:15<02:32,  3.20it/s, loss=0.013, v_num=0, train_loss=0.00709, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  33%|███▎      | 240/727 [01:15<02:32,  3.19it/s, loss=0.0117, v_num=0, train_loss=0.0051, val_loss=0.0124]\u001b[0m\n",
      "\u001b[32mEpoch 2:  29%|██▉       | 210/727 [01:08<02:47,  3.08it/s, loss=0.0149, v_num=0, train_loss=0.0352, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  29%|██▉       | 210/727 [01:08<02:47,  3.08it/s, loss=0.0149, v_num=0, train_loss=0.0352, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  29%|██▉       | 210/727 [01:08<02:47,  3.08it/s, loss=0.0186, v_num=0, train_loss=0.00678, val_loss=0.0156]\u001b[0m\n",
      "\u001b[34mEpoch 2:  54%|█████▎    | 390/727 [01:50<01:35,  3.52it/s, loss=0.0127, v_num=0, train_loss=0.00534, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  54%|█████▎    | 390/727 [01:50<01:35,  3.52it/s, loss=0.0127, v_num=0, train_loss=0.00534, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  54%|█████▎    | 390/727 [01:50<01:35,  3.52it/s, loss=0.00758, v_num=0, train_loss=0.00552, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 2:  37%|███▋      | 270/727 [01:24<02:23,  3.20it/s, loss=0.0117, v_num=0, train_loss=0.0051, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  37%|███▋      | 270/727 [01:24<02:23,  3.20it/s, loss=0.0117, v_num=0, train_loss=0.0051, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  37%|███▋      | 270/727 [01:24<02:23,  3.19it/s, loss=0.0123, v_num=0, train_loss=0.00716, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  58%|█████▊    | 420/727 [01:59<01:27,  3.52it/s, loss=0.00758, v_num=0, train_loss=0.00552, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  58%|█████▊    | 420/727 [01:59<01:27,  3.52it/s, loss=0.00758, v_num=0, train_loss=0.00552, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  58%|█████▊    | 420/727 [01:59<01:27,  3.52it/s, loss=0.0137, v_num=0, train_loss=0.00531, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  33%|███▎      | 240/727 [01:17<02:37,  3.08it/s, loss=0.0186, v_num=0, train_loss=0.00678, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  33%|███▎      | 240/727 [01:17<02:37,  3.08it/s, loss=0.0186, v_num=0, train_loss=0.00678, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  33%|███▎      | 240/727 [01:17<02:38,  3.08it/s, loss=0.0179, v_num=0, train_loss=0.00653, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  41%|████▏     | 300/727 [01:33<02:13,  3.20it/s, loss=0.0123, v_num=0, train_loss=0.00716, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  41%|████▏     | 300/727 [01:33<02:13,  3.20it/s, loss=0.0123, v_num=0, train_loss=0.00716, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  41%|████▏     | 300/727 [01:33<02:13,  3.19it/s, loss=0.0128, v_num=0, train_loss=0.00502, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  62%|██████▏   | 450/727 [02:07<01:18,  3.52it/s, loss=0.0137, v_num=0, train_loss=0.00531, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  62%|██████▏   | 450/727 [02:07<01:18,  3.52it/s, loss=0.0137, v_num=0, train_loss=0.00531, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  62%|██████▏   | 450/727 [02:07<01:18,  3.52it/s, loss=0.00799, v_num=0, train_loss=0.00612, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  37%|███▋      | 270/727 [01:27<02:28,  3.08it/s, loss=0.0179, v_num=0, train_loss=0.00653, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  37%|███▋      | 270/727 [01:27<02:28,  3.08it/s, loss=0.0179, v_num=0, train_loss=0.00653, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  37%|███▋      | 270/727 [01:27<02:28,  3.08it/s, loss=0.0138, v_num=0, train_loss=0.0115, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  45%|████▌     | 330/727 [01:43<02:04,  3.20it/s, loss=0.0128, v_num=0, train_loss=0.00502, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  45%|████▌     | 330/727 [01:43<02:04,  3.20it/s, loss=0.0128, v_num=0, train_loss=0.00502, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  45%|████▌     | 330/727 [01:43<02:04,  3.20it/s, loss=0.00898, v_num=0, train_loss=0.00588, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  66%|██████▌   | 480/727 [02:16<01:10,  3.52it/s, loss=0.00799, v_num=0, train_loss=0.00612, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  66%|██████▌   | 480/727 [02:16<01:10,  3.52it/s, loss=0.00799, v_num=0, train_loss=0.00612, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  66%|██████▌   | 480/727 [02:16<01:10,  3.52it/s, loss=0.00715, v_num=0, train_loss=0.00521, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  41%|████▏     | 300/727 [01:37<02:18,  3.08it/s, loss=0.0138, v_num=0, train_loss=0.0115, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  41%|████▏     | 300/727 [01:37<02:18,  3.08it/s, loss=0.0138, v_num=0, train_loss=0.0115, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  41%|████▏     | 300/727 [01:37<02:18,  3.08it/s, loss=0.0149, v_num=0, train_loss=0.060, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  50%|████▉     | 360/727 [01:52<01:54,  3.20it/s, loss=0.00898, v_num=0, train_loss=0.00588, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  50%|████▉     | 360/727 [01:52<01:54,  3.20it/s, loss=0.00898, v_num=0, train_loss=0.00588, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  50%|████▉     | 360/727 [01:52<01:54,  3.20it/s, loss=0.00818, v_num=0, train_loss=0.00569, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  70%|███████   | 510/727 [02:24<01:01,  3.52it/s, loss=0.00715, v_num=0, train_loss=0.00521, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  70%|███████   | 510/727 [02:24<01:01,  3.52it/s, loss=0.00715, v_num=0, train_loss=0.00521, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  70%|███████   | 510/727 [02:24<01:01,  3.52it/s, loss=0.00693, v_num=0, train_loss=0.00414, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  45%|████▌     | 330/727 [01:46<02:08,  3.09it/s, loss=0.0149, v_num=0, train_loss=0.060, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  45%|████▌     | 330/727 [01:46<02:08,  3.09it/s, loss=0.0149, v_num=0, train_loss=0.060, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  45%|████▌     | 330/727 [01:46<02:08,  3.09it/s, loss=0.0116, v_num=0, train_loss=0.00643, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  54%|█████▎    | 390/727 [02:02<01:45,  3.20it/s, loss=0.00818, v_num=0, train_loss=0.00569, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  54%|█████▎    | 390/727 [02:02<01:45,  3.20it/s, loss=0.00818, v_num=0, train_loss=0.00569, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  54%|█████▎    | 390/727 [02:02<01:45,  3.20it/s, loss=0.00983, v_num=0, train_loss=0.00508, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  74%|███████▍  | 540/727 [02:33<00:53,  3.52it/s, loss=0.00693, v_num=0, train_loss=0.00414, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  74%|███████▍  | 540/727 [02:33<00:53,  3.52it/s, loss=0.00693, v_num=0, train_loss=0.00414, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  74%|███████▍  | 540/727 [02:33<00:53,  3.52it/s, loss=0.0112, v_num=0, train_loss=0.0055, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  50%|████▉     | 360/727 [01:56<01:58,  3.09it/s, loss=0.0116, v_num=0, train_loss=0.00643, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  50%|████▉     | 360/727 [01:56<01:58,  3.09it/s, loss=0.0116, v_num=0, train_loss=0.00643, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  50%|████▉     | 360/727 [01:56<01:58,  3.09it/s, loss=0.0141, v_num=0, train_loss=0.00592, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  58%|█████▊    | 420/727 [02:11<01:36,  3.20it/s, loss=0.00983, v_num=0, train_loss=0.00508, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  58%|█████▊    | 420/727 [02:11<01:36,  3.20it/s, loss=0.00983, v_num=0, train_loss=0.00508, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  58%|█████▊    | 420/727 [02:11<01:36,  3.20it/s, loss=0.00775, v_num=0, train_loss=0.0223, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  78%|███████▊  | 570/727 [02:41<00:44,  3.52it/s, loss=0.0112, v_num=0, train_loss=0.0055, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  78%|███████▊  | 570/727 [02:41<00:44,  3.52it/s, loss=0.0112, v_num=0, train_loss=0.0055, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  78%|███████▊  | 570/727 [02:41<00:44,  3.52it/s, loss=0.0135, v_num=0, train_loss=0.00555, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  54%|█████▎    | 390/727 [02:06<01:49,  3.09it/s, loss=0.0141, v_num=0, train_loss=0.00592, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  54%|█████▎    | 390/727 [02:06<01:49,  3.09it/s, loss=0.0141, v_num=0, train_loss=0.00592, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  54%|█████▎    | 390/727 [02:06<01:49,  3.09it/s, loss=0.0135, v_num=0, train_loss=0.00733, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  62%|██████▏   | 450/727 [02:20<01:26,  3.20it/s, loss=0.00775, v_num=0, train_loss=0.0223, val_loss=0.0124]#015Epoch 2:  62%|██████▏   | 450/727 [02:20<01:26,  3.20it/s, loss=0.00775, v_num=0, train_loss=0.0223, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  62%|██████▏   | 450/727 [02:20<01:26,  3.20it/s, loss=0.0147, v_num=0, train_loss=0.00629, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 600/727 [02:50<00:36,  3.52it/s, loss=0.0135, v_num=0, train_loss=0.00555, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 600/727 [02:50<00:36,  3.52it/s, loss=0.0135, v_num=0, train_loss=0.00555, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 600/727 [02:50<00:36,  3.52it/s, loss=0.0135, v_num=0, train_loss=0.00615, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  58%|█████▊    | 420/727 [02:15<01:39,  3.09it/s, loss=0.0135, v_num=0, train_loss=0.00733, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  58%|█████▊    | 420/727 [02:15<01:39,  3.09it/s, loss=0.0135, v_num=0, train_loss=0.00733, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  58%|█████▊    | 420/727 [02:15<01:39,  3.09it/s, loss=0.00986, v_num=0, train_loss=0.00651, val_loss=0.0156]\u001b[0m\n",
      "\u001b[34mEpoch 2:  87%|████████▋ | 630/727 [02:58<00:27,  3.52it/s, loss=0.0135, v_num=0, train_loss=0.00615, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  87%|████████▋ | 630/727 [02:58<00:27,  3.52it/s, loss=0.0135, v_num=0, train_loss=0.00615, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  87%|████████▋ | 630/727 [02:58<00:27,  3.52it/s, loss=0.0149, v_num=0, train_loss=0.00541, val_loss=0.0142]\u001b[0m\n",
      "\u001b[35mEpoch 2:  66%|██████▌   | 480/727 [02:30<01:17,  3.20it/s, loss=0.0147, v_num=0, train_loss=0.00629, val_loss=0.0124]#015Epoch 2:  66%|██████▌   | 480/727 [02:30<01:17,  3.20it/s, loss=0.0147, v_num=0, train_loss=0.00629, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  66%|██████▌   | 480/727 [02:30<01:17,  3.20it/s, loss=0.00924, v_num=0, train_loss=0.00775, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  91%|█████████ | 660/727 [03:06<00:18,  3.54it/s, loss=0.0149, v_num=0, train_loss=0.00541, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  91%|█████████ | 660/727 [03:06<00:18,  3.54it/s, loss=0.0149, v_num=0, train_loss=0.00541, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:03, 11.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:03, 11.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  95%|█████████▍| 690/727 [03:09<00:10,  3.65it/s, loss=0.0149, v_num=0, train_loss=0.00541, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  95%|█████████▍| 690/727 [03:09<00:10,  3.65it/s, loss=0.0149, v_num=0, train_loss=0.00541, val_loss=0.0142]\u001b[0m\n",
      "\u001b[32mEpoch 2:  62%|██████▏   | 450/727 [02:25<01:29,  3.10it/s, loss=0.00986, v_num=0, train_loss=0.00651, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  62%|██████▏   | 450/727 [02:25<01:29,  3.10it/s, loss=0.00986, v_num=0, train_loss=0.00651, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  62%|██████▏   | 450/727 [02:25<01:29,  3.10it/s, loss=0.00614, v_num=0, train_loss=0.00661, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  70%|███████   | 510/727 [02:39<01:07,  3.20it/s, loss=0.00924, v_num=0, train_loss=0.00775, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  70%|███████   | 510/727 [02:39<01:07,  3.20it/s, loss=0.00924, v_num=0, train_loss=0.00775, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  70%|███████   | 510/727 [02:39<01:07,  3.20it/s, loss=0.0155, v_num=0, train_loss=0.00442, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01, 10.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01, 10.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  99%|█████████▉| 720/727 [03:11<00:01,  3.75it/s, loss=0.0149, v_num=0, train_loss=0.00541, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2:  99%|█████████▉| 720/727 [03:11<00:01,  3.75it/s, loss=0.0149, v_num=0, train_loss=0.00541, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 73/73 [00:06<00:00, 10.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 73/73 [00:06<00:00, 10.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 727/727 [03:12<00:00,  3.77it/s, loss=0.0149, v_num=0, train_loss=0.00541, val_loss=0.0142]\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 727/727 [03:12<00:00,  3.77it/s, loss=0.00539, v_num=0, train_loss=0.00286, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 727/727 [03:13<00:00,  3.76it/s, loss=0.00539, v_num=0, train_loss=0.00286, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 2:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.00539, v_num=0, train_loss=0.00286, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.00539, v_num=0, train_loss=0.00286, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 2:  66%|██████▌   | 480/727 [02:34<01:19,  3.10it/s, loss=0.00614, v_num=0, train_loss=0.00661, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  66%|██████▌   | 480/727 [02:34<01:19,  3.10it/s, loss=0.00614, v_num=0, train_loss=0.00661, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  66%|██████▌   | 480/727 [02:34<01:19,  3.10it/s, loss=0.0104, v_num=0, train_loss=0.00604, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  74%|███████▍  | 540/727 [02:48<00:58,  3.20it/s, loss=0.0155, v_num=0, train_loss=0.00442, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  74%|███████▍  | 540/727 [02:48<00:58,  3.20it/s, loss=0.0155, v_num=0, train_loss=0.00442, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  74%|███████▍  | 540/727 [02:48<00:58,  3.20it/s, loss=0.021, v_num=0, train_loss=0.019, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 3:   4%|▍         | 30/727 [00:08<03:27,  3.35it/s, loss=0.00539, v_num=0, train_loss=0.00286, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:   4%|▍         | 30/727 [00:08<03:27,  3.35it/s, loss=0.00539, v_num=0, train_loss=0.00286, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:   4%|▍         | 30/727 [00:08<03:28,  3.35it/s, loss=0.00562, v_num=0, train_loss=0.00498, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 2:  70%|███████   | 510/727 [02:44<01:09,  3.10it/s, loss=0.0104, v_num=0, train_loss=0.00604, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  70%|███████   | 510/727 [02:44<01:09,  3.10it/s, loss=0.0104, v_num=0, train_loss=0.00604, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  70%|███████   | 510/727 [02:44<01:09,  3.10it/s, loss=0.0109, v_num=0, train_loss=0.00576, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  78%|███████▊  | 570/727 [02:58<00:49,  3.20it/s, loss=0.021, v_num=0, train_loss=0.019, val_loss=0.0124]#015Epoch 2:  78%|███████▊  | 570/727 [02:58<00:49,  3.20it/s, loss=0.021, v_num=0, train_loss=0.019, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  78%|███████▊  | 570/727 [02:58<00:49,  3.20it/s, loss=0.00557, v_num=0, train_loss=0.00468, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 3:   8%|▊         | 60/727 [00:17<03:14,  3.43it/s, loss=0.00562, v_num=0, train_loss=0.00498, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:   8%|▊         | 60/727 [00:17<03:14,  3.43it/s, loss=0.00562, v_num=0, train_loss=0.00498, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:   8%|▊         | 60/727 [00:17<03:14,  3.42it/s, loss=0.0138, v_num=0, train_loss=0.0353, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 2:  74%|███████▍  | 540/727 [02:53<01:00,  3.10it/s, loss=0.0109, v_num=0, train_loss=0.00576, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  74%|███████▍  | 540/727 [02:53<01:00,  3.10it/s, loss=0.0109, v_num=0, train_loss=0.00576, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  74%|███████▍  | 540/727 [02:54<01:00,  3.10it/s, loss=0.00701, v_num=0, train_loss=0.00536, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  83%|████████▎ | 600/727 [03:07<00:39,  3.20it/s, loss=0.00557, v_num=0, train_loss=0.00468, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  83%|████████▎ | 600/727 [03:07<00:39,  3.20it/s, loss=0.00557, v_num=0, train_loss=0.00468, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  83%|████████▎ | 600/727 [03:07<00:39,  3.20it/s, loss=0.00872, v_num=0, train_loss=0.00494, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 3:  12%|█▏        | 90/727 [00:26<03:04,  3.45it/s, loss=0.0138, v_num=0, train_loss=0.0353, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  12%|█▏        | 90/727 [00:26<03:04,  3.45it/s, loss=0.0138, v_num=0, train_loss=0.0353, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  12%|█▏        | 90/727 [00:26<03:04,  3.44it/s, loss=0.00787, v_num=0, train_loss=0.00437, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 2:  78%|███████▊  | 570/727 [03:03<00:50,  3.11it/s, loss=0.00701, v_num=0, train_loss=0.00536, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  78%|███████▊  | 570/727 [03:03<00:50,  3.11it/s, loss=0.00701, v_num=0, train_loss=0.00536, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  78%|███████▊  | 570/727 [03:03<00:50,  3.11it/s, loss=0.0111, v_num=0, train_loss=0.00621, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:  87%|████████▋ | 630/727 [03:16<00:30,  3.20it/s, loss=0.00872, v_num=0, train_loss=0.00494, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  87%|████████▋ | 630/727 [03:16<00:30,  3.20it/s, loss=0.00872, v_num=0, train_loss=0.00494, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  87%|████████▋ | 630/727 [03:16<00:30,  3.20it/s, loss=0.0114, v_num=0, train_loss=0.00504, val_loss=0.0124]\u001b[0m\n",
      "\u001b[34mEpoch 3:  17%|█▋        | 120/727 [00:34<02:55,  3.46it/s, loss=0.00787, v_num=0, train_loss=0.00437, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  17%|█▋        | 120/727 [00:34<02:55,  3.46it/s, loss=0.00787, v_num=0, train_loss=0.00437, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  17%|█▋        | 120/727 [00:34<02:55,  3.45it/s, loss=0.00753, v_num=0, train_loss=0.00442, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 2:  91%|█████████ | 660/727 [03:24<00:20,  3.22it/s, loss=0.0114, v_num=0, train_loss=0.00504, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  91%|█████████ | 660/727 [03:24<00:20,  3.22it/s, loss=0.0114, v_num=0, train_loss=0.00504, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:04, 10.45it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:04, 10.45it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 2:  95%|█████████▍| 690/727 [03:28<00:11,  3.32it/s, loss=0.0114, v_num=0, train_loss=0.00504, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  95%|█████████▍| 690/727 [03:28<00:11,  3.32it/s, loss=0.0114, v_num=0, train_loss=0.00504, val_loss=0.0124]\u001b[0m\n",
      "\u001b[32mEpoch 2:  83%|████████▎ | 600/727 [03:13<00:40,  3.11it/s, loss=0.0111, v_num=0, train_loss=0.00621, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  83%|████████▎ | 600/727 [03:13<00:40,  3.11it/s, loss=0.0111, v_num=0, train_loss=0.00621, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  83%|████████▎ | 600/727 [03:13<00:40,  3.11it/s, loss=0.0107, v_num=0, train_loss=0.0361, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.88it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 3:  21%|██        | 150/727 [00:43<02:46,  3.46it/s, loss=0.00753, v_num=0, train_loss=0.00442, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  21%|██        | 150/727 [00:43<02:46,  3.46it/s, loss=0.00753, v_num=0, train_loss=0.00442, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  21%|██        | 150/727 [00:43<02:46,  3.46it/s, loss=0.00985, v_num=0, train_loss=0.00419, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 2:  99%|█████████▉| 720/727 [03:31<00:02,  3.41it/s, loss=0.0114, v_num=0, train_loss=0.00504, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2:  99%|█████████▉| 720/727 [03:31<00:02,  3.41it/s, loss=0.0114, v_num=0, train_loss=0.00504, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.82it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.82it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 2: 100%|██████████| 727/727 [03:31<00:00,  3.43it/s, loss=0.0114, v_num=0, train_loss=0.00504, val_loss=0.0124]\u001b[0m\n",
      "\u001b[35mEpoch 2: 100%|██████████| 727/727 [03:32<00:00,  3.43it/s, loss=0.00745, v_num=0, train_loss=0.00429, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mEpoch 2: 100%|██████████| 727/727 [03:32<00:00,  3.42it/s, loss=0.00745, v_num=0, train_loss=0.00429, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 2:  87%|████████▋ | 630/727 [03:22<00:31,  3.11it/s, loss=0.0107, v_num=0, train_loss=0.0361, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  87%|████████▋ | 630/727 [03:22<00:31,  3.11it/s, loss=0.0107, v_num=0, train_loss=0.0361, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  87%|████████▋ | 630/727 [03:22<00:31,  3.11it/s, loss=0.0108, v_num=0, train_loss=0.00554, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 2:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.00745, v_num=0, train_loss=0.00429, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.00745, v_num=0, train_loss=0.00429, val_loss=0.0111]\u001b[0m\n",
      "\u001b[34mEpoch 3:  25%|██▍       | 180/727 [00:52<02:38,  3.46it/s, loss=0.00985, v_num=0, train_loss=0.00419, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  25%|██▍       | 180/727 [00:52<02:38,  3.46it/s, loss=0.00985, v_num=0, train_loss=0.00419, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  25%|██▍       | 180/727 [00:52<02:38,  3.46it/s, loss=0.01, v_num=0, train_loss=0.0416, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 2:  91%|█████████ | 660/727 [03:31<00:21,  3.13it/s, loss=0.0108, v_num=0, train_loss=0.00554, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  91%|█████████ | 660/727 [03:31<00:21,  3.13it/s, loss=0.0108, v_num=0, train_loss=0.00554, val_loss=0.0156]\u001b[0m\n",
      "\u001b[35mEpoch 3:   4%|▍         | 30/727 [00:09<03:44,  3.10it/s, loss=0.00745, v_num=0, train_loss=0.00429, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:   4%|▍         | 30/727 [00:09<03:44,  3.10it/s, loss=0.00745, v_num=0, train_loss=0.00429, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:   4%|▍         | 30/727 [00:09<03:45,  3.10it/s, loss=0.00818, v_num=0, train_loss=0.00448, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  41%|████      | 30/73 [00:03<00:04,  9.59it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  41%|████      | 30/73 [00:03<00:04,  9.59it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 2:  95%|█████████▍| 690/727 [03:34<00:11,  3.22it/s, loss=0.0108, v_num=0, train_loss=0.00554, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  95%|█████████▍| 690/727 [03:34<00:11,  3.22it/s, loss=0.0108, v_num=0, train_loss=0.00554, val_loss=0.0156]\u001b[0m\n",
      "\u001b[34mEpoch 3:  29%|██▉       | 210/727 [01:00<02:29,  3.46it/s, loss=0.01, v_num=0, train_loss=0.0416, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  29%|██▉       | 210/727 [01:00<02:29,  3.46it/s, loss=0.01, v_num=0, train_loss=0.0416, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  29%|██▉       | 210/727 [01:00<02:29,  3.46it/s, loss=0.0102, v_num=0, train_loss=0.0047, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.06it/s]#033[A\u001b[0m\n",
      "\u001b[32m#015Validation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.06it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 2:  99%|█████████▉| 720/727 [03:37<00:02,  3.30it/s, loss=0.0108, v_num=0, train_loss=0.00554, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2:  99%|█████████▉| 720/727 [03:37<00:02,  3.30it/s, loss=0.0108, v_num=0, train_loss=0.00554, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0: 100%|██████████| 73/73 [00:08<00:00,  8.96it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0: 100%|██████████| 73/73 [00:08<00:00,  8.96it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 2: 100%|██████████| 727/727 [03:38<00:00,  3.32it/s, loss=0.0108, v_num=0, train_loss=0.00554, val_loss=0.0156]\u001b[0m\n",
      "\u001b[32mEpoch 2: 100%|██████████| 727/727 [03:39<00:00,  3.32it/s, loss=0.0154, v_num=0, train_loss=0.00592, val_loss=0.014]\u001b[0m\n",
      "\u001b[32m#033[A\u001b[0m\n",
      "\u001b[32mEpoch 2: 100%|██████████| 727/727 [03:39<00:00,  3.31it/s, loss=0.0154, v_num=0, train_loss=0.00592, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3:   8%|▊         | 60/727 [00:18<03:30,  3.17it/s, loss=0.00818, v_num=0, train_loss=0.00448, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:   8%|▊         | 60/727 [00:18<03:30,  3.17it/s, loss=0.00818, v_num=0, train_loss=0.00448, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:   8%|▊         | 60/727 [00:18<03:30,  3.17it/s, loss=0.00828, v_num=0, train_loss=0.00492, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 2:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0154, v_num=0, train_loss=0.00592, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:   0%|          | 0/727 [00:00<?, ?it/s, loss=0.0154, v_num=0, train_loss=0.00592, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  33%|███▎      | 240/727 [01:09<02:20,  3.46it/s, loss=0.0102, v_num=0, train_loss=0.0047, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  33%|███▎      | 240/727 [01:09<02:20,  3.46it/s, loss=0.0102, v_num=0, train_loss=0.0047, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  33%|███▎      | 240/727 [01:09<02:20,  3.46it/s, loss=0.00652, v_num=0, train_loss=0.00611, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  12%|█▏        | 90/727 [00:28<03:19,  3.19it/s, loss=0.00828, v_num=0, train_loss=0.00492, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  12%|█▏        | 90/727 [00:28<03:19,  3.19it/s, loss=0.00828, v_num=0, train_loss=0.00492, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  12%|█▏        | 90/727 [00:28<03:20,  3.18it/s, loss=0.00581, v_num=0, train_loss=0.00446, val_loss=0.0111]\u001b[0m\n",
      "\u001b[34mEpoch 3:  37%|███▋      | 270/727 [01:17<02:11,  3.47it/s, loss=0.00652, v_num=0, train_loss=0.00611, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  37%|███▋      | 270/727 [01:17<02:11,  3.47it/s, loss=0.00652, v_num=0, train_loss=0.00611, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  37%|███▋      | 270/727 [01:17<02:11,  3.47it/s, loss=0.0103, v_num=0, train_loss=0.00716, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 3:   4%|▍         | 30/727 [00:10<03:54,  2.97it/s, loss=0.0154, v_num=0, train_loss=0.00592, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:   4%|▍         | 30/727 [00:10<03:54,  2.97it/s, loss=0.0154, v_num=0, train_loss=0.00592, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:   4%|▍         | 30/727 [00:10<03:55,  2.96it/s, loss=0.0101, v_num=0, train_loss=0.00741, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3:  17%|█▋        | 120/727 [00:37<03:10,  3.19it/s, loss=0.00581, v_num=0, train_loss=0.00446, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  17%|█▋        | 120/727 [00:37<03:10,  3.19it/s, loss=0.00581, v_num=0, train_loss=0.00446, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  17%|█▋        | 120/727 [00:37<03:10,  3.19it/s, loss=0.0118, v_num=0, train_loss=0.00464, val_loss=0.0111]\u001b[0m\n",
      "\u001b[34mEpoch 3:  41%|████▏     | 300/727 [01:26<02:02,  3.47it/s, loss=0.0103, v_num=0, train_loss=0.00716, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  41%|████▏     | 300/727 [01:26<02:02,  3.47it/s, loss=0.0103, v_num=0, train_loss=0.00716, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  41%|████▏     | 300/727 [01:26<02:03,  3.47it/s, loss=0.00743, v_num=0, train_loss=0.00417, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 3:   8%|▊         | 60/727 [00:19<03:40,  3.02it/s, loss=0.0101, v_num=0, train_loss=0.00741, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:   8%|▊         | 60/727 [00:19<03:40,  3.02it/s, loss=0.0101, v_num=0, train_loss=0.00741, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:   8%|▊         | 60/727 [00:19<03:40,  3.02it/s, loss=0.00887, v_num=0, train_loss=0.00505, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  45%|████▌     | 330/727 [01:34<01:54,  3.48it/s, loss=0.00743, v_num=0, train_loss=0.00417, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  45%|████▌     | 330/727 [01:34<01:54,  3.48it/s, loss=0.00743, v_num=0, train_loss=0.00417, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  45%|████▌     | 330/727 [01:34<01:54,  3.48it/s, loss=0.00601, v_num=0, train_loss=0.0039, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  21%|██        | 150/727 [00:46<03:00,  3.19it/s, loss=0.0118, v_num=0, train_loss=0.00464, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  21%|██        | 150/727 [00:46<03:00,  3.19it/s, loss=0.0118, v_num=0, train_loss=0.00464, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  21%|██        | 150/727 [00:46<03:00,  3.19it/s, loss=0.0107, v_num=0, train_loss=0.00537, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  12%|█▏        | 90/727 [00:29<03:29,  3.04it/s, loss=0.00887, v_num=0, train_loss=0.00505, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  12%|█▏        | 90/727 [00:29<03:29,  3.04it/s, loss=0.00887, v_num=0, train_loss=0.00505, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  12%|█▏        | 90/727 [00:29<03:29,  3.04it/s, loss=0.00786, v_num=0, train_loss=0.00488, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  50%|████▉     | 360/727 [01:43<01:45,  3.48it/s, loss=0.00601, v_num=0, train_loss=0.0039, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  50%|████▉     | 360/727 [01:43<01:45,  3.48it/s, loss=0.00601, v_num=0, train_loss=0.0039, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  50%|████▉     | 360/727 [01:43<01:45,  3.48it/s, loss=0.00499, v_num=0, train_loss=0.00592, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  25%|██▍       | 180/727 [00:56<02:51,  3.19it/s, loss=0.0107, v_num=0, train_loss=0.00537, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  25%|██▍       | 180/727 [00:56<02:51,  3.19it/s, loss=0.0107, v_num=0, train_loss=0.00537, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  25%|██▍       | 180/727 [00:56<02:51,  3.19it/s, loss=0.00604, v_num=0, train_loss=0.00551, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  17%|█▋        | 120/727 [00:39<03:19,  3.05it/s, loss=0.00786, v_num=0, train_loss=0.00488, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  17%|█▋        | 120/727 [00:39<03:19,  3.05it/s, loss=0.00786, v_num=0, train_loss=0.00488, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  17%|█▋        | 120/727 [00:39<03:19,  3.04it/s, loss=0.0126, v_num=0, train_loss=0.00636, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  54%|█████▎    | 390/727 [01:51<01:36,  3.48it/s, loss=0.00499, v_num=0, train_loss=0.00592, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  54%|█████▎    | 390/727 [01:51<01:36,  3.48it/s, loss=0.00499, v_num=0, train_loss=0.00592, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  54%|█████▎    | 390/727 [01:51<01:36,  3.48it/s, loss=0.00935, v_num=0, train_loss=0.00445, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  29%|██▉       | 210/727 [01:05<02:41,  3.19it/s, loss=0.00604, v_num=0, train_loss=0.00551, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  29%|██▉       | 210/727 [01:05<02:41,  3.19it/s, loss=0.00604, v_num=0, train_loss=0.00551, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  29%|██▉       | 210/727 [01:05<02:41,  3.19it/s, loss=0.00994, v_num=0, train_loss=0.00479, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  21%|██        | 150/727 [00:49<03:09,  3.05it/s, loss=0.0126, v_num=0, train_loss=0.00636, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  21%|██        | 150/727 [00:49<03:09,  3.05it/s, loss=0.0126, v_num=0, train_loss=0.00636, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  21%|██        | 150/727 [00:49<03:09,  3.05it/s, loss=0.0124, v_num=0, train_loss=0.0308, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  58%|█████▊    | 420/727 [02:00<01:28,  3.49it/s, loss=0.00935, v_num=0, train_loss=0.00445, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  58%|█████▊    | 420/727 [02:00<01:28,  3.49it/s, loss=0.00935, v_num=0, train_loss=0.00445, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  58%|█████▊    | 420/727 [02:00<01:28,  3.49it/s, loss=0.00586, v_num=0, train_loss=0.00597, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  33%|███▎      | 240/727 [01:15<02:32,  3.19it/s, loss=0.00994, v_num=0, train_loss=0.00479, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  33%|███▎      | 240/727 [01:15<02:32,  3.19it/s, loss=0.00994, v_num=0, train_loss=0.00479, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  33%|███▎      | 240/727 [01:15<02:32,  3.19it/s, loss=0.00684, v_num=0, train_loss=0.0044, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  25%|██▍       | 180/727 [00:58<02:58,  3.06it/s, loss=0.0124, v_num=0, train_loss=0.0308, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  25%|██▍       | 180/727 [00:58<02:58,  3.06it/s, loss=0.0124, v_num=0, train_loss=0.0308, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  25%|██▍       | 180/727 [00:58<02:58,  3.06it/s, loss=0.00629, v_num=0, train_loss=0.00723, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  62%|██████▏   | 450/727 [02:08<01:19,  3.49it/s, loss=0.00586, v_num=0, train_loss=0.00597, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  62%|██████▏   | 450/727 [02:08<01:19,  3.49it/s, loss=0.00586, v_num=0, train_loss=0.00597, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  62%|██████▏   | 450/727 [02:08<01:19,  3.49it/s, loss=0.0131, v_num=0, train_loss=0.00451, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  37%|███▋      | 270/727 [01:24<02:23,  3.19it/s, loss=0.00684, v_num=0, train_loss=0.0044, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  37%|███▋      | 270/727 [01:24<02:23,  3.19it/s, loss=0.00684, v_num=0, train_loss=0.0044, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  37%|███▋      | 270/727 [01:24<02:23,  3.19it/s, loss=0.00884, v_num=0, train_loss=0.00391, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  29%|██▉       | 210/727 [01:08<02:48,  3.07it/s, loss=0.00629, v_num=0, train_loss=0.00723, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  29%|██▉       | 210/727 [01:08<02:48,  3.07it/s, loss=0.00629, v_num=0, train_loss=0.00723, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  29%|██▉       | 210/727 [01:08<02:48,  3.07it/s, loss=0.00974, v_num=0, train_loss=0.00529, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  66%|██████▌   | 480/727 [02:17<01:10,  3.49it/s, loss=0.0131, v_num=0, train_loss=0.00451, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  66%|██████▌   | 480/727 [02:17<01:10,  3.49it/s, loss=0.0131, v_num=0, train_loss=0.00451, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  66%|██████▌   | 480/727 [02:17<01:10,  3.49it/s, loss=0.00678, v_num=0, train_loss=0.00501, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  41%|████▏     | 300/727 [01:33<02:13,  3.20it/s, loss=0.00884, v_num=0, train_loss=0.00391, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  41%|████▏     | 300/727 [01:33<02:13,  3.20it/s, loss=0.00884, v_num=0, train_loss=0.00391, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  41%|████▏     | 300/727 [01:33<02:13,  3.19it/s, loss=0.00541, v_num=0, train_loss=0.00391, val_loss=0.0111]\u001b[0m\n",
      "\u001b[34mEpoch 3:  70%|███████   | 510/727 [02:25<01:02,  3.49it/s, loss=0.00678, v_num=0, train_loss=0.00501, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  70%|███████   | 510/727 [02:25<01:02,  3.49it/s, loss=0.00678, v_num=0, train_loss=0.00501, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  70%|███████   | 510/727 [02:26<01:02,  3.49it/s, loss=0.0101, v_num=0, train_loss=0.00521, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 3:  33%|███▎      | 240/727 [01:17<02:38,  3.08it/s, loss=0.00974, v_num=0, train_loss=0.00529, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  33%|███▎      | 240/727 [01:17<02:38,  3.08it/s, loss=0.00974, v_num=0, train_loss=0.00529, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  33%|███▎      | 240/727 [01:17<02:38,  3.08it/s, loss=0.0149, v_num=0, train_loss=0.00364, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3:  45%|████▌     | 330/727 [01:43<02:04,  3.20it/s, loss=0.00541, v_num=0, train_loss=0.00391, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  45%|████▌     | 330/727 [01:43<02:04,  3.20it/s, loss=0.00541, v_num=0, train_loss=0.00391, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  45%|████▌     | 330/727 [01:43<02:04,  3.20it/s, loss=0.0144, v_num=0, train_loss=0.00386, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  37%|███▋      | 270/727 [01:27<02:28,  3.09it/s, loss=0.0149, v_num=0, train_loss=0.00364, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  37%|███▋      | 270/727 [01:27<02:28,  3.09it/s, loss=0.0149, v_num=0, train_loss=0.00364, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  37%|███▋      | 270/727 [01:27<02:28,  3.09it/s, loss=0.0105, v_num=0, train_loss=0.0397, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  74%|███████▍  | 540/727 [02:34<00:53,  3.50it/s, loss=0.0101, v_num=0, train_loss=0.00521, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  74%|███████▍  | 540/727 [02:34<00:53,  3.50it/s, loss=0.0101, v_num=0, train_loss=0.00521, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  74%|███████▍  | 540/727 [02:34<00:53,  3.49it/s, loss=0.0087, v_num=0, train_loss=0.00399, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  50%|████▉     | 360/727 [01:52<01:54,  3.20it/s, loss=0.0144, v_num=0, train_loss=0.00386, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  50%|████▉     | 360/727 [01:52<01:54,  3.20it/s, loss=0.0144, v_num=0, train_loss=0.00386, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  50%|████▉     | 360/727 [01:52<01:54,  3.20it/s, loss=0.0089, v_num=0, train_loss=0.00513, val_loss=0.0111]\u001b[0m\n",
      "\u001b[34mEpoch 3:  78%|███████▊  | 570/727 [02:43<00:44,  3.50it/s, loss=0.0087, v_num=0, train_loss=0.00399, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  78%|███████▊  | 570/727 [02:43<00:44,  3.50it/s, loss=0.0087, v_num=0, train_loss=0.00399, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  78%|███████▊  | 570/727 [02:43<00:44,  3.50it/s, loss=0.0066, v_num=0, train_loss=0.0039, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 3:  41%|████▏     | 300/727 [01:36<02:17,  3.10it/s, loss=0.0105, v_num=0, train_loss=0.0397, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  41%|████▏     | 300/727 [01:36<02:17,  3.10it/s, loss=0.0105, v_num=0, train_loss=0.0397, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  41%|████▏     | 300/727 [01:36<02:17,  3.10it/s, loss=0.0135, v_num=0, train_loss=0.00563, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3:  54%|█████▎    | 390/727 [02:01<01:45,  3.20it/s, loss=0.0089, v_num=0, train_loss=0.00513, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  54%|█████▎    | 390/727 [02:01<01:45,  3.20it/s, loss=0.0089, v_num=0, train_loss=0.00513, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  54%|█████▎    | 390/727 [02:01<01:45,  3.20it/s, loss=0.0103, v_num=0, train_loss=0.00415, val_loss=0.0111]\u001b[0m\n",
      "\u001b[34mEpoch 3:  83%|████████▎ | 600/727 [02:51<00:36,  3.50it/s, loss=0.0066, v_num=0, train_loss=0.0039, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  83%|████████▎ | 600/727 [02:51<00:36,  3.50it/s, loss=0.0066, v_num=0, train_loss=0.0039, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  83%|████████▎ | 600/727 [02:51<00:36,  3.50it/s, loss=0.01, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[32mEpoch 3:  45%|████▌     | 330/727 [01:46<02:08,  3.10it/s, loss=0.0135, v_num=0, train_loss=0.00563, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  45%|████▌     | 330/727 [01:46<02:08,  3.10it/s, loss=0.0135, v_num=0, train_loss=0.00563, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  45%|████▌     | 330/727 [01:46<02:08,  3.10it/s, loss=0.00727, v_num=0, train_loss=0.0054, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  87%|████████▋ | 630/727 [03:00<00:27,  3.50it/s, loss=0.01, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  87%|████████▋ | 630/727 [03:00<00:27,  3.50it/s, loss=0.01, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  87%|████████▋ | 630/727 [03:00<00:27,  3.50it/s, loss=0.0103, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  58%|█████▊    | 420/727 [02:11<01:35,  3.20it/s, loss=0.0103, v_num=0, train_loss=0.00415, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  58%|█████▊    | 420/727 [02:11<01:35,  3.20it/s, loss=0.0103, v_num=0, train_loss=0.00415, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  58%|█████▊    | 420/727 [02:11<01:35,  3.20it/s, loss=0.0113, v_num=0, train_loss=0.00484, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  50%|████▉     | 360/727 [01:55<01:58,  3.10it/s, loss=0.00727, v_num=0, train_loss=0.0054, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  50%|████▉     | 360/727 [01:55<01:58,  3.10it/s, loss=0.00727, v_num=0, train_loss=0.0054, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  50%|████▉     | 360/727 [01:55<01:58,  3.10it/s, loss=0.0143, v_num=0, train_loss=0.00528, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 3:  91%|█████████ | 660/727 [03:07<00:19,  3.52it/s, loss=0.0103, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  91%|█████████ | 660/727 [03:07<00:19,  3.52it/s, loss=0.0103, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[35mEpoch 3:  62%|██████▏   | 450/727 [02:20<01:26,  3.20it/s, loss=0.0113, v_num=0, train_loss=0.00484, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  62%|██████▏   | 450/727 [02:20<01:26,  3.20it/s, loss=0.0113, v_num=0, train_loss=0.00484, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  62%|██████▏   | 450/727 [02:20<01:26,  3.20it/s, loss=0.00685, v_num=0, train_loss=0.00537, val_loss=0.0111]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:03, 11.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:03, 11.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 3:  95%|█████████▍| 690/727 [03:10<00:10,  3.62it/s, loss=0.0103, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  95%|█████████▍| 690/727 [03:10<00:10,  3.62it/s, loss=0.0103, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01, 10.86it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01, 10.86it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 3:  54%|█████▎    | 390/727 [02:05<01:48,  3.11it/s, loss=0.0143, v_num=0, train_loss=0.00528, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  54%|█████▎    | 390/727 [02:05<01:48,  3.11it/s, loss=0.0143, v_num=0, train_loss=0.00528, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  54%|█████▎    | 390/727 [02:05<01:48,  3.11it/s, loss=0.00738, v_num=0, train_loss=0.0057, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3:  99%|█████████▉| 720/727 [03:13<00:01,  3.73it/s, loss=0.0103, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3:  99%|█████████▉| 720/727 [03:13<00:01,  3.73it/s, loss=0.0103, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 73/73 [00:06<00:00, 10.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 73/73 [00:06<00:00, 10.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 727/727 [03:13<00:00,  3.75it/s, loss=0.0103, v_num=0, train_loss=0.00465, val_loss=0.0125]\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 727/727 [03:14<00:00,  3.74it/s, loss=0.00634, v_num=0, train_loss=0.00485, val_loss=0.012]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 727/727 [03:14<00:00,  3.73it/s, loss=0.00634, v_num=0, train_loss=0.00485, val_loss=0.012]\u001b[0m\n",
      "\u001b[35mEpoch 3:  66%|██████▌   | 480/727 [02:29<01:17,  3.21it/s, loss=0.00685, v_num=0, train_loss=0.00537, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  66%|██████▌   | 480/727 [02:29<01:17,  3.21it/s, loss=0.00685, v_num=0, train_loss=0.00537, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  66%|██████▌   | 480/727 [02:29<01:17,  3.21it/s, loss=0.00796, v_num=0, train_loss=0.00513, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  58%|█████▊    | 420/727 [02:15<01:38,  3.10it/s, loss=0.00738, v_num=0, train_loss=0.0057, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  58%|█████▊    | 420/727 [02:15<01:38,  3.10it/s, loss=0.00738, v_num=0, train_loss=0.0057, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  58%|█████▊    | 420/727 [02:15<01:38,  3.10it/s, loss=0.00816, v_num=0, train_loss=0.00495, val_loss=0.014]\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 727/727 [03:17<00:00,  3.68it/s, loss=0.00634, v_num=0, train_loss=0.00485, val_loss=0.012]\u001b[0m\n",
      "\u001b[34mtrain_model_output: trainer: <pytorch_lightning.trainer.trainer.Trainer object at 0x7f09bd3f78e0>\u001b[0m\n",
      "\u001b[34mtrain_model completed!\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34mcreate_model_input: LABEL_COLUMNS: ['general criticsm', 'disability shaming', 'racial prejudice', 'sexism', 'lgbtq+ phobia']\u001b[0m\n",
      "\u001b[34mcreate_model_input: trainer: <pytorch_lightning.trainer.trainer.Trainer object at 0x7f09bd3f78e0>\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mcreate_model_output: trained_model: TweetTagger(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (criterion): BCELoss()\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mcreate_model completed!\u001b[0m\n",
      "\u001b[34m====================================================================================================\u001b[0m\n",
      "\u001b[34m2022-07-19 05:59:07,059 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-07-19 05:59:07,060 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-07-19 05:59:07,060 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35mEpoch 3:  70%|███████   | 510/727 [02:38<01:07,  3.21it/s, loss=0.00796, v_num=0, train_loss=0.00513, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  70%|███████   | 510/727 [02:38<01:07,  3.21it/s, loss=0.00796, v_num=0, train_loss=0.00513, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  70%|███████   | 510/727 [02:39<01:07,  3.21it/s, loss=0.00664, v_num=0, train_loss=0.00556, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  62%|██████▏   | 450/727 [02:25<01:29,  3.10it/s, loss=0.00816, v_num=0, train_loss=0.00495, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  62%|██████▏   | 450/727 [02:25<01:29,  3.10it/s, loss=0.00816, v_num=0, train_loss=0.00495, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  62%|██████▏   | 450/727 [02:25<01:29,  3.10it/s, loss=0.0137, v_num=0, train_loss=0.00443, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3:  74%|███████▍  | 540/727 [02:48<00:58,  3.21it/s, loss=0.00664, v_num=0, train_loss=0.00556, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  74%|███████▍  | 540/727 [02:48<00:58,  3.21it/s, loss=0.00664, v_num=0, train_loss=0.00556, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  74%|███████▍  | 540/727 [02:48<00:58,  3.21it/s, loss=0.00711, v_num=0, train_loss=0.00367, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  66%|██████▌   | 480/727 [02:34<01:19,  3.10it/s, loss=0.0137, v_num=0, train_loss=0.00443, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  66%|██████▌   | 480/727 [02:34<01:19,  3.10it/s, loss=0.0137, v_num=0, train_loss=0.00443, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  66%|██████▌   | 480/727 [02:34<01:19,  3.10it/s, loss=0.0105, v_num=0, train_loss=0.00439, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3:  78%|███████▊  | 570/727 [02:57<00:48,  3.21it/s, loss=0.00711, v_num=0, train_loss=0.00367, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  78%|███████▊  | 570/727 [02:57<00:48,  3.21it/s, loss=0.00711, v_num=0, train_loss=0.00367, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  78%|███████▊  | 570/727 [02:57<00:48,  3.21it/s, loss=0.00568, v_num=0, train_loss=0.00439, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  70%|███████   | 510/727 [02:44<01:10,  3.10it/s, loss=0.0105, v_num=0, train_loss=0.00439, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  70%|███████   | 510/727 [02:44<01:10,  3.10it/s, loss=0.0105, v_num=0, train_loss=0.00439, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  70%|███████   | 510/727 [02:44<01:10,  3.10it/s, loss=0.0109, v_num=0, train_loss=0.00405, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3:  83%|████████▎ | 600/727 [03:06<00:39,  3.21it/s, loss=0.00568, v_num=0, train_loss=0.00439, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  83%|████████▎ | 600/727 [03:06<00:39,  3.21it/s, loss=0.00568, v_num=0, train_loss=0.00439, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  83%|████████▎ | 600/727 [03:06<00:39,  3.21it/s, loss=0.0113, v_num=0, train_loss=0.00433, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  74%|███████▍  | 540/727 [02:54<01:00,  3.10it/s, loss=0.0109, v_num=0, train_loss=0.00405, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  74%|███████▍  | 540/727 [02:54<01:00,  3.10it/s, loss=0.0109, v_num=0, train_loss=0.00405, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  74%|███████▍  | 540/727 [02:54<01:00,  3.09it/s, loss=0.0101, v_num=0, train_loss=0.00457, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3:  87%|████████▋ | 630/727 [03:16<00:30,  3.21it/s, loss=0.0113, v_num=0, train_loss=0.00433, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  87%|████████▋ | 630/727 [03:16<00:30,  3.21it/s, loss=0.0113, v_num=0, train_loss=0.00433, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  87%|████████▋ | 630/727 [03:16<00:30,  3.21it/s, loss=0.00626, v_num=0, train_loss=0.00458, val_loss=0.0111]\u001b[0m\n",
      "\u001b[32mEpoch 3:  78%|███████▊  | 570/727 [03:04<00:50,  3.09it/s, loss=0.0101, v_num=0, train_loss=0.00457, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  78%|███████▊  | 570/727 [03:04<00:50,  3.09it/s, loss=0.0101, v_num=0, train_loss=0.00457, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  78%|███████▊  | 570/727 [03:04<00:50,  3.09it/s, loss=0.00782, v_num=0, train_loss=0.0306, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 3:  91%|█████████ | 660/727 [03:24<00:20,  3.23it/s, loss=0.00626, v_num=0, train_loss=0.00458, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  91%|█████████ | 660/727 [03:24<00:20,  3.23it/s, loss=0.00626, v_num=0, train_loss=0.00458, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:04, 10.55it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  41%|████      | 30/73 [00:02<00:04, 10.55it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 3:  95%|█████████▍| 690/727 [03:27<00:11,  3.33it/s, loss=0.00626, v_num=0, train_loss=0.00458, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  95%|█████████▍| 690/727 [03:27<00:11,  3.33it/s, loss=0.00626, v_num=0, train_loss=0.00458, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01,  9.95it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:05<00:01,  9.95it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 3:  99%|█████████▉| 720/727 [03:30<00:02,  3.42it/s, loss=0.00626, v_num=0, train_loss=0.00458, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3:  99%|█████████▉| 720/727 [03:30<00:02,  3.42it/s, loss=0.00626, v_num=0, train_loss=0.00458, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.88it/s]#033[A\u001b[0m\n",
      "\u001b[35mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.88it/s]#033[A\u001b[0m\n",
      "\u001b[35mEpoch 3: 100%|██████████| 727/727 [03:31<00:00,  3.44it/s, loss=0.00626, v_num=0, train_loss=0.00458, val_loss=0.0111]\u001b[0m\n",
      "\u001b[35mEpoch 3: 100%|██████████| 727/727 [03:31<00:00,  3.44it/s, loss=0.00772, v_num=0, train_loss=0.00494, val_loss=0.0105]\u001b[0m\n",
      "\u001b[35m#033[A\u001b[0m\n",
      "\u001b[35mEpoch 3: 100%|██████████| 727/727 [03:31<00:00,  3.43it/s, loss=0.00772, v_num=0, train_loss=0.00494, val_loss=0.0105]\u001b[0m\n",
      "\u001b[32mEpoch 3:  83%|████████▎ | 600/727 [03:13<00:41,  3.09it/s, loss=0.00782, v_num=0, train_loss=0.0306, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  83%|████████▎ | 600/727 [03:13<00:41,  3.09it/s, loss=0.00782, v_num=0, train_loss=0.0306, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  83%|████████▎ | 600/727 [03:13<00:41,  3.09it/s, loss=0.0135, v_num=0, train_loss=0.00516, val_loss=0.014]\u001b[0m\n",
      "\u001b[35mEpoch 3: 100%|██████████| 727/727 [03:34<00:00,  3.38it/s, loss=0.00772, v_num=0, train_loss=0.00494, val_loss=0.0105]\u001b[0m\n",
      "\u001b[35mtrain_model_output: trainer: <pytorch_lightning.trainer.trainer.Trainer object at 0x7f1b7bdf82e0>\u001b[0m\n",
      "\u001b[35mtrain_model completed!\u001b[0m\n",
      "\u001b[35m====================================================================================================\u001b[0m\n",
      "\u001b[35mcreate_model_input: LABEL_COLUMNS: ['general criticsm', 'disability shaming', 'racial prejudice', 'sexism', 'lgbtq+ phobia']\u001b[0m\n",
      "\u001b[35mcreate_model_input: trainer: <pytorch_lightning.trainer.trainer.Trainer object at 0x7f1b7bdf82e0>\u001b[0m\n",
      "\u001b[35mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[35m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[35m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[35mcreate_model_output: trained_model: TweetTagger(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (criterion): BCELoss()\u001b[0m\n",
      "\u001b[35m)\u001b[0m\n",
      "\u001b[35mcreate_model completed!\u001b[0m\n",
      "\u001b[35m====================================================================================================\u001b[0m\n",
      "\u001b[35m2022-07-19 06:00:12,255 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2022-07-19 06:00:12,256 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2022-07-19 06:00:12,256 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32mEpoch 3:  87%|████████▋ | 630/727 [03:23<00:31,  3.10it/s, loss=0.0135, v_num=0, train_loss=0.00516, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  87%|████████▋ | 630/727 [03:23<00:31,  3.10it/s, loss=0.0135, v_num=0, train_loss=0.00516, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  87%|████████▋ | 630/727 [03:23<00:31,  3.09it/s, loss=0.00888, v_num=0, train_loss=0.00681, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:   0%|          | 0/73 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 3:  91%|█████████ | 660/727 [03:31<00:21,  3.12it/s, loss=0.00888, v_num=0, train_loss=0.00681, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  91%|█████████ | 660/727 [03:31<00:21,  3.12it/s, loss=0.00888, v_num=0, train_loss=0.00681, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  41%|████      | 30/73 [00:03<00:04,  9.68it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  41%|████      | 30/73 [00:03<00:04,  9.68it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 3:  95%|█████████▍| 690/727 [03:35<00:11,  3.21it/s, loss=0.00888, v_num=0, train_loss=0.00681, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  95%|█████████▍| 690/727 [03:35<00:11,  3.21it/s, loss=0.00888, v_num=0, train_loss=0.00681, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.15it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0:  82%|████████▏ | 60/73 [00:06<00:01,  9.15it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 3:  99%|█████████▉| 720/727 [03:38<00:02,  3.29it/s, loss=0.00888, v_num=0, train_loss=0.00681, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3:  99%|█████████▉| 720/727 [03:38<00:02,  3.29it/s, loss=0.00888, v_num=0, train_loss=0.00681, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.05it/s]#033[A\u001b[0m\n",
      "\u001b[32mValidation DataLoader 0: 100%|██████████| 73/73 [00:07<00:00,  9.05it/s]#033[A\u001b[0m\n",
      "\u001b[32mEpoch 3: 100%|██████████| 727/727 [03:39<00:00,  3.31it/s, loss=0.00888, v_num=0, train_loss=0.00681, val_loss=0.014]\u001b[0m\n",
      "\u001b[32mEpoch 3: 100%|██████████| 727/727 [03:39<00:00,  3.31it/s, loss=0.0087, v_num=0, train_loss=0.00353, val_loss=0.0132]\u001b[0m\n",
      "\u001b[32m#033[A\u001b[0m\n",
      "\u001b[32mEpoch 3: 100%|██████████| 727/727 [03:40<00:00,  3.30it/s, loss=0.0087, v_num=0, train_loss=0.00353, val_loss=0.0132]\u001b[0m\n",
      "\u001b[32mEpoch 3: 100%|██████████| 727/727 [03:43<00:00,  3.26it/s, loss=0.0087, v_num=0, train_loss=0.00353, val_loss=0.0132]\u001b[0m\n",
      "\u001b[32mtrain_model_output: trainer: <pytorch_lightning.trainer.trainer.Trainer object at 0x7fee64e0d8e0>\u001b[0m\n",
      "\u001b[32mtrain_model completed!\u001b[0m\n",
      "\u001b[32m====================================================================================================\u001b[0m\n",
      "\u001b[32mcreate_model_input: LABEL_COLUMNS: ['general criticsm', 'disability shaming', 'racial prejudice', 'sexism', 'lgbtq+ phobia']\u001b[0m\n",
      "\u001b[32mcreate_model_input: trainer: <pytorch_lightning.trainer.trainer.Trainer object at 0x7fee64e0d8e0>\u001b[0m\n",
      "\u001b[32mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[32m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[32m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32mcreate_model_output: trained_model: TweetTagger(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (criterion): BCELoss()\u001b[0m\n",
      "\u001b[32m)\u001b[0m\n",
      "\u001b[32mcreate_model completed!\u001b[0m\n",
      "\u001b[32m====================================================================================================\u001b[0m\n",
      "\u001b[32m2022-07-19 06:00:39,628 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2022-07-19 06:00:39,628 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2022-07-19 06:00:39,628 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-07-19 06:00:50 Uploading - Uploading generated training model\n",
      "2022-07-19 06:02:05 Completed - Training job completed\n",
      "ProfilerReport-1658209139: IssuesFound\n",
      "Training seconds: 3807\n",
      "Billable seconds: 3807\n"
     ]
    }
   ],
   "source": [
    "# cell 07\n",
    "pytorch_estimator.fit(training_data_uri)\n",
    "#pytorch_estimator.fit({'train': training_data_uri,\n",
    "#                       'test': test_data_uri})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Estimator is not associated with a training job",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q_/glkcp8_n5h300xwrkqscmh9c0000gn/T/ipykernel_48512/1290557714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cell 09\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m predictor = pytorch_estimator.deploy(instance_type='ml.c5.12xlarge',\n\u001b[0m\u001b[1;32m      3\u001b[0m                                      initial_instance_count=3)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, serverless_inference_config, async_inference_config, **kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0mremoved_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_endpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mis_serverless\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserverless_inference_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_latest_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_base_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36m_ensure_latest_training_job\u001b[0;34m(self, error_message)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m     \u001b[0mdelete_endpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoved_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete_endpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Estimator is not associated with a training job"
     ]
    }
   ],
   "source": [
    "# cell 09\n",
    "predictor = pytorch_estimator.deploy(instance_type='ml.c5.12xlarge',\n",
    "                                     initial_instance_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11\n",
    "data = \"I am a test.\"\n",
    "response = predictor.predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
